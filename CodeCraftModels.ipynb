{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc0zIIPjmHLe",
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVlZuvOkmJu0",
    "tags": []
   },
   "source": [
    "## Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ozqz7L-aByro"
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade pip\n",
    "#%pip install transformers==4.37.0\n",
    "#%pip uninstall torch torchvision torchaudio -y\n",
    "#%pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116 -y\n",
    "#%pip install torch torchvision torchaudio\n",
    "#%pip install tqdm\n",
    "#%pip install urllib3==1.26.15\n",
    "#%pip install accelerate==0.25.0\n",
    "#%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ScYYGSMOpU_X",
    "outputId": "061854c3-0977-4445-f330-df7a079e5b76",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n",
      "We are using the device cuda.\n",
      "Device count: 1\n",
      "Device name: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('We are using the device {}.'.format(device))\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41BNhCQ72BD3",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C5c0hfrX2BD3"
   },
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KtAQQDYt9tpn",
    "outputId": "185d672c-0fe8-4d22-dc5b-7fd16b17e856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory: 79.15 GB\n",
      "Currently allocated: 0.00 GB\n",
      "Cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Display total GPU memory\n",
    "print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Display currently allocated memory\n",
    "print(f\"Currently allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Display cached memory (reserved by PyTorch but not used)\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Gh1-kS8L9j_7",
    "outputId": "afc75e87-ec6c-4b4b-d727-088efa05c72f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                  Size  Used Avail Use% Mounted on\n",
      "devtmpfs                                    252G     0  252G   0% /dev\n",
      "tmpfs                                       252G  8.0K  252G   1% /dev/shm\n",
      "tmpfs                                       252G   59M  252G   1% /run\n",
      "tmpfs                                       252G     0  252G   0% /sys/fs/cgroup\n",
      "/dev/sda3                                    20G  5.3G   15G  27% /\n",
      "/dev/sda2                                   994M  188M  806M  19% /boot\n",
      "/dev/sda11                                  359G  401M  358G   1% /tmp\n",
      "/dev/sda7                                   9.8G  479M  9.3G   5% /var\n",
      "/dev/sda8                                   9.8G  299M  9.5G   3% /var/log\n",
      "/dev/sda9                                   9.8G   72M  9.7G   1% /var/log/audit\n",
      "/dev/sda10                                  9.8G   33M  9.8G   1% /var/tmp\n",
      "vast1-mghpcc-ib.neu.edu:/discovery/home     155T  133T   23T  86% /home\n",
      "vast1-mghpcc-ib.neu.edu:/vast_shared         30T   18T   13T  60% /shared\n",
      "vast1-mghpcc-ib.neu.edu:/courses             36T   14T   23T  38% /courses\n",
      "vast1-mghpcc-ib.neu.edu:/work_project       3.7P  2.5P  1.2P  69% /work\n",
      "vast1-mghpcc-ib.neu.edu:/discovery/scratch  2.2P  1.4P  730T  67% /scratch\n",
      "192.168.4.70:/datasets                       21T   21T  256G  99% /datasets\n"
     ]
    }
   ],
   "source": [
    "# # Check disk space\n",
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.9.12\n",
      "PyTorch: 1.13.1+cu116\n",
      "CUDA available: True\n",
      "PyTorch CUDA version: 11.6\n",
      "transformers: 4.37.0\n",
      "accelerate: 0.25.0\n",
      "huggingface_hub: 0.30.1\n",
      "datasets: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def check_versions():\n",
    "    # Python version\n",
    "    print(\"Python:\", sys.version.split()[0])\n",
    "    \n",
    "    # PyTorch and CUDA\n",
    "    try:\n",
    "        import torch\n",
    "        print(\"PyTorch:\", torch.__version__)\n",
    "        print(\"CUDA available:\", torch.cuda.is_available())\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"PyTorch CUDA version:\", torch.version.cuda)\n",
    "    except ImportError:\n",
    "        print(\"PyTorch: Not installed\")\n",
    "    \n",
    "    # Key packages\n",
    "    try:\n",
    "        import transformers\n",
    "        print(\"transformers:\", transformers.__version__)\n",
    "    except ImportError:\n",
    "        print(\"transformers: Not installed\")\n",
    "        \n",
    "    try:\n",
    "        import accelerate\n",
    "        print(\"accelerate:\", accelerate.__version__)\n",
    "    except ImportError:\n",
    "        print(\"accelerate: Not installed\")\n",
    "    \n",
    "#     try:\n",
    "#         import bitsandbytes\n",
    "#         print(\"bitsandbytes:\", bitsandbytes.__version__)\n",
    "#     except ImportError:\n",
    "#         print(\"bitsandbytes: Not installed\")\n",
    "    \n",
    "    try:\n",
    "        import huggingface_hub\n",
    "        print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
    "    except ImportError:\n",
    "        print(\"huggingface_hub: Not installed\")\n",
    "    \n",
    "    try:\n",
    "        import datasets\n",
    "        print(\"datasets:\", datasets.__version__)\n",
    "    except ImportError:\n",
    "        print(\"datasets: Not installed\")\n",
    "\n",
    "check_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iKaqNcom15F",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l14eRfYHW4xi",
    "tags": []
   },
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COT_PROMPT_TEMPLATE = \"\"\"Provide ONE concise algorithm strategy for this coding problem in EXACTLY 4 numbered points:\n",
    "\n",
    "1. Input/output: Single sentence describing parameters and return value\n",
    "2. Approach: Name the exact algorithm/data structure\n",
    "3. Key steps: 3-4 bullet points with specific algorithmic operations\n",
    "4. Edge cases: 2-3 specific edge conditions, no explanations needed\n",
    "\n",
    "Keep total response short. Be direct and technical.\n",
    "DO NOT include pseudocode, explanations, test cases, or implementation details.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Algorithm strategy:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODER_PROMPT_TEMPLATE = \"\"\"Generate only the Python code implementation for this problem.\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Using this algorithm strategy:\n",
    "{solution_cot}\n",
    "\n",
    "STRICT REQUIREMENTS:\n",
    "- Your output must begin with ```python\n",
    "- Your output must end with ```\n",
    "- ONLY write clean, efficient Python code\n",
    "- NO text before or after the code block\n",
    "- NO descriptions of what the code does\n",
    "\n",
    "Python code:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "778DzMEG7FSW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUGGER_PROMPT_TEMPLATE = \"\"\"Fix all bugs and inefficiencies in this Python code.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Original code:\n",
    "{code}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Output MUST start with ```python and end with ``` ONLY\n",
    "- NO explanations before or after the code\n",
    "- NO test cases or example output\n",
    "- NO justification of your changes\n",
    "- MINIMAL code changes to fix bugs/inefficiencies\n",
    "\n",
    "Debugged code:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6c3mVY375SPC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPLAINER_PROMPT_TEMPLATE = \"\"\"Create a short, beginner-friendly explanation of this code.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Code to explain:\n",
    "{code}\n",
    "\n",
    "Keep your explanation to 1-2 paragraphs.\n",
    "Focus on:\n",
    "- What the code accomplishes\n",
    "- The core algorithm approach used\n",
    "- One insightful observation about why it works\n",
    "- Any clever tricks worth noting\n",
    "\n",
    "Use friendly language that makes the solution approachable.\n",
    "\n",
    "Explanation:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvGPN7o2W8G5",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dataset Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FLXXoByzpNP2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CodeCraftDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A generalized dataset for Code Craft agents that works with various prompt templates.\n",
    "\n",
    "    Args:\n",
    "        examples: List of dictionaries that hold all agent prompt information.\n",
    "        tokenizer: Used to tokenize the inputs to the model.\n",
    "        prompt_template: The prompt template string with placeholders.\n",
    "        output_field: The name of the field in examples that contains the expected output.\n",
    "        max_length: The maximum token length of the inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, examples, tokenizer, prompt_template, output_field, max_length=512):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_template = prompt_template\n",
    "        self.output_field = output_field\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        output = example[self.output_field]\n",
    "\n",
    "        # Create prompt by formatting template with example data\n",
    "        # This will use all fields from the example that match placeholders in the template\n",
    "        try:\n",
    "            prompt = self.prompt_template.format(**example)\n",
    "        except KeyError as e:\n",
    "            missing_key = str(e).strip(\"'\")\n",
    "            raise KeyError(f\"Example at index {idx} is missing required field '{missing_key}' \"\n",
    "                          f\"for prompt template: {self.prompt_template}\")\n",
    "\n",
    "        # Combine prompt with expected output\n",
    "        full_text_with_output = prompt + output\n",
    "\n",
    "        # Tokenize the combined text\n",
    "        encoded = self.tokenizer(\n",
    "            full_text_with_output,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Create labels (same as input_ids but with -100 for prompt tokens)\n",
    "        prompt_tokens = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        prompt_length = len(prompt_tokens)\n",
    "\n",
    "        labels = encoded[\"input_ids\"].clone()\n",
    "        labels[0, :prompt_length] = -100  # Don't compute loss for prompt tokens\n",
    "\n",
    "        result = {\n",
    "            \"input_ids\": encoded[\"input_ids\"][0],\n",
    "            \"attention_mask\": encoded[\"attention_mask\"][0],\n",
    "            \"labels\": labels[0]\n",
    "        }\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3x0EQoEK8Oql",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dataset(problem_dataset, task_prompt, solution_field, output_marker,\n",
    "    model, tokenizer, num_examples=50, max_new_tokens=512, temperature=0.7, teacher=True, regen=False,\n",
    "    output_dir=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Generate a dataset by prompting a teacher model to solve problems for distillation.\n",
    "\n",
    "    Args:\n",
    "        problem_dataset: List of dictionaries containing problem data\n",
    "        task_prompt: Prompt template string with placeholders\n",
    "        solution_field: Field name for the generated solution in output examples\n",
    "        output_marker: String marker after which the solution starts in the model output\n",
    "                       (or None if the entire output is the solution)\n",
    "        model: The model used to generate solutions\n",
    "        tokenizer: Tokenizer for the model\n",
    "        num_examples: Number of examples to generate\n",
    "        max_new_tokens: Maximum token length for generation\n",
    "        teacher: a flag indicating if the model is teacher (true) or student (f)\n",
    "        regen: a flag indicating if the data should be regenerated if it already exists\n",
    "        output_dir: Directory to save the generated examples\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing the problems and their solutions\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get the model type from the teacher param\n",
    "    if teacher:\n",
    "        model_name = \"teacher\"\n",
    "    else:\n",
    "        model_name = \"student\"\n",
    "\n",
    "    # If indicated not to regenerate the examples and they exist then return them\n",
    "    file_name = os.path.join(output_dir, f\"{solution_field}_{model_name}_{num_examples}_dataset.json\")\n",
    "    if regen and os.path.exists(file_name):\n",
    "        with open(file_name, 'r') as examples_file:\n",
    "            examples = json.load(examples_file)\n",
    "        print(\"loaded examples from json\")\n",
    "        return examples\n",
    "\n",
    "    examples = []\n",
    "    logger.info(f\"Generating {solution_field} with {model_name} for {num_examples} problems...\")\n",
    "\n",
    "    # Take a subset of problems for efficiency\n",
    "    problems_subset = problem_dataset[:num_examples]\n",
    "\n",
    "    for i, problem in enumerate(tqdm(problems_subset, desc=f\"Generating {solution_field}\")):\n",
    "        try:\n",
    "            # Format the prompt with the problem data\n",
    "            prompt = task_prompt.format(**problem)\n",
    "\n",
    "            # Tokenize the prompt\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            # Generate the solution from the model\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    num_return_sequences=1\n",
    "                )\n",
    "\n",
    "            # Decode the model output\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            # Extract the solution portion if an output marker is provided\n",
    "            if output_marker and output_marker in generated_text:\n",
    "                solution_start_idx = generated_text.find(output_marker) + len(output_marker)\n",
    "                solution = generated_text[solution_start_idx:].strip()\n",
    "            else:\n",
    "                # Use the entire output if no marker is provided or found\n",
    "                solution = generated_text.replace(prompt, \"\").strip()\n",
    "\n",
    "            # Create the example with all original problem fields plus the solution\n",
    "            example = problem.copy()  # Preserve all original fields\n",
    "            example[solution_field] = solution  # Add the generated solution\n",
    "            examples.append(example)\n",
    "\n",
    "            # Save a few examples for inspection\n",
    "            if i < 2:\n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                print(f\"Problem: {example['problem'][:150]}...\")\n",
    "                print(f\"Solution (first 150 chars): {example[solution_field][:150]}...\")\n",
    "\n",
    "            # Log progress details periodically\n",
    "            if (i + 1) % 10 == 0:\n",
    "                logger.info(f\"Generated {i + 1}/{len(problems_subset)} solutions\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating solution for problem {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"Successfully generated {len(examples)} {solution_field} solutions\")\n",
    "\n",
    "    # Save the dataset\n",
    "    with open(file_name, \"w\") as f:\n",
    "        json.dump(examples, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Dataset saved to {file_name}\")\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJYlB4F42BD5",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load Dataset Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vDP6VUFU2BD5"
   },
   "outputs": [],
   "source": [
    "# Load MBPP dataset\n",
    "def load_mbpp_dataset():\n",
    "    mbpp = load_dataset(\"mbpp\")\n",
    "\n",
    "    train_problems = []\n",
    "    # Extract problems from the MBPP dataset with correct field names\n",
    "    for item in mbpp[\"train\"]:\n",
    "        train_problems.append({\n",
    "            \"problem\": item[\"text\"],\n",
    "            \"test_case\": item[\"test_list\"],\n",
    "            \"solution_code\": item[\"code\"]\n",
    "        })\n",
    "\n",
    "    test_problems = []\n",
    "    for item in mbpp[\"test\"]:\n",
    "        test_problems.append({\n",
    "            \"problem\": item[\"text\"],\n",
    "            \"test_case\": item[\"test_list\"],\n",
    "            \"solution_code\": item[\"code\"]\n",
    "        })\n",
    "\n",
    "    print(f\"Loaded {len(train_problems)} train problems and {len(test_problems)} evaluation problems from MBPP dataset\")\n",
    "    return train_problems, test_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nh2SiyeD2BD6"
   },
   "outputs": [],
   "source": [
    "# Load BAAI/TACO dataset\n",
    "def load_taco_dataset():\n",
    "    taco = load_dataset(\"BAAI/TACO\")\n",
    "\n",
    "    train_problems = []\n",
    "    for item in taco[\"train\"]:\n",
    "        train_problems.append({\n",
    "            \"problem\": item[\"question\"],\n",
    "            \"test_case\": item[\"input_output\"],\n",
    "            \"solution_code\": item[\"solutions\"][0]\n",
    "        })\n",
    "\n",
    "    test_problems = []\n",
    "    for item in taco[\"test\"]:\n",
    "        train_problems.append({\n",
    "            \"problem\": item[\"question\"],\n",
    "            \"test_case\": item[\"test_cases\"],\n",
    "            \"solution_code\": item[\"solutions\"][0]\n",
    "        })\n",
    "\n",
    "    print(f\"Loaded {len(train_problems)} train problems and {len(test_problems)} test problems from TACO dataset\")\n",
    "    return train_problems, test_problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnoxXUO9FAOU",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Agent Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvOVJuMDFCLy",
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "h2l4VajfELPR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load models\n",
    "def load_models(teacher_model_name, student_model_name):\n",
    "    logger.info(f\"Loading teacher model: {teacher_model_name}\")\n",
    "    teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
    "    teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "        teacher_model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "    logger.info(f\"Teacher model loaded successfully\")\n",
    "\n",
    "    logger.info(f\"Loading student model: {student_model_name}\")\n",
    "    student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "    student_model = AutoModelForCausalLM.from_pretrained(\n",
    "        student_model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "    logger.info(f\"Student model loaded successfully\")\n",
    "\n",
    "    return teacher_model, teacher_tokenizer, student_model, student_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0hx7iq02UDu",
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AcHhd7BGRdQC"
   },
   "outputs": [],
   "source": [
    "def fine_tune_student_model(student_model, student_tokenizer, train_data, prompt,\n",
    "                        output_field, batch_size=8, num_epochs=3, learning_rate=5e-5,\n",
    "                        max_grad_norm=1.0, warmup_steps=0, max_length=512,\n",
    "                        output_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Fine-tune the student model on examples generated by the teacher model.\n",
    "\n",
    "    Args:\n",
    "        student_model: The student model to train\n",
    "        student_tokenizer: Tokenizer for the student model\n",
    "        train_data: List of data dictionaries for training\n",
    "        prompt: The prompt containing fields for training\n",
    "        output: The output data field to train on\n",
    "        batch_size: Training batch size\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for the optimizer\n",
    "        max_grad_norm: Maximum gradient norm for gradient clipping\n",
    "        warmup_steps: Linear warmup steps for the learning rate scheduler\n",
    "        max_length: the maximum number of tokens in the dataset values\n",
    "        output_dir: Directory to save the trained model\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logger.info(f\"Starting training the student model for {num_epochs} epochs\")\n",
    "\n",
    "    # Create PyTorch dataset and dataloader\n",
    "    dataset = CodeCraftDataset(\n",
    "        examples=train_data,\n",
    "        tokenizer=student_tokenizer,\n",
    "        prompt_template=prompt,\n",
    "        output_field=output_field,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Set up optimizer and learning rate scheduler\n",
    "    optimizer = optim.AdamW(student_model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(dataloader) * num_epochs\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=learning_rate, total_steps=total_steps,\n",
    "        pct_start=warmup_steps/total_steps if warmup_steps > 0 else 0.1\n",
    "    )\n",
    "\n",
    "    # Set up training tracking\n",
    "    best_loss = float('inf')\n",
    "    global_step = 0\n",
    "    student_model.train()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch[\"input_ids\"].to(student_model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(student_model.device)\n",
    "            labels = batch[\"labels\"].to(student_model.device)\n",
    "\n",
    "            # Forward pass - compute student model outputs\n",
    "            outputs = student_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_grad_norm)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Track loss\n",
    "            epoch_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "            # Save checkpoint occasionally\n",
    "            if global_step % 100 == 0:\n",
    "                logger.info(f\"Step {global_step}: loss = {loss.item():.4f}\")\n",
    "\n",
    "        # Compute average epoch loss\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs} - Average loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint if it's the best model so far\n",
    "        # if avg_epoch_loss < best_loss:\n",
    "        #     best_loss = avg_epoch_loss\n",
    "        #     checkpoint_path = os.path.join(output_dir, f\"student_model_{output_field}_epoch_{epoch+1}\")\n",
    "        #     logger.info(f\"Saving best model so far (loss: {best_loss:.4f}) to {checkpoint_path}\")\n",
    "        #     student_model.save_pretrained(checkpoint_path)\n",
    "        #     student_tokenizer.save_pretrained(checkpoint_path)\n",
    "\n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(output_dir, f\"student_model_{output_field}_final\")\n",
    "    logger.info(f\"Training completed. Saving final model to {final_model_path}\")\n",
    "    student_model.save_pretrained(final_model_path)\n",
    "    student_tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JENI9d5P2UDu",
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ePM5WNnP2UDu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_student_model(student_model, student_tokenizer, test_problems, teacher_model=None,\n",
    "                          batch_size=4, max_length=512, temperature=0.7, output_dir=\"results/evaluations\"):\n",
    "    \"\"\"\n",
    "    Evaluate the student model on a set of test problems.\n",
    "\n",
    "    Args:\n",
    "        student_model: Trained student model\n",
    "        student_tokenizer: Tokenizer for the student model\n",
    "        test_problems: List of test problems to evaluate on\n",
    "        teacher_model: Optional teacher model for comparison\n",
    "        batch_size: Batch size for evaluation\n",
    "        max_length: Maximum sequence length for generation\n",
    "        output_dir: Directory to save evaluation results\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logger.info(f\"Evaluating student model on {len(test_problems)} test problems\")\n",
    "\n",
    "    # Set models to evaluation mode\n",
    "    student_model.eval()\n",
    "    if teacher_model is not None:\n",
    "        teacher_model.eval()\n",
    "\n",
    "    results = {\n",
    "        \"total_problems\": len(test_problems),\n",
    "        \"student_generations\": [],\n",
    "        \"teacher_generations\": [] if teacher_model else None,\n",
    "        \"prompts\": []\n",
    "    }\n",
    "\n",
    "    # Process test problems in batches\n",
    "    for i in range(0, len(test_problems), batch_size):\n",
    "        batch_problems = test_problems[i:i+batch_size]\n",
    "        batch_prompts = []\n",
    "\n",
    "        for problem in batch_problems:\n",
    "            prompt = PROMPT_TEMPLATE.format(problem=problem[\"problem\"])\n",
    "            batch_prompts.append(prompt)\n",
    "            results[\"prompts\"].append(prompt)\n",
    "\n",
    "        # Generate solutions with student model\n",
    "        student_outputs = []\n",
    "        for prompt in tqdm(batch_prompts, desc=\"Generating student solutions\"):\n",
    "            inputs = student_tokenizer(prompt, return_tensors=\"pt\").to(student_model.device)\n",
    "\n",
    "            student_model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = student_model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=max_length,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    num_return_sequences=1\n",
    "                )\n",
    "\n",
    "            decoded_output = student_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            student_outputs.append(decoded_output)\n",
    "\n",
    "        results[\"student_generations\"].extend(student_outputs)\n",
    "\n",
    "        # If teacher model is provided, generate solutions for comparison\n",
    "        if teacher_model:\n",
    "            teacher_outputs = []\n",
    "            for prompt in tqdm(batch_prompts, desc=\"Generating teacher solutions\"):\n",
    "                inputs = student_tokenizer(prompt, return_tensors=\"pt\").to(teacher_model.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = teacher_model.generate(\n",
    "                        **inputs,\n",
    "                        max_length=max_length,\n",
    "                        temperature=temperature,\n",
    "                        do_sample=True,\n",
    "                        top_p=0.9,\n",
    "                        num_return_sequences=1\n",
    "                    )\n",
    "\n",
    "                decoded_output = student_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "                teacher_outputs.append(decoded_output)\n",
    "\n",
    "            results[\"teacher_generations\"].extend(teacher_outputs)\n",
    "\n",
    "    # Process and extract solutions\n",
    "    logger.info(\"Processing generated solutions\")\n",
    "    student_solutions = []\n",
    "    teacher_solutions = [] if teacher_model else None\n",
    "\n",
    "    for output in results[\"student_generations\"]:\n",
    "        solution_start_marker = \"Step-by-step solution:\"\n",
    "        solution_start_idx = output.find(solution_start_marker) + len(solution_start_marker)\n",
    "        solution = output[solution_start_idx:].strip()\n",
    "        student_solutions.append(solution)\n",
    "\n",
    "    if teacher_model:\n",
    "        for output in results[\"teacher_generations\"]:\n",
    "            solution_start_marker = \"Step-by-step solution:\"\n",
    "            solution_start_idx = output.find(solution_start_marker) + len(solution_start_marker)\n",
    "            solution = output[solution_start_idx:].strip()\n",
    "            teacher_solutions.append(solution)\n",
    "\n",
    "    # Calculate some basic metrics\n",
    "    logger.info(\"Calculating evaluation metrics\")\n",
    "\n",
    "    # Calculate average solution length\n",
    "    student_avg_length = sum(len(solution.split()) for solution in student_solutions) / len(student_solutions)\n",
    "    results[\"student_avg_word_count\"] = student_avg_length\n",
    "\n",
    "    if teacher_model:\n",
    "        teacher_avg_length = sum(len(solution.split()) for solution in teacher_solutions) / len(teacher_solutions)\n",
    "        results[\"teacher_avg_word_count\"] = teacher_avg_length\n",
    "        results[\"length_ratio\"] = student_avg_length / teacher_avg_length if teacher_avg_length > 0 else 0\n",
    "\n",
    "    # Check for step-by-step reasoning keywords\n",
    "    reasoning_keywords = [\"first\", \"second\", \"third\", \"next\", \"then\", \"finally\", \"step\", \"let's\", \"because\", \"reason\"]\n",
    "    student_keyword_counts = []\n",
    "\n",
    "    for solution in student_solutions:\n",
    "        solution_lower = solution.lower()\n",
    "        count = sum(1 for keyword in reasoning_keywords if keyword in solution_lower)\n",
    "        student_keyword_counts.append(count)\n",
    "\n",
    "    results[\"student_avg_reasoning_markers\"] = sum(student_keyword_counts) / len(student_keyword_counts)\n",
    "\n",
    "    if teacher_model:\n",
    "        teacher_keyword_counts = []\n",
    "        for solution in teacher_solutions:\n",
    "            solution_lower = solution.lower()\n",
    "            count = sum(1 for keyword in reasoning_keywords if keyword in solution_lower)\n",
    "            teacher_keyword_counts.append(count)\n",
    "\n",
    "        results[\"teacher_avg_reasoning_markers\"] = sum(teacher_keyword_counts) / len(teacher_keyword_counts)\n",
    "        results[\"reasoning_marker_ratio\"] = (results[\"student_avg_reasoning_markers\"] /\n",
    "                                           results[\"teacher_avg_reasoning_markers\"]\n",
    "                                           if results[\"teacher_avg_reasoning_markers\"] > 0 else 0)\n",
    "\n",
    "    # Save a few example comparisons\n",
    "    with open(os.path.join(output_dir, \"solution_examples.txt\"), \"w\") as f:\n",
    "        for i in range(min(5, len(student_solutions))):\n",
    "            f.write(f\"Problem {i+1}:\\n\")\n",
    "            f.write(f\"{results['prompts'][i]}\\n\\n\")\n",
    "            f.write(f\"Student solution:\\n{student_solutions[i]}\\n\\n\")\n",
    "            if teacher_model:\n",
    "                f.write(f\"Teacher solution:\\n{teacher_solutions[i]}\\n\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "    # Save all evaluation results\n",
    "    with open(os.path.join(output_dir, \"evaluation_results.json\"), \"w\") as f:\n",
    "        # Create a summary version without the full generations for easier reading\n",
    "        summary_results = {k: v for k, v in results.items()\n",
    "                         if k not in [\"student_generations\", \"teacher_generations\", \"prompts\"]}\n",
    "        json.dump(summary_results, f, indent=2)\n",
    "\n",
    "    # Save the full results separately\n",
    "    with open(os.path.join(output_dir, \"full_results.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Evaluation complete. Results saved to {output_dir}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qS8JBDuQ2UDv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def track_best_model(evaluation_results, best_metrics, model_path, output_dir=\"results/best_model\"):\n",
    "    \"\"\"\n",
    "    Track and save the best student model based on evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        evaluation_results: Results dictionary from evaluate_student_model\n",
    "        best_metrics: Dictionary with current best metrics\n",
    "        model_path: Path to the current model\n",
    "        output_dir: Directory to save the best model\n",
    "\n",
    "    Returns:\n",
    "        Updated best_metrics dictionary\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define a scoring function to rank models (higher is better)\n",
    "    # Here we prioritize reasoning marker ratio and solution length ratio\n",
    "    current_score = (\n",
    "        evaluation_results.get(\"reasoning_marker_ratio\", 0) * 0.7 +\n",
    "        evaluation_results.get(\"length_ratio\", 0) * 0.3\n",
    "    )\n",
    "\n",
    "    best_score = (\n",
    "        best_metrics.get(\"reasoning_marker_ratio\", 0) * 0.7 +\n",
    "        best_metrics.get(\"length_ratio\", 0) * 0.3\n",
    "    )\n",
    "\n",
    "    # Check if current model is better than the best so far\n",
    "    if current_score > best_score:\n",
    "        logger.info(f\"New best model found! Score: {current_score:.4f} (previous: {best_score:.4f})\")\n",
    "\n",
    "        # Update best metrics\n",
    "        best_metrics = {\n",
    "            \"score\": current_score,\n",
    "            \"model_path\": model_path,\n",
    "            \"reasoning_marker_ratio\": evaluation_results.get(\"reasoning_marker_ratio\", 0),\n",
    "            \"length_ratio\": evaluation_results.get(\"length_ratio\", 0),\n",
    "            \"student_avg_reasoning_markers\": evaluation_results.get(\"student_avg_reasoning_markers\", 0),\n",
    "            \"student_avg_word_count\": evaluation_results.get(\"student_avg_word_count\", 0)\n",
    "        }\n",
    "\n",
    "        # Copy the model to the best model directory\n",
    "        if os.path.exists(model_path):\n",
    "            logger.info(f\"Copying best model from {model_path} to {output_dir}\")\n",
    "\n",
    "            # Clear previous best model\n",
    "            if os.path.exists(output_dir):\n",
    "                for file in os.listdir(output_dir):\n",
    "                    file_path = os.path.join(output_dir, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "\n",
    "            # Copy new best model\n",
    "            for file in os.listdir(model_path):\n",
    "                source_file = os.path.join(model_path, file)\n",
    "                if os.path.isfile(source_file):\n",
    "                    shutil.copy(source_file, os.path.join(output_dir, file))\n",
    "\n",
    "        # Save best metrics\n",
    "        with open(os.path.join(output_dir, \"best_metrics.json\"), \"w\") as f:\n",
    "            json.dump(best_metrics, f, indent=2)\n",
    "\n",
    "    return best_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgfrDq5qFP78",
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0DDmMTV2xfR",
    "tags": []
   },
   "source": [
    "## Params and Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xEqcyVZA5mtZ"
   },
   "outputs": [],
   "source": [
    "# Global Params\n",
    "TEACHER_EXAMPLE_LEN = 374 # number of train mbpp problems\n",
    "STUDENT_EXAMPLE_LEN = 50\n",
    "GENERATED_TOKEN_LEN = 512\n",
    "\n",
    "# Training Params\n",
    "NUM_EPOCHS = 6\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 10\n",
    "WARMUP_STEPS = TEACHER_EXAMPLE_LEN * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "bc2a8de9260a4822ab6b6b596e226270",
      "c4fb72d144d74d62b99b0bf39c7eb3de"
     ]
    },
    "id": "llYcwAkM5FW7",
    "outputId": "e915aeda-6f61-4337-cecc-5d3ef39ae9a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MBPP dataset...\n",
      "Loaded 374 train problems and 500 evaluation problems from MBPP dataset\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "print(\"Loading MBPP dataset...\")\n",
    "mbpp_train_examples, mbpp_test_examples = load_mbpp_dataset()\n",
    "\n",
    "# print(\"Loading Instruct models...\")\n",
    "# teacher_model, tokenizer, student_model, student_tokenizer = load_models(\"Qwen/Qwen2.5-7B-Instruct\", \"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "# print(\"Loading Coder models...\")\n",
    "# code_teacher_model, code_tokenizer, code_student_model, code_student_tokenizer = load_models(\"Qwen/Qwen2.5-Coder-7B-Instruct\", \"Qwen/Qwen2.5-Coder-0.5B-Instruct\")\n",
    "\n",
    "# save student model initial states for efficient memory storage when fine tuning later\n",
    "# student_initial_state = {k: v.detach().clone() for k, v in student_model.state_dict().items()}\n",
    "# code_student_initial_state = {k: v.detach().clone() for k, v in code_student_model.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DpMPflAGee4h"
   },
   "outputs": [],
   "source": [
    "def generate_base_model_examples(train_problems, test_problems, teacher_model, student_model, tokenizer):\n",
    "    print(f\"Generating {SOLUTION_FIELD} examples using Teacher model...\")\n",
    "    train_examples = generate_dataset(\n",
    "        train_problems,\n",
    "        PROMPT_TEMPLATE,\n",
    "        SOLUTION_FIELD,\n",
    "        OUTPUT_MARKER,\n",
    "        teacher_model,\n",
    "        tokenizer,\n",
    "        num_examples=TEACHER_EXAMPLE_LEN,\n",
    "        max_new_tokens=GENERATED_TOKEN_LEN,\n",
    "        temperature=TEMPERATURE\n",
    "    )\n",
    "\n",
    "    print(f\"Generating {SOLUTION_FIELD} examples using untrained Student model...\")\n",
    "    untrained_examples = generate_dataset(\n",
    "        test_problems,\n",
    "        PROMPT_TEMPLATE,\n",
    "        SOLUTION_FIELD,\n",
    "        OUTPUT_MARKER,\n",
    "        student_model,\n",
    "        tokenizer,\n",
    "        num_examples=STUDENT_EXAMPLE_LEN,\n",
    "        max_new_tokens=GENERATED_TOKEN_LEN,\n",
    "        temperature=TEMPERATURE,\n",
    "        teacher=False\n",
    "    )\n",
    "\n",
    "    return train_examples, untrained_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WPMrOcQthBnH"
   },
   "outputs": [],
   "source": [
    "def start_fine_tuning(student_model, tokenizer, train_examples):\n",
    "  print(f\"Fine-Tuning {SOLUTION_FIELD} on Student model...\")\n",
    "  trained_student_model = fine_tune_student_model(\n",
    "      student_model=student_model,\n",
    "      student_tokenizer=tokenizer,\n",
    "      train_data=train_examples,\n",
    "      prompt=PROMPT_TEMPLATE,\n",
    "      output_field=SOLUTION_FIELD,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      num_epochs=NUM_EPOCHS,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      warmup_steps=WARMUP_STEPS,\n",
    "      max_length=GENERATED_TOKEN_LEN\n",
    "  )\n",
    "\n",
    "  return trained_student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ZhIOlsM0iZ3A"
   },
   "outputs": [],
   "source": [
    "def generate_trained_model_examples(test_problems, trained_student_model, tokenizer):\n",
    "    print(f\"Generating {SOLUTION_FIELD} examples using Trained Student model...\")\n",
    "    trained_examples = generate_dataset(\n",
    "        test_problems,\n",
    "        PROMPT_TEMPLATE,\n",
    "        SOLUTION_FIELD,\n",
    "        OUTPUT_MARKER,\n",
    "        trained_student_model,\n",
    "        tokenizer,\n",
    "        num_examples=STUDENT_EXAMPLE_LEN+1, # add 1 to not overwrite the untrained student data file\n",
    "        max_new_tokens=GENERATED_TOKEN_LEN,\n",
    "        temperature=TEMPERATURE,\n",
    "        teacher=False\n",
    "    )\n",
    "\n",
    "    return trained_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_I_xjz02xfg",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Restart and Run all above\n",
    "In case of disk/memory filling, restart the kernel and run cells above here. Then load data generated so far from json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEujXBBEVQ9Z",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CoT Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EqccoRV4nKQG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 19:47:43,088 - INFO - Loading teacher model: Qwen/Qwen2.5-7B-Instruct\n",
      "/home/javvaji.m/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Instruct models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 19:47:44,833 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991131ed131f415db5c32bf4dcaf9c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 19:47:53,626 - INFO - Teacher model loaded successfully\n",
      "2025-04-19 19:47:53,627 - INFO - Loading student model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-04-19 19:47:53,880 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-04-19 19:47:54,691 - INFO - Student model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "# teacher_model.to(device)\n",
    "# code_teacher_model.cpu\n",
    "# student_model.to(device)\n",
    "# code_student_model.cpu\n",
    "\n",
    "# CoT Agent Params\n",
    "PROMPT_TEMPLATE = COT_PROMPT_TEMPLATE\n",
    "SOLUTION_FIELD = \"solution_cot\"\n",
    "OUTPUT_MARKER = \"Step-by-step solution:\"\n",
    "GENERATED_TOKEN_LEN = 150\n",
    "TEMPERATURE=0.6\n",
    "\n",
    "print(\"Loading Instruct models...\")\n",
    "teacher_model, tokenizer, student_model, student_tokenizer = load_models(\"Qwen/Qwen2.5-7B-Instruct\", \"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "student_initial_state = {k: v.detach().clone() for k, v in student_model.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PEkb1GQgVUDu",
    "outputId": "621ffe85-10e6-4f32-9aeb-b0b38cddb78f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 19:47:57,316 - INFO - Generating solution_cot with teacher for 374 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating solution_cot examples using Teacher model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:   0%|          | 1/374 [00:04<25:22,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a function to find the longest chain which can be formed from the given set of pairs....\n",
      "Solution (first 150 chars): Greedy\n",
      "\n",
      "1. Input/output: Given a list of n pairs of integers [(a1, b1), (a2, b2), ..., (an, bn)], return an integer representing the length of the lon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:   1%|          | 2/374 [00:07<23:25,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a python function to find the first repeated character in a given string....\n",
      "Solution (first 150 chars): Hash Table\n",
      "\n",
      "1. Input/output: Given a string; return the first repeated character as a string\n",
      "2. Approach: Use a hash table to track character occurren...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:   2%|         | 9/374 [00:32<21:25,  3.52s/it]2025-04-19 19:48:33,198 - INFO - Generated 10/374 solutions\n",
      "Generating solution_cot:   5%|         | 19/374 [01:07<21:04,  3.56s/it]2025-04-19 19:49:08,768 - INFO - Generated 20/374 solutions\n",
      "Generating solution_cot:   8%|         | 29/374 [01:43<20:30,  3.57s/it]2025-04-19 19:49:44,415 - INFO - Generated 30/374 solutions\n",
      "Generating solution_cot:  10%|         | 39/374 [02:19<19:56,  3.57s/it]2025-04-19 19:50:20,058 - INFO - Generated 40/374 solutions\n",
      "Generating solution_cot:  13%|        | 49/374 [02:53<19:05,  3.53s/it]2025-04-19 19:50:54,711 - INFO - Generated 50/374 solutions\n",
      "Generating solution_cot:  16%|        | 59/374 [03:29<18:41,  3.56s/it]2025-04-19 19:51:30,326 - INFO - Generated 60/374 solutions\n",
      "Generating solution_cot:  18%|        | 69/374 [04:05<18:06,  3.56s/it]2025-04-19 19:52:05,937 - INFO - Generated 70/374 solutions\n",
      "Generating solution_cot:  21%|        | 79/374 [04:40<17:31,  3.56s/it]2025-04-19 19:52:41,581 - INFO - Generated 80/374 solutions\n",
      "Generating solution_cot:  24%|       | 89/374 [05:16<16:58,  3.58s/it]2025-04-19 19:53:16,753 - INFO - Generated 90/374 solutions\n",
      "Generating solution_cot:  26%|       | 99/374 [05:50<16:12,  3.54s/it]2025-04-19 19:53:51,776 - INFO - Generated 100/374 solutions\n",
      "Generating solution_cot:  29%|       | 109/374 [06:26<15:25,  3.49s/it]2025-04-19 19:54:26,940 - INFO - Generated 110/374 solutions\n",
      "Generating solution_cot:  32%|      | 119/374 [07:01<15:07,  3.56s/it]2025-04-19 19:55:02,581 - INFO - Generated 120/374 solutions\n",
      "Generating solution_cot:  34%|      | 129/374 [07:36<14:19,  3.51s/it]2025-04-19 19:55:37,213 - INFO - Generated 130/374 solutions\n",
      "Generating solution_cot:  37%|      | 139/374 [08:11<13:56,  3.56s/it]2025-04-19 19:56:12,824 - INFO - Generated 140/374 solutions\n",
      "Generating solution_cot:  40%|      | 149/374 [08:46<12:23,  3.30s/it]2025-04-19 19:56:47,192 - INFO - Generated 150/374 solutions\n",
      "Generating solution_cot:  43%|     | 159/374 [09:20<11:30,  3.21s/it]2025-04-19 19:57:21,663 - INFO - Generated 160/374 solutions\n",
      "Generating solution_cot:  45%|     | 169/374 [09:56<12:07,  3.55s/it]2025-04-19 19:57:57,258 - INFO - Generated 170/374 solutions\n",
      "Generating solution_cot:  48%|     | 179/374 [10:31<11:34,  3.56s/it]2025-04-19 19:58:32,867 - INFO - Generated 180/374 solutions\n",
      "Generating solution_cot:  51%|     | 189/374 [11:07<10:58,  3.56s/it]2025-04-19 19:59:08,474 - INFO - Generated 190/374 solutions\n",
      "Generating solution_cot:  53%|    | 199/374 [11:43<10:22,  3.56s/it]2025-04-19 19:59:44,068 - INFO - Generated 200/374 solutions\n",
      "Generating solution_cot:  56%|    | 209/374 [12:18<09:43,  3.54s/it]2025-04-19 20:00:19,049 - INFO - Generated 210/374 solutions\n",
      "Generating solution_cot:  59%|    | 219/374 [12:53<09:12,  3.56s/it]2025-04-19 20:00:54,694 - INFO - Generated 220/374 solutions\n",
      "Generating solution_cot:  61%|    | 229/374 [13:28<08:33,  3.54s/it]2025-04-19 20:01:29,453 - INFO - Generated 230/374 solutions\n",
      "Generating solution_cot:  64%|   | 239/374 [14:03<07:50,  3.49s/it]2025-04-19 20:02:04,621 - INFO - Generated 240/374 solutions\n",
      "Generating solution_cot:  67%|   | 249/374 [14:39<07:25,  3.56s/it]2025-04-19 20:02:40,274 - INFO - Generated 250/374 solutions\n",
      "Generating solution_cot:  69%|   | 259/374 [15:14<06:43,  3.51s/it]2025-04-19 20:03:15,051 - INFO - Generated 260/374 solutions\n",
      "Generating solution_cot:  72%|  | 269/374 [15:49<06:13,  3.56s/it]2025-04-19 20:03:50,709 - INFO - Generated 270/374 solutions\n",
      "Generating solution_cot:  75%|  | 279/374 [16:24<05:04,  3.20s/it]2025-04-19 20:04:25,120 - INFO - Generated 280/374 solutions\n",
      "Generating solution_cot:  77%|  | 289/374 [16:59<05:01,  3.55s/it]2025-04-19 20:05:00,746 - INFO - Generated 290/374 solutions\n",
      "Generating solution_cot:  80%|  | 299/374 [17:35<04:27,  3.57s/it]2025-04-19 20:05:36,395 - INFO - Generated 300/374 solutions\n",
      "Generating solution_cot:  83%| | 309/374 [18:10<03:49,  3.53s/it]2025-04-19 20:06:11,764 - INFO - Generated 310/374 solutions\n",
      "Generating solution_cot:  85%| | 319/374 [18:46<03:11,  3.48s/it]2025-04-19 20:06:47,114 - INFO - Generated 320/374 solutions\n",
      "Generating solution_cot:  88%| | 329/374 [19:21<02:40,  3.56s/it]2025-04-19 20:07:22,741 - INFO - Generated 330/374 solutions\n",
      "Generating solution_cot:  91%| | 339/374 [19:57<02:04,  3.56s/it]2025-04-19 20:07:58,409 - INFO - Generated 340/374 solutions\n",
      "Generating solution_cot:  93%|| 349/374 [20:33<01:29,  3.56s/it]2025-04-19 20:08:34,017 - INFO - Generated 350/374 solutions\n",
      "Generating solution_cot:  96%|| 359/374 [21:08<00:53,  3.56s/it]2025-04-19 20:09:09,621 - INFO - Generated 360/374 solutions\n",
      "Generating solution_cot:  99%|| 369/374 [21:42<00:17,  3.40s/it]2025-04-19 20:09:43,510 - INFO - Generated 370/374 solutions\n",
      "Generating solution_cot: 100%|| 374/374 [22:00<00:00,  3.53s/it]\n",
      "2025-04-19 20:09:57,786 - INFO - Successfully generated 374 solution_cot solutions\n",
      "2025-04-19 20:09:57,796 - INFO - Dataset saved to dataset/solution_cot_teacher_374_dataset.json\n",
      "2025-04-19 20:09:57,797 - INFO - Generating solution_cot with student for 50 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating solution_cot examples using untrained Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:   2%|         | 1/50 [00:02<02:11,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): 1. Define an empty list named 'chars' to store characters that need to be removed.\n",
      "2. Use a while loop to iterate through each character in the input ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:   4%|         | 2/50 [00:05<02:08,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): 1. Calculate the sum of each row.\n",
      "2. Sort these sums in ascending order.\n",
      "3. Construct the sorted matrix by placing each element at its corresponding p...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:  18%|        | 9/50 [00:24<01:50,  2.69s/it]2025-04-19 20:10:24,699 - INFO - Generated 10/50 solutions\n",
      "Generating solution_cot:  38%|      | 19/50 [00:51<01:23,  2.68s/it]2025-04-19 20:10:51,524 - INFO - Generated 20/50 solutions\n",
      "Generating solution_cot:  58%|    | 29/50 [01:17<00:56,  2.69s/it]2025-04-19 20:11:18,440 - INFO - Generated 30/50 solutions\n",
      "Generating solution_cot:  78%|  | 39/50 [01:44<00:29,  2.68s/it]2025-04-19 20:11:45,252 - INFO - Generated 40/50 solutions\n",
      "Generating solution_cot:  98%|| 49/50 [02:11<00:02,  2.68s/it]2025-04-19 20:12:12,054 - INFO - Generated 50/50 solutions\n",
      "Generating solution_cot: 100%|| 50/50 [02:14<00:00,  2.69s/it]\n",
      "2025-04-19 20:12:12,055 - INFO - Successfully generated 50 solution_cot solutions\n",
      "2025-04-19 20:12:12,061 - INFO - Dataset saved to dataset/solution_cot_student_50_dataset.json\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "student_model.load_state_dict(student_initial_state)\n",
    "train_cot_examples, untrained_cot_examples = generate_base_model_examples(\n",
    "    mbpp_train_examples,\n",
    "    mbpp_test_examples,\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3spoWpDH2BD9"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the examples from json\n",
    "\n",
    "# clear_gpu_memory()\n",
    "\n",
    "# mdpp_examples_file = open(f\"dataset/{SOLUTION_FIELD}_teacher_{TEACHER_EXAMPLE_LEN}_dataset.json\")\n",
    "# train_cot_examples = json.load(mdpp_examples_file)\n",
    "\n",
    "# print(train_cot_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jPV6i43RnKQH",
    "outputId": "7224f298-9487-44b8-88eb-86e5f83430e1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:12:12,223 - INFO - Starting training the student model for 6 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning solution_cot on Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|| 38/38 [00:13<00:00,  2.84it/s, loss=0.761]\n",
      "2025-04-19 20:12:25,593 - INFO - Epoch 1/6 - Average loss: 0.8007\n",
      "Epoch 2/6: 100%|| 38/38 [00:13<00:00,  2.85it/s, loss=0.34] \n",
      "2025-04-19 20:12:38,917 - INFO - Epoch 2/6 - Average loss: 0.4000\n",
      "Epoch 3/6:  61%|    | 23/38 [00:08<00:05,  2.82it/s, loss=0.302]2025-04-19 20:12:47,436 - INFO - Step 100: loss = 0.3015\n",
      "Epoch 3/6: 100%|| 38/38 [00:13<00:00,  2.85it/s, loss=0.116]\n",
      "2025-04-19 20:12:52,234 - INFO - Epoch 3/6 - Average loss: 0.2299\n",
      "Epoch 4/6: 100%|| 38/38 [00:13<00:00,  2.85it/s, loss=0.132] \n",
      "2025-04-19 20:13:05,551 - INFO - Epoch 4/6 - Average loss: 0.1140\n",
      "Epoch 5/6: 100%|| 38/38 [00:13<00:00,  2.85it/s, loss=0.0257]\n",
      "2025-04-19 20:13:18,864 - INFO - Epoch 5/6 - Average loss: 0.0462\n",
      "Epoch 6/6:  24%|       | 9/38 [00:03<00:10,  2.82it/s, loss=0.041] 2025-04-19 20:13:22,414 - INFO - Step 200: loss = 0.0410\n",
      "Epoch 6/6: 100%|| 38/38 [00:13<00:00,  2.85it/s, loss=0.00684]\n",
      "2025-04-19 20:13:32,178 - INFO - Epoch 6/6 - Average loss: 0.0160\n",
      "2025-04-19 20:13:32,178 - INFO - Training completed. Saving final model to results/student_model_solution_cot_final\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "# Fine-tune the student model\n",
    "student_model.load_state_dict(student_initial_state)\n",
    "trained_cot_student_model = start_fine_tuning(student_model, tokenizer, train_cot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "W82IwgDA2BED"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the trained model from files\n",
    "\n",
    "# trained_student_path = f\"results/student_model_{SOLUTION_FIELD}_final\"\n",
    "# trained_student_model = AutoModelForCausalLM.from_pretrained(trained_student_path).to(device)\n",
    "# trained_tokenizer = AutoTokenizer.from_pretrained(trained_student_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "RSpu8jkY2BED",
    "outputId": "07349c76-cbd7-4de1-8e91-1d906073f2ce",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:13:36,892 - INFO - Generating solution_cot with student for 51 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating solution_cot examples using Trained Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:   2%|         | 1/51 [00:02<02:16,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): Rabin-Karp String Manipulation\n",
      "\n",
      "1. Input/output: A string and a character; returns modified string without first and last occurrences\n",
      "2. Approach: Mod...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:   4%|         | 2/51 [00:05<02:13,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): Greedy Algorithm\n",
      "\n",
      "1. Input/output: Given a matrix, return the sorted matrix.\n",
      "2. Approach: Use the Greedy Algorithm to calculate the sum of elements in...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:  18%|        | 9/51 [00:24<01:53,  2.70s/it]2025-04-19 20:14:03,933 - INFO - Generated 10/51 solutions\n",
      "Generating solution_cot:  37%|      | 19/51 [00:51<01:26,  2.70s/it]2025-04-19 20:14:30,944 - INFO - Generated 20/51 solutions\n",
      "Generating solution_cot:  57%|    | 29/51 [01:18<00:59,  2.70s/it]2025-04-19 20:14:57,990 - INFO - Generated 30/51 solutions\n",
      "Generating solution_cot:  76%|  | 39/51 [01:45<00:32,  2.71s/it]2025-04-19 20:15:25,070 - INFO - Generated 40/51 solutions\n",
      "Generating solution_cot:  96%|| 49/51 [02:12<00:05,  2.71s/it]2025-04-19 20:15:52,112 - INFO - Generated 50/51 solutions\n",
      "Generating solution_cot: 100%|| 51/51 [02:17<00:00,  2.70s/it]\n",
      "2025-04-19 20:15:54,832 - INFO - Successfully generated 51 solution_cot solutions\n",
      "2025-04-19 20:15:54,837 - INFO - Dataset saved to dataset/solution_cot_student_51_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Generate fine-tuned student model outputs\n",
    "trained_cot_examples = generate_trained_model_examples(mbpp_test_examples, trained_cot_student_model, tokenizer)\n",
    "\n",
    "# print(\"Evaluating CoT student model...\")\n",
    "# evaluation_results = evaluate_student_model(\n",
    "#     student_model=student_model,\n",
    "#     student_tokenizer=student_tokenizer,\n",
    "#     test_problems=test_problems,\n",
    "#     teacher_model=teacher_model,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     max_length=GENERATED_TOKEN_LEN,\n",
    "#     output_dir=\"results/evaluations\"\n",
    "# )\n",
    "\n",
    "del teacher_model, tokenizer\n",
    "del student_model, student_tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj_ICDvhmYWF",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Coder Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Kp9ioW4XmYWG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:46:55,168 - INFO - Loading teacher model: Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "/home/javvaji.m/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Coder models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:46:56,863 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298938d3114f465d9a0fc4af6423296a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:47:06,777 - INFO - Teacher model loaded successfully\n",
      "2025-04-19 20:47:06,778 - INFO - Loading student model: Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-04-19 20:47:07,033 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-04-19 20:47:07,848 - INFO - Student model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "# CoT Agent Params\n",
    "PROMPT_TEMPLATE = CODER_PROMPT_TEMPLATE\n",
    "SOLUTION_FIELD = \"code\"\n",
    "OUTPUT_MARKER = \"Python code:\"\n",
    "GENERATED_TOKEN_LEN = 512\n",
    "TEMPERATURE=0.7\n",
    "\n",
    "print(\"Loading Coder models...\")\n",
    "code_teacher_model, tokenizer, code_student_model, code_student_tokenizer = load_models(\"Qwen/Qwen2.5-Coder-7B-Instruct\", \"Qwen/Qwen2.5-Coder-0.5B-Instruct\")\n",
    "code_student_initial_state = {k: v.detach().clone() for k, v in code_student_model.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cot_examples_file = open(f\"dataset/solution_cot_teacher_{TEACHER_EXAMPLE_LEN}_dataset.json\")\n",
    "# train_cot_examples = json.load(train_cot_examples_file)\n",
    "\n",
    "# print(train_cot_examples[0])\n",
    "\n",
    "# trained_cot_examples_file = open(f\"dataset/solution_cot_student_{STUDENT_EXAMPLE_LEN+1}_dataset.json\")\n",
    "# trained_cot_examples = json.load(trained_cot_examples_file)\n",
    "\n",
    "# print(trained_cot_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "feu-GFfqmYWG",
    "outputId": "fc3f3460-53b2-49ee-eac6-5ed0dba80fe3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:22:12,289 - INFO - Generating code with teacher for 374 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating code examples using Teacher model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:   0%|          | 1/374 [00:03<23:57,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a function to find the longest chain which can be formed from the given set of pairs....\n",
      "Solution (first 150 chars): ```python\n",
      "def findLongestChain(pairs):\n",
      "    if not pairs:\n",
      "        return 0\n",
      "    \n",
      "    # Sort pairs by their second element\n",
      "    pairs.sort(key=lambda x: x...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:   1%|          | 2/374 [00:05<14:44,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a python function to find the first repeated character in a given string....\n",
      "Solution (first 150 chars): ```python\n",
      "def first_repeated_char(s):\n",
      "    char_dict = {}\n",
      "    for char in s:\n",
      "        if char in char_dict:\n",
      "            return char\n",
      "        else:\n",
      "      ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:   2%|         | 9/374 [00:21<19:43,  3.24s/it]2025-04-19 20:22:35,161 - INFO - Generated 10/374 solutions\n",
      "Generating code:   5%|         | 19/374 [00:55<21:35,  3.65s/it]2025-04-19 20:23:10,569 - INFO - Generated 20/374 solutions\n",
      "Generating code:   8%|         | 29/374 [01:28<16:37,  2.89s/it]2025-04-19 20:23:43,913 - INFO - Generated 30/374 solutions\n",
      "Generating code:  10%|         | 39/374 [02:01<17:28,  3.13s/it]2025-04-19 20:24:15,999 - INFO - Generated 40/374 solutions\n",
      "Generating code:  13%|        | 49/374 [02:36<19:53,  3.67s/it]2025-04-19 20:24:49,998 - INFO - Generated 50/374 solutions\n",
      "Generating code:  16%|        | 59/374 [03:00<10:12,  1.94s/it]2025-04-19 20:25:13,183 - INFO - Generated 60/374 solutions\n",
      "Generating code:  18%|        | 69/374 [03:23<12:13,  2.40s/it]2025-04-19 20:25:36,995 - INFO - Generated 70/374 solutions\n",
      "Generating code:  21%|        | 79/374 [03:42<11:11,  2.28s/it]2025-04-19 20:25:56,861 - INFO - Generated 80/374 solutions\n",
      "Generating code:  24%|       | 89/374 [04:11<09:15,  1.95s/it]2025-04-19 20:26:26,537 - INFO - Generated 90/374 solutions\n",
      "Generating code:  26%|       | 99/374 [04:31<10:23,  2.27s/it]2025-04-19 20:26:49,783 - INFO - Generated 100/374 solutions\n",
      "Generating code:  29%|       | 109/374 [05:04<12:01,  2.72s/it]2025-04-19 20:27:19,478 - INFO - Generated 110/374 solutions\n",
      "Generating code:  32%|      | 119/374 [05:30<08:54,  2.10s/it]2025-04-19 20:27:44,722 - INFO - Generated 120/374 solutions\n",
      "Generating code:  34%|      | 129/374 [05:56<09:25,  2.31s/it]2025-04-19 20:28:10,897 - INFO - Generated 130/374 solutions\n",
      "Generating code:  37%|      | 139/374 [06:25<10:05,  2.58s/it]2025-04-19 20:28:39,454 - INFO - Generated 140/374 solutions\n",
      "Generating code:  40%|      | 149/374 [07:11<19:19,  5.15s/it]2025-04-19 20:29:25,587 - INFO - Generated 150/374 solutions\n",
      "Generating code:  43%|     | 159/374 [07:38<09:10,  2.56s/it]2025-04-19 20:29:51,926 - INFO - Generated 160/374 solutions\n",
      "Generating code:  45%|     | 169/374 [08:07<08:20,  2.44s/it]2025-04-19 20:30:21,310 - INFO - Generated 170/374 solutions\n",
      "Generating code:  48%|     | 179/374 [08:41<07:06,  2.19s/it]2025-04-19 20:30:58,446 - INFO - Generated 180/374 solutions\n",
      "Generating code:  51%|     | 189/374 [09:13<06:50,  2.22s/it]2025-04-19 20:31:27,444 - INFO - Generated 190/374 solutions\n",
      "Generating code:  53%|    | 199/374 [09:37<07:12,  2.47s/it]2025-04-19 20:31:51,053 - INFO - Generated 200/374 solutions\n",
      "Generating code:  56%|    | 209/374 [09:59<06:13,  2.27s/it]2025-04-19 20:32:12,278 - INFO - Generated 210/374 solutions\n",
      "Generating code:  59%|    | 219/374 [10:17<04:27,  1.73s/it]2025-04-19 20:32:31,277 - INFO - Generated 220/374 solutions\n",
      "Generating code:  61%|    | 229/374 [10:35<03:40,  1.52s/it]2025-04-19 20:32:52,088 - INFO - Generated 230/374 solutions\n",
      "Generating code:  64%|   | 239/374 [11:12<10:19,  4.59s/it]2025-04-19 20:33:30,957 - INFO - Generated 240/374 solutions\n",
      "Generating code:  67%|   | 249/374 [11:54<08:13,  3.94s/it]2025-04-19 20:34:08,370 - INFO - Generated 250/374 solutions\n",
      "Generating code:  69%|   | 259/374 [12:17<04:47,  2.50s/it]2025-04-19 20:34:31,753 - INFO - Generated 260/374 solutions\n",
      "Generating code:  72%|  | 269/374 [12:53<06:21,  3.63s/it]2025-04-19 20:35:07,120 - INFO - Generated 270/374 solutions\n",
      "Generating code:  75%|  | 279/374 [13:29<04:00,  2.53s/it]2025-04-19 20:35:43,937 - INFO - Generated 280/374 solutions\n",
      "Generating code:  77%|  | 289/374 [13:47<01:59,  1.41s/it]2025-04-19 20:36:05,476 - INFO - Generated 290/374 solutions\n",
      "Generating code:  80%|  | 299/374 [14:18<03:40,  2.94s/it]2025-04-19 20:36:37,150 - INFO - Generated 300/374 solutions\n",
      "Generating code:  83%| | 309/374 [14:47<02:04,  1.92s/it]2025-04-19 20:37:03,084 - INFO - Generated 310/374 solutions\n",
      "Generating code:  85%| | 319/374 [15:19<02:11,  2.39s/it]2025-04-19 20:37:32,523 - INFO - Generated 320/374 solutions\n",
      "Generating code:  88%| | 329/374 [15:48<02:43,  3.62s/it]2025-04-19 20:38:02,071 - INFO - Generated 330/374 solutions\n",
      "Generating code:  91%| | 339/374 [16:22<02:40,  4.58s/it]2025-04-19 20:38:41,455 - INFO - Generated 340/374 solutions\n",
      "Generating code:  93%|| 349/374 [16:57<01:48,  4.33s/it]2025-04-19 20:39:16,144 - INFO - Generated 350/374 solutions\n",
      "Generating code:  96%|| 359/374 [17:31<00:52,  3.51s/it]2025-04-19 20:39:46,059 - INFO - Generated 360/374 solutions\n",
      "Generating code:  99%|| 369/374 [18:08<00:20,  4.05s/it]2025-04-19 20:40:21,615 - INFO - Generated 370/374 solutions\n",
      "Generating code: 100%|| 374/374 [18:21<00:00,  2.94s/it]\n",
      "2025-04-19 20:40:33,427 - INFO - Successfully generated 374 code solutions\n",
      "2025-04-19 20:40:33,437 - INFO - Dataset saved to dataset/code_teacher_374_dataset.json\n",
      "2025-04-19 20:40:33,438 - INFO - Generating code with student for 50 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating code examples using untrained Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:   2%|         | 1/50 [00:06<04:59,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): ```python\n",
      "def remove_first_and_last_occurrence(s, char):\n",
      "    # Calculate prefix sums array\n",
      "    n = len(s)\n",
      "    prefix_sums = [0] * (n + 1)\n",
      "    \n",
      "    # C...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:   4%|         | 2/50 [00:10<04:11,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): ```python\n",
      "def sort_matrix_by_row_sum(matrix):\n",
      "    \"\"\"\n",
      "    Sorts a given matrix in ascending order based on the sum of its rows.\n",
      "\n",
      "    :param matrix: A ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:  18%|        | 9/50 [00:49<03:32,  5.18s/it]2025-04-19 20:41:26,352 - INFO - Generated 10/50 solutions\n",
      "Generating code:  38%|      | 19/50 [01:40<02:53,  5.59s/it]2025-04-19 20:42:18,630 - INFO - Generated 20/50 solutions\n",
      "Generating code:  58%|    | 29/50 [02:27<01:49,  5.23s/it]2025-04-19 20:43:06,884 - INFO - Generated 30/50 solutions\n",
      "Generating code:  78%|  | 39/50 [03:18<00:51,  4.66s/it]2025-04-19 20:43:57,804 - INFO - Generated 40/50 solutions\n",
      "Generating code:  98%|| 49/50 [04:04<00:03,  3.70s/it]2025-04-19 20:44:41,269 - INFO - Generated 50/50 solutions\n",
      "Generating code: 100%|| 50/50 [04:07<00:00,  4.96s/it]\n",
      "2025-04-19 20:44:41,270 - INFO - Successfully generated 50 code solutions\n",
      "2025-04-19 20:44:41,277 - INFO - Dataset saved to dataset/code_student_50_dataset.json\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "code_student_model.load_state_dict(code_student_initial_state)\n",
    "train_code_examples, untrained_code_examples = generate_base_model_examples(\n",
    "    train_cot_examples,\n",
    "    trained_cot_examples,\n",
    "    code_teacher_model,\n",
    "    code_student_model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "NuwbUK2lmYWG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'Write a function to find the longest chain which can be formed from the given set of pairs.', 'test_case': ['assert max_chain_length([Pair(5, 24), Pair(15, 25),Pair(27, 40), Pair(50, 60)], 4) == 3', 'assert max_chain_length([Pair(1, 2), Pair(3, 4),Pair(5, 6), Pair(7, 8)], 4) == 4', 'assert max_chain_length([Pair(19, 10), Pair(11, 12),Pair(13, 14), Pair(15, 16), Pair(31, 54)], 5) == 5'], 'solution_code': 'class Pair(object): \\r\\n\\tdef __init__(self, a, b): \\r\\n\\t\\tself.a = a \\r\\n\\t\\tself.b = b \\r\\ndef max_chain_length(arr, n): \\r\\n\\tmax = 0\\r\\n\\tmcl = [1 for i in range(n)] \\r\\n\\tfor i in range(1, n): \\r\\n\\t\\tfor j in range(0, i): \\r\\n\\t\\t\\tif (arr[i].a > arr[j].b and\\r\\n\\t\\t\\t\\tmcl[i] < mcl[j] + 1): \\r\\n\\t\\t\\t\\tmcl[i] = mcl[j] + 1\\r\\n\\tfor i in range(n): \\r\\n\\t\\tif (max < mcl[i]): \\r\\n\\t\\t\\tmax = mcl[i] \\r\\n\\treturn max', 'solution_cot': \"Greedy\\n\\n1. Input/output: Given a list of n pairs of integers [(a1, b1), (a2, b2), ..., (an, bn)], return an integer representing the length of the longest chain.\\n2. Approach: Greedy\\n3. Key steps: \\n   - Sort pairs by their second element in ascending order\\n   - Initialize variable 'chains' to 0\\n   - Iterate through sorted pairs, increment 'chains' when current pair's first element is greater than previous pair's second element\\n4. Edge cases: \\n   - Empty list\\n   - All pairs have overlapping intervals\\n   - First pair's second element is less than second pair's first element To find the longest chain from a\", 'code': \"```python\\ndef findLongestChain(pairs):\\n    if not pairs:\\n        return 0\\n    \\n    # Sort pairs by their second element\\n    pairs.sort(key=lambda x: x[1])\\n    \\n    chains = 0\\n    end = float('-inf')\\n    \\n    for start, finish in pairs:\\n        if start > end:\\n            chains += 1\\n            end = finish\\n    \\n    return chains\\n```\\n\\nThis code implements the greedy algorithm strategy to find the longest chain from a given set of pairs. It sorts the pairs by their second element and iterates through them, counting the number of non-overlapping pairs that form the chain. The time complexity is O(n log n) due to sorting, and the space complexity is O(1).\"}\n",
      "{'problem': 'Write a python function to remove first and last occurrence of a given character from the string.', 'test_case': ['assert remove_Occ(\"hello\",\"l\") == \"heo\"', 'assert remove_Occ(\"abcda\",\"a\") == \"bcd\"', 'assert remove_Occ(\"PHP\",\"P\") == \"H\"'], 'solution_code': 'def remove_Occ(s,ch): \\r\\n    for i in range(len(s)): \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    for i in range(len(s) - 1,-1,-1):  \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    return s ', 'solution_cot': 'Rabin-Karp String Manipulation\\n\\n1. Input/output: A string and a character; returns modified string without first and last occurrences\\n2. Approach: Modified Rabin-Karp String Search\\n3. Key steps: \\n   - Compute prefix sums array\\n   - Perform modified Rabin-Karp String Search on each prefix\\n   - Return modified string\\n4. Edge cases: Given string \"abc\", character \\'b\\' appears once before and once after, return \"ac\"\\n5. Complexity: O(n) time, O(1) space\\n6. Note: Modified Rabin-Karp String Search is more efficient than regular search\\n\\n1. Input/output: Given string \"abc\" and character \\'b\\', return modified string \"'}\n"
     ]
    }
   ],
   "source": [
    "# in case of disk/memory filling, this reloads the examples from json\n",
    "\n",
    "clear_gpu_memory()\n",
    "\n",
    "train_code_examples_file = open(f\"dataset/{SOLUTION_FIELD}_teacher_{TEACHER_EXAMPLE_LEN}_dataset.json\")\n",
    "train_code_examples = json.load(train_code_examples_file)\n",
    "\n",
    "print(train_code_examples[0])\n",
    "\n",
    "trained_cot_examples_file = open(f\"dataset/solution_cot_student_{STUDENT_EXAMPLE_LEN+1}_dataset.json\")\n",
    "trained_cot_examples = json.load(trained_cot_examples_file)\n",
    "\n",
    "print(trained_cot_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WiF4P6bSmYWG",
    "outputId": "be67e633-bbab-4e1e-d706-49bcda0b2dc2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:47:08,039 - INFO - Starting training the student model for 6 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning code on Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|| 38/38 [00:41<00:00,  1.09s/it, loss=0.0831]\n",
      "2025-04-19 20:47:49,367 - INFO - Epoch 1/6 - Average loss: 0.5465\n",
      "Epoch 2/6: 100%|| 38/38 [00:40<00:00,  1.07s/it, loss=0.049] \n",
      "2025-04-19 20:48:30,133 - INFO - Epoch 2/6 - Average loss: 0.0738\n",
      "Epoch 3/6:  61%|    | 23/38 [00:26<00:16,  1.09s/it, loss=0.0494]2025-04-19 20:48:56,258 - INFO - Step 100: loss = 0.0494\n",
      "Epoch 3/6: 100%|| 38/38 [00:40<00:00,  1.07s/it, loss=0.0632]\n",
      "2025-04-19 20:49:10,888 - INFO - Epoch 3/6 - Average loss: 0.0424\n",
      "Epoch 4/6: 100%|| 38/38 [00:40<00:00,  1.07s/it, loss=0.0109]\n",
      "2025-04-19 20:49:51,639 - INFO - Epoch 4/6 - Average loss: 0.0185\n",
      "Epoch 5/6: 100%|| 38/38 [00:40<00:00,  1.07s/it, loss=0.00618]\n",
      "2025-04-19 20:50:32,381 - INFO - Epoch 5/6 - Average loss: 0.0081\n",
      "Epoch 6/6:  24%|       | 9/38 [00:10<00:31,  1.09s/it, loss=0.00406]2025-04-19 20:50:43,265 - INFO - Step 200: loss = 0.0041\n",
      "Epoch 6/6: 100%|| 38/38 [00:40<00:00,  1.07s/it, loss=0.0039] \n",
      "2025-04-19 20:51:13,131 - INFO - Epoch 6/6 - Average loss: 0.0043\n",
      "2025-04-19 20:51:13,131 - INFO - Training completed. Saving final model to results/student_model_code_final\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "# Fine-tune the student model\n",
    "code_student_model.load_state_dict(code_student_initial_state)\n",
    "trained_code_student_model = start_fine_tuning(code_student_model, tokenizer, train_code_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "mH0JEK1kmYWH"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the trained model from files\n",
    "\n",
    "# trained_student_path = f\"results/student_model_{SOLUTION_FIELD}_final\"\n",
    "# trained_student_model = AutoModelForCausalLM.from_pretrained(trained_student_path).to(device)\n",
    "# trained_tokenizer = AutoTokenizer.from_pretrained(trained_student_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "p63Lz1d6mYWH",
    "outputId": "c8109c67-04a6-488c-f116-506182736ddd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:52:18,501 - INFO - Generating code with student for 51 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating code examples using Trained Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:   2%|         | 1/51 [00:02<02:13,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): ```python\n",
      "def remove_char(s, c):\n",
      "    if not s or c not in s:\n",
      "        return s\n",
      "    \n",
      "    n = len(s)\n",
      "    prefix_sum = [0] * (n + 1)\n",
      "    \n",
      "    for i in ran...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:   4%|         | 2/51 [00:04<01:58,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): ```python\n",
      "def sort_matrix_by_row_sum(matrix):\n",
      "    if not matrix:\n",
      "        return []\n",
      "    \n",
      "    n = len(matrix)\n",
      "    result = [[0] * n for _ in range(n)]\n",
      " ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:  18%|        | 9/51 [00:18<01:03,  1.51s/it]2025-04-19 20:52:37,332 - INFO - Generated 10/51 solutions\n",
      "Generating code:  37%|      | 19/51 [00:33<00:57,  1.79s/it]2025-04-19 20:52:55,988 - INFO - Generated 20/51 solutions\n",
      "Generating code:  57%|    | 29/51 [01:01<00:57,  2.61s/it]2025-04-19 20:53:20,990 - INFO - Generated 30/51 solutions\n",
      "Generating code:  76%|  | 39/51 [01:11<00:11,  1.06it/s]2025-04-19 20:53:35,685 - INFO - Generated 40/51 solutions\n",
      "Generating code:  96%|| 49/51 [01:29<00:02,  1.02s/it]2025-04-19 20:53:50,449 - INFO - Generated 50/51 solutions\n",
      "Generating code: 100%|| 51/51 [01:33<00:00,  1.83s/it]\n",
      "2025-04-19 20:53:52,029 - INFO - Successfully generated 51 code solutions\n",
      "2025-04-19 20:53:52,034 - INFO - Dataset saved to dataset/code_student_51_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Generate Code examples using Trained Student model\n",
    "trained_code_examples = generate_trained_model_examples(trained_cot_examples, trained_code_student_model, tokenizer)\n",
    "\n",
    "# print(\"Evaluating CoT student model...\")\n",
    "# evaluation_results = evaluate_student_model(\n",
    "#     student_model=student_model,\n",
    "#     student_tokenizer=student_tokenizer,\n",
    "#     test_problems=test_problems,\n",
    "#     teacher_model=teacher_model,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     max_length=GENERATED_TOKEN_LEN,\n",
    "#     output_dir=\"results/evaluations\"\n",
    "# )\n",
    "\n",
    "del code_teacher_model, tokenizer\n",
    "del code_student_model, code_student_tokenizer\n",
    "torch.cuda.empty_cache()  # Clear CUDA cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHeEWY2In-LU",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Debugger Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "9SLWObWSn-LU",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:54:37,144 - INFO - Loading teacher model: Qwen/Qwen2.5-Coder-7B-Instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Coder models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javvaji.m/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-04-19 20:54:37,549 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30bbe215c204c01a178f2cdf471b9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:54:48,230 - INFO - Teacher model loaded successfully\n",
      "2025-04-19 20:54:48,231 - INFO - Loading student model: Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-04-19 20:54:48,538 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-04-19 20:54:49,249 - INFO - Student model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "# CoT Agent Params\n",
    "PROMPT_TEMPLATE = DEBUGGER_PROMPT_TEMPLATE\n",
    "SOLUTION_FIELD = \"debugged\"\n",
    "OUTPUT_MARKER = \"Debugged Python code:\"\n",
    "GENERATED_TOKEN_LEN = 512\n",
    "TEMPERATURE=0.7\n",
    "\n",
    "print(\"Loading Coder models...\")\n",
    "code_teacher_model, tokenizer, code_student_model, code_student_tokenizer = load_models(\"Qwen/Qwen2.5-Coder-7B-Instruct\", \"Qwen/Qwen2.5-Coder-0.5B-Instruct\")\n",
    "code_student_initial_state = {k: v.detach().clone() for k, v in code_student_model.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "EtoWGgLUn-LV",
    "outputId": "375cf997-c1cf-40c1-9cac-b88e6a23ea11",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 20:54:49,361 - INFO - Generating debugged with teacher for 374 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating debugged examples using Teacher model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged:   0%|          | 1/374 [00:02<13:55,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a function to find the longest chain which can be formed from the given set of pairs....\n",
      "Solution (first 150 chars): ```python\n",
      "def findLongestChain(pairs):\n",
      "    if not pairs:\n",
      "        return 0\n",
      "    \n",
      "    # Sort pairs by their first element\n",
      "    pairs.sort(key=lambda x: x[...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged:   1%|          | 2/374 [00:03<09:58,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a python function to find the first repeated character in a given string....\n",
      "Solution (first 150 chars): ```python\n",
      "def first_repeated_char(s):\n",
      "    char_dict = set()\n",
      "    for char in s:\n",
      "        if char in char_dict:\n",
      "            return char\n",
      "        char_dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged:   2%|         | 9/374 [00:19<16:24,  2.70s/it]2025-04-19 20:55:09,542 - INFO - Generated 10/374 solutions\n",
      "Generating debugged:   5%|         | 19/374 [00:44<15:55,  2.69s/it]2025-04-19 20:55:37,416 - INFO - Generated 20/374 solutions\n",
      "Generating debugged:   8%|         | 29/374 [01:11<13:56,  2.42s/it]2025-04-19 20:56:03,170 - INFO - Generated 30/374 solutions\n",
      "Generating debugged:  10%|         | 39/374 [01:37<16:03,  2.88s/it]2025-04-19 20:56:28,020 - INFO - Generated 40/374 solutions\n",
      "Generating debugged:  13%|        | 49/374 [02:05<16:43,  3.09s/it]2025-04-19 20:56:55,872 - INFO - Generated 50/374 solutions\n",
      "Generating debugged:  16%|        | 59/374 [02:28<10:42,  2.04s/it]2025-04-19 20:57:19,444 - INFO - Generated 60/374 solutions\n",
      "Generating debugged:  18%|        | 69/374 [02:52<11:54,  2.34s/it]2025-04-19 20:57:42,301 - INFO - Generated 70/374 solutions\n",
      "Generating debugged:  21%|        | 79/374 [03:05<07:10,  1.46s/it]2025-04-19 20:57:56,587 - INFO - Generated 80/374 solutions\n",
      "Generating debugged:  24%|       | 89/374 [03:37<13:53,  2.93s/it]2025-04-19 20:58:28,543 - INFO - Generated 90/374 solutions\n",
      "Generating debugged:  26%|       | 99/374 [03:51<08:08,  1.78s/it]2025-04-19 20:58:47,255 - INFO - Generated 100/374 solutions\n",
      "Generating debugged:  29%|       | 109/374 [04:19<09:16,  2.10s/it]2025-04-19 20:59:10,346 - INFO - Generated 110/374 solutions\n",
      "Generating debugged:  32%|      | 119/374 [04:37<05:39,  1.33s/it]2025-04-19 20:59:28,231 - INFO - Generated 120/374 solutions\n",
      "Generating debugged:  34%|      | 129/374 [04:56<06:06,  1.50s/it]2025-04-19 20:59:47,161 - INFO - Generated 130/374 solutions\n",
      "Generating debugged:  37%|      | 139/374 [05:17<07:38,  1.95s/it]2025-04-19 21:00:08,031 - INFO - Generated 140/374 solutions\n",
      "Generating debugged:  40%|      | 149/374 [05:54<16:25,  4.38s/it]2025-04-19 21:00:45,891 - INFO - Generated 150/374 solutions\n",
      "Generating debugged:  43%|     | 159/374 [06:15<06:00,  1.68s/it]2025-04-19 21:01:05,381 - INFO - Generated 160/374 solutions\n",
      "Generating debugged:  45%|     | 169/374 [06:33<05:52,  1.72s/it]2025-04-19 21:01:23,847 - INFO - Generated 170/374 solutions\n",
      "Generating debugged:  48%|     | 179/374 [07:02<05:58,  1.84s/it]2025-04-19 21:01:56,163 - INFO - Generated 180/374 solutions\n",
      "Generating debugged:  51%|     | 189/374 [07:32<06:07,  1.99s/it]2025-04-19 21:02:22,563 - INFO - Generated 190/374 solutions\n",
      "Generating debugged:  53%|    | 199/374 [07:52<06:36,  2.27s/it]2025-04-19 21:02:42,349 - INFO - Generated 200/374 solutions\n",
      "Generating debugged:  56%|    | 209/374 [08:09<04:59,  1.82s/it]2025-04-19 21:03:00,184 - INFO - Generated 210/374 solutions\n",
      "Generating debugged:  59%|    | 219/374 [08:25<04:06,  1.59s/it]2025-04-19 21:03:16,201 - INFO - Generated 220/374 solutions\n",
      "Generating debugged:  61%|    | 229/374 [08:42<04:52,  2.02s/it]2025-04-19 21:03:33,477 - INFO - Generated 230/374 solutions\n",
      "Generating debugged:  64%|   | 239/374 [09:13<09:22,  4.17s/it]2025-04-19 21:04:09,110 - INFO - Generated 240/374 solutions\n",
      "Generating debugged:  67%|   | 249/374 [09:51<07:43,  3.71s/it]2025-04-19 21:04:41,560 - INFO - Generated 250/374 solutions\n",
      "Generating debugged:  69%|   | 259/374 [10:08<03:12,  1.68s/it]2025-04-19 21:04:59,967 - INFO - Generated 260/374 solutions\n",
      "Generating debugged:  72%|  | 269/374 [10:37<04:06,  2.35s/it]2025-04-19 21:05:28,428 - INFO - Generated 270/374 solutions\n",
      "Generating debugged:  75%|  | 279/374 [11:10<03:14,  2.05s/it]2025-04-19 21:06:02,233 - INFO - Generated 280/374 solutions\n",
      "Generating debugged:  77%|  | 289/374 [11:27<01:45,  1.25s/it]2025-04-19 21:06:18,935 - INFO - Generated 290/374 solutions\n",
      "Generating debugged:  80%|  | 299/374 [11:49<02:53,  2.31s/it]2025-04-19 21:06:45,306 - INFO - Generated 300/374 solutions\n",
      "Generating debugged:  83%| | 309/374 [12:16<02:17,  2.12s/it]2025-04-19 21:07:10,721 - INFO - Generated 310/374 solutions\n",
      "Generating debugged:  85%| | 319/374 [12:45<02:01,  2.20s/it]2025-04-19 21:07:36,212 - INFO - Generated 320/374 solutions\n",
      "Generating debugged:  88%| | 329/374 [13:07<01:32,  2.05s/it]2025-04-19 21:07:58,091 - INFO - Generated 330/374 solutions\n",
      "Generating debugged:  91%| | 339/374 [13:35<02:08,  3.69s/it]2025-04-19 21:08:32,113 - INFO - Generated 340/374 solutions\n",
      "Generating debugged:  93%|| 349/374 [14:05<01:23,  3.32s/it]2025-04-19 21:08:58,429 - INFO - Generated 350/374 solutions\n",
      "Generating debugged:  96%|| 359/374 [14:32<00:41,  2.74s/it]2025-04-19 21:09:24,486 - INFO - Generated 360/374 solutions\n",
      "Generating debugged:  99%|| 369/374 [14:58<00:15,  3.03s/it]2025-04-19 21:09:48,774 - INFO - Generated 370/374 solutions\n",
      "Generating debugged: 100%|| 374/374 [15:09<00:00,  2.43s/it]\n",
      "2025-04-19 21:09:58,599 - INFO - Successfully generated 374 debugged solutions\n",
      "2025-04-19 21:09:58,614 - INFO - Dataset saved to dataset/debugged_teacher_374_dataset.json\n",
      "2025-04-19 21:09:58,615 - INFO - Generating debugged with student for 50 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating debugged examples using untrained Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged:   2%|         | 1/50 [00:05<04:52,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): ```python\n",
      "def remove_char(s, c):\n",
      "    if not s or c not in s:\n",
      "        return s\n",
      "    \n",
      "    n = len(s)\n",
      "    prefix_sum = [0] * (n + 1)\n",
      "    \n",
      "    for i in ran...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged:   4%|         | 2/50 [00:10<04:02,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): ```python\n",
      "def sort_matrix_by_row_sum(matrix):\n",
      "    if not matrix:\n",
      "        return []\n",
      "    \n",
      "    n = len(matrix)\n",
      "    result = [[0] * n for _ in range(n)]\n",
      " ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged:  18%|        | 9/50 [00:51<03:21,  4.92s/it]2025-04-19 21:10:59,494 - INFO - Generated 10/50 solutions\n",
      "Generating debugged:  38%|      | 19/50 [01:55<02:58,  5.77s/it]2025-04-19 21:11:59,278 - INFO - Generated 20/50 solutions\n",
      "Generating debugged:  58%|    | 29/50 [03:00<02:16,  6.50s/it]2025-04-19 21:13:03,809 - INFO - Generated 30/50 solutions\n",
      "Generating debugged:  78%|  | 39/50 [03:58<00:56,  5.15s/it]2025-04-19 21:14:03,557 - INFO - Generated 40/50 solutions\n",
      "Generating debugged:  98%|| 49/50 [04:58<00:05,  5.16s/it]2025-04-19 21:15:01,337 - INFO - Generated 50/50 solutions\n",
      "Generating debugged: 100%|| 50/50 [05:02<00:00,  6.05s/it]\n",
      "2025-04-19 21:15:01,338 - INFO - Successfully generated 50 debugged solutions\n",
      "2025-04-19 21:15:01,344 - INFO - Dataset saved to dataset/debugged_student_50_dataset.json\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "code_student_model.load_state_dict(code_student_initial_state)\n",
    "train_debug_examples, untrained_debug_examples = generate_base_model_examples(\n",
    "    train_code_examples,\n",
    "    trained_code_examples,\n",
    "    code_teacher_model,\n",
    "    code_student_model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-UAHuDYfn-LV"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the examples from json\n",
    "\n",
    "# clear_gpu_memory()\n",
    "\n",
    "# train_debug_examples_file = open(f\"dataset/{SOLUTION_FIELD}_teacher_{TEACHER_EXAMPLE_LEN}_dataset.json\")\n",
    "# train_debug_examples = json.load(train_debug_examples_file)\n",
    "\n",
    "# print(train_debug_examples[0])\n",
    "\n",
    "# trained_code_examples_file = open(f\"dataset/code_student_{STUDENT_EXAMPLE_LEN+1}_dataset.json\")\n",
    "# trained_code_examples = json.load(trained_code_examples_file)\n",
    "\n",
    "# print(trained_code_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3jb3TN5In-LV",
    "outputId": "ff5fdfd0-6737-48f3-f629-b6cea2aae8f2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 21:15:01,480 - INFO - Starting training the student model for 6 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning debugged on Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|| 38/38 [00:40<00:00,  1.07s/it, loss=0.0512]\n",
      "2025-04-19 21:15:42,229 - INFO - Epoch 1/6 - Average loss: 0.4451\n",
      "Epoch 2/6: 100%|| 38/38 [00:40<00:00,  1.07s/it, loss=0.0163] \n",
      "2025-04-19 21:16:22,945 - INFO - Epoch 2/6 - Average loss: 0.0184\n",
      "Epoch 3/6:  61%|    | 23/38 [00:26<00:16,  1.09s/it, loss=0.00528]2025-04-19 21:16:49,037 - INFO - Step 100: loss = 0.0053\n",
      "Epoch 3/6: 100%|| 38/38 [00:40<00:00,  1.07s/it, loss=0.00666]\n",
      "2025-04-19 21:17:03,656 - INFO - Epoch 3/6 - Average loss: 0.0112\n",
      "Epoch 4/6: 100%|| 38/38 [00:40<00:00,  1.07s/it, loss=0.000419]\n",
      "2025-04-19 21:17:44,359 - INFO - Epoch 4/6 - Average loss: 0.0051\n",
      "Epoch 5/6: 100%|| 38/38 [00:40<00:00,  1.07s/it, loss=0.00175] \n",
      "2025-04-19 21:18:25,059 - INFO - Epoch 5/6 - Average loss: 0.0025\n",
      "Epoch 6/6:  24%|       | 9/38 [00:10<00:31,  1.09s/it, loss=0.000824]2025-04-19 21:18:35,929 - INFO - Step 200: loss = 0.0008\n",
      "Epoch 6/6: 100%|| 38/38 [00:40<00:00,  1.07s/it, loss=0.000183]\n",
      "2025-04-19 21:19:05,761 - INFO - Epoch 6/6 - Average loss: 0.0018\n",
      "2025-04-19 21:19:05,761 - INFO - Training completed. Saving final model to results/student_model_debugged_final\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "# Fine-tune the student model\n",
    "code_student_model.load_state_dict(code_student_initial_state)\n",
    "trained_debug_student_model = start_fine_tuning(code_student_model, tokenizer, train_debug_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "56nI0JqUn-LW"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the trained model from files\n",
    "\n",
    "# trained_student_path = f\"results/student_model_{SOLUTION_FIELD}_final\"\n",
    "# trained_student_model = AutoModelForCausalLM.from_pretrained(trained_student_path).to(device)\n",
    "# trained_tokenizer = AutoTokenizer.from_pretrained(trained_student_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Wyl_u_DHn-LW",
    "outputId": "584be62e-2209-4cbf-c3b3-fbb943f70333"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 21:19:10,216 - INFO - Generating debugged with student for 51 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating debugged examples using Trained Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged:   2%|         | 1/51 [00:02<02:12,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): ```python\n",
      "def remove_char(s, c):\n",
      "    if not s or c not in s:\n",
      "        return s\n",
      "    \n",
      "    n = len(s)\n",
      "    prefix_sum = [0] * (n + 1)\n",
      "    \n",
      "    for i in ran...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged:   4%|         | 2/51 [00:04<01:38,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): ```python\n",
      "def sort_matrix_by_row_sum(matrix):\n",
      "    if not matrix:\n",
      "        return []\n",
      "    \n",
      "    n = len(matrix)\n",
      "    result = [[0] * n for _ in range(n)]\n",
      " ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged:  18%|        | 9/51 [00:17<01:03,  1.51s/it]2025-04-19 21:19:28,461 - INFO - Generated 10/51 solutions\n",
      "Generating debugged:  37%|      | 19/51 [00:30<00:50,  1.57s/it]2025-04-19 21:19:44,497 - INFO - Generated 20/51 solutions\n",
      "Generating debugged:  57%|    | 29/51 [00:56<00:53,  2.44s/it]2025-04-19 21:20:07,553 - INFO - Generated 30/51 solutions\n",
      "Generating debugged:  76%|  | 39/51 [01:05<00:11,  1.08it/s]2025-04-19 21:20:21,553 - INFO - Generated 40/51 solutions\n",
      "Generating debugged:  96%|| 49/51 [01:22<00:01,  1.03it/s]2025-04-19 21:20:35,149 - INFO - Generated 50/51 solutions\n",
      "Generating debugged: 100%|| 51/51 [01:26<00:00,  1.70s/it]\n",
      "2025-04-19 21:20:36,749 - INFO - Successfully generated 51 debugged solutions\n",
      "2025-04-19 21:20:36,755 - INFO - Dataset saved to dataset/debugged_student_51_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Generate debug examples using Trained Student model...\n",
    "trained_debug_examples = generate_trained_model_examples(trained_code_examples, trained_debug_student_model, tokenizer)\n",
    "\n",
    "# print(\"Evaluating CoT student model...\")\n",
    "# evaluation_results = evaluate_student_model(\n",
    "#     student_model=student_model,\n",
    "#     student_tokenizer=student_tokenizer,\n",
    "#     test_problems=test_problems,\n",
    "#     teacher_model=teacher_model,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     max_length=GENERATED_TOKEN_LEN,\n",
    "#     output_dir=\"results/evaluations\"\n",
    "# )\n",
    "\n",
    "del code_teacher_model, tokenizer\n",
    "del code_student_model, code_student_tokenizer\n",
    "torch.cuda.empty_cache()  # Clear CUDA cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5rBXu9_qgqh",
    "tags": []
   },
   "source": [
    "## Explainer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TuMJYGSaqgqh",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 09:46:39,963 - INFO - Loading teacher model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Instruct models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javvaji.m/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-04-20 09:46:43,243 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82dc37a3ed6c4f57bd81e576d62159d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 09:46:57,162 - INFO - Teacher model loaded successfully\n",
      "2025-04-20 09:46:57,162 - INFO - Loading student model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-04-20 09:46:57,424 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-04-20 09:46:58,420 - INFO - Student model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "# CoT Agent Params\n",
    "PROMPT_TEMPLATE = EXPLAINER_PROMPT_TEMPLATE\n",
    "SOLUTION_FIELD = \"explanation\"\n",
    "OUTPUT_MARKER = \"Python code explanation:\"\n",
    "GENERATED_TOKEN_LEN = 512\n",
    "TEMPERATURE=0.7\n",
    "\n",
    "print(\"Loading Instruct models...\")\n",
    "teacher_model, tokenizer, student_model, student_tokenizer = load_models(\"Qwen/Qwen2.5-7B-Instruct\", \"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "student_initial_state = {k: v.detach().clone() for k, v in student_model.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "aeGsGNoUqgqi",
    "outputId": "4afd8214-61de-4935-8f08-88afa9db2436",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 21:22:55,635 - INFO - Generating explanation with teacher for 374 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating explanation examples using Teacher model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanation:   0%|          | 1/374 [00:11<1:09:06, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a function to find the longest chain which can be formed from the given set of pairs....\n",
      "Solution (first 150 chars): This Python function solves the problem of finding the longest chain that can be formed from a given set of pairs. It uses a greedy algorithm approach...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanation:   1%|          | 2/374 [00:22<1:10:13, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a python function to find the first repeated character in a given string....\n",
      "Solution (first 150 chars): The code defines a Python function called `first_repeated_char` which takes a string `s` as input. It aims to find and return the first character that...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanation:   2%|         | 9/374 [01:37<1:06:29, 10.93s/it]2025-04-19 21:24:45,641 - INFO - Generated 10/374 solutions\n",
      "Generating explanation:   5%|         | 19/374 [03:27<1:08:54, 11.65s/it]2025-04-19 21:26:35,444 - INFO - Generated 20/374 solutions\n",
      "Generating explanation:   8%|         | 29/374 [05:04<53:43,  9.34s/it]  2025-04-19 21:28:07,961 - INFO - Generated 30/374 solutions\n",
      "Generating explanation:  10%|         | 39/374 [06:44<59:21, 10.63s/it]  2025-04-19 21:29:52,675 - INFO - Generated 40/374 solutions\n",
      "Generating explanation:  13%|        | 49/374 [08:28<49:57,  9.22s/it]  2025-04-19 21:31:35,410 - INFO - Generated 50/374 solutions\n",
      "Generating explanation:  16%|        | 59/374 [09:59<50:36,  9.64s/it]2025-04-19 21:33:05,212 - INFO - Generated 60/374 solutions\n",
      "Generating explanation:  18%|        | 69/374 [11:36<49:27,  9.73s/it]2025-04-19 21:34:39,757 - INFO - Generated 70/374 solutions\n",
      "Generating explanation:  21%|        | 79/374 [13:11<45:16,  9.21s/it]2025-04-19 21:36:19,591 - INFO - Generated 80/374 solutions\n",
      "Generating explanation:  24%|       | 89/374 [14:49<49:57, 10.52s/it]2025-04-19 21:37:51,316 - INFO - Generated 90/374 solutions\n",
      "Generating explanation:  26%|       | 99/374 [16:30<48:32, 10.59s/it]2025-04-19 21:39:39,063 - INFO - Generated 100/374 solutions\n",
      "Generating explanation:  29%|       | 109/374 [18:10<48:39, 11.02s/it]2025-04-19 21:41:16,516 - INFO - Generated 110/374 solutions\n",
      "Generating explanation:  32%|      | 119/374 [19:47<39:31,  9.30s/it]2025-04-19 21:42:50,815 - INFO - Generated 120/374 solutions\n",
      "Generating explanation:  34%|      | 129/374 [21:32<45:07, 11.05s/it]2025-04-19 21:44:37,611 - INFO - Generated 130/374 solutions\n",
      "Generating explanation:  37%|      | 139/374 [23:11<38:52,  9.92s/it]2025-04-19 21:46:17,440 - INFO - Generated 140/374 solutions\n",
      "Generating explanation:  40%|      | 149/374 [24:57<42:39, 11.38s/it]2025-04-19 21:47:59,816 - INFO - Generated 150/374 solutions\n",
      "Generating explanation:  43%|     | 159/374 [26:39<38:42, 10.80s/it]2025-04-19 21:49:42,957 - INFO - Generated 160/374 solutions\n",
      "Generating explanation:  45%|     | 169/374 [28:17<30:41,  8.98s/it]2025-04-19 21:51:24,866 - INFO - Generated 170/374 solutions\n",
      "Generating explanation:  48%|     | 179/374 [29:49<26:50,  8.26s/it]2025-04-19 21:52:56,991 - INFO - Generated 180/374 solutions\n",
      "Generating explanation:  51%|     | 189/374 [31:27<30:54, 10.03s/it]2025-04-19 21:54:32,968 - INFO - Generated 190/374 solutions\n",
      "Generating explanation:  53%|    | 199/374 [33:00<29:34, 10.14s/it]2025-04-19 21:56:08,753 - INFO - Generated 200/374 solutions\n",
      "Generating explanation:  59%|    | 219/374 [36:22<25:15,  9.78s/it]2025-04-19 21:59:26,116 - INFO - Generated 220/374 solutions\n",
      "Generating explanation:  61%|    | 229/374 [38:05<27:07, 11.23s/it]2025-04-19 22:01:13,284 - INFO - Generated 230/374 solutions\n",
      "Generating explanation:  64%|   | 239/374 [39:43<23:14, 10.33s/it]2025-04-19 22:02:49,480 - INFO - Generated 240/374 solutions\n",
      "Generating explanation:  67%|   | 249/374 [41:20<19:35,  9.40s/it]2025-04-19 22:04:27,652 - INFO - Generated 250/374 solutions\n",
      "Generating explanation:  69%|   | 259/374 [43:01<18:08,  9.46s/it]2025-04-19 22:06:08,453 - INFO - Generated 260/374 solutions\n",
      "Generating explanation:  72%|  | 269/374 [44:39<15:01,  8.58s/it]2025-04-19 22:07:45,104 - INFO - Generated 270/374 solutions\n",
      "Generating explanation:  75%|  | 279/374 [46:27<18:12, 11.50s/it]2025-04-19 22:09:35,731 - INFO - Generated 280/374 solutions\n",
      "Generating explanation:  77%|  | 289/374 [47:54<10:07,  7.14s/it]2025-04-19 22:11:02,490 - INFO - Generated 290/374 solutions\n",
      "Generating explanation:  80%|  | 299/374 [49:32<11:17,  9.03s/it]2025-04-19 22:12:34,471 - INFO - Generated 300/374 solutions\n",
      "Generating explanation:  83%| | 309/374 [51:15<12:28, 11.51s/it]2025-04-19 22:14:18,120 - INFO - Generated 310/374 solutions\n",
      "Generating explanation:  85%| | 319/374 [53:06<10:09, 11.09s/it]2025-04-19 22:16:11,346 - INFO - Generated 320/374 solutions\n",
      "Generating explanation:  88%| | 329/374 [54:53<08:28, 11.30s/it]2025-04-19 22:18:01,467 - INFO - Generated 330/374 solutions\n",
      "Generating explanation:  91%| | 339/374 [56:43<05:43,  9.83s/it]2025-04-19 22:19:47,086 - INFO - Generated 340/374 solutions\n",
      "Generating explanation:  93%|| 349/374 [58:20<04:32, 10.90s/it]2025-04-19 22:21:26,673 - INFO - Generated 350/374 solutions\n",
      "Generating explanation:  96%|| 359/374 [1:00:01<02:15,  9.06s/it]2025-04-19 22:23:08,146 - INFO - Generated 360/374 solutions\n",
      "Generating explanation:  99%|| 369/374 [1:02:01<00:59, 11.87s/it]2025-04-19 22:25:04,428 - INFO - Generated 370/374 solutions\n",
      "Generating explanation: 100%|| 374/374 [1:02:44<00:00, 10.07s/it]\n",
      "2025-04-19 22:25:40,613 - INFO - Successfully generated 374 explanation solutions\n",
      "2025-04-19 22:25:40,629 - INFO - Dataset saved to dataset/explanation_teacher_374_dataset.json\n",
      "2025-04-19 22:25:40,630 - INFO - Generating explanation with student for 50 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating explanation examples using untrained Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanation:   2%|         | 1/50 [00:04<03:58,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): The Python function `remove_char` takes two arguments: `s`, which is the input string, and `c`, which is the character to be removed. It returns a new...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanation:   4%|         | 2/50 [00:12<05:10,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): The provided Python function `sort_matrix_by_row_sum` is designed to sort an input list or 2D numpy array based on the total value of elements in each...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanation:  18%|        | 9/50 [00:53<03:39,  5.36s/it]2025-04-19 22:26:41,125 - INFO - Generated 10/50 solutions\n",
      "Generating explanation:  38%|      | 19/50 [01:50<02:39,  5.13s/it]2025-04-19 22:27:39,666 - INFO - Generated 20/50 solutions\n",
      "Generating explanation:  58%|    | 29/50 [02:43<01:51,  5.31s/it]2025-04-19 22:28:28,335 - INFO - Generated 30/50 solutions\n",
      "Generating explanation:  78%|  | 39/50 [03:50<01:10,  6.39s/it]2025-04-19 22:29:35,137 - INFO - Generated 40/50 solutions\n",
      "Generating explanation:  98%|| 49/50 [04:48<00:06,  6.67s/it]2025-04-19 22:30:35,163 - INFO - Generated 50/50 solutions\n",
      "Generating explanation: 100%|| 50/50 [04:54<00:00,  5.89s/it]\n",
      "2025-04-19 22:30:35,164 - INFO - Successfully generated 50 explanation solutions\n",
      "2025-04-19 22:30:35,172 - INFO - Dataset saved to dataset/explanation_student_50_dataset.json\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "#TODO: concat debug code examples to the train code examples(?)\n",
    "student_model.load_state_dict(student_initial_state)\n",
    "train_explain_examples, untrained_explain_examples = generate_base_model_examples(\n",
    "    train_code_examples,\n",
    "    trained_code_examples,\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 'Write a function to find the longest chain which can be formed from the given set of pairs.', 'test_case': ['assert max_chain_length([Pair(5, 24), Pair(15, 25),Pair(27, 40), Pair(50, 60)], 4) == 3', 'assert max_chain_length([Pair(1, 2), Pair(3, 4),Pair(5, 6), Pair(7, 8)], 4) == 4', 'assert max_chain_length([Pair(19, 10), Pair(11, 12),Pair(13, 14), Pair(15, 16), Pair(31, 54)], 5) == 5'], 'solution_code': 'class Pair(object): \\r\\n\\tdef __init__(self, a, b): \\r\\n\\t\\tself.a = a \\r\\n\\t\\tself.b = b \\r\\ndef max_chain_length(arr, n): \\r\\n\\tmax = 0\\r\\n\\tmcl = [1 for i in range(n)] \\r\\n\\tfor i in range(1, n): \\r\\n\\t\\tfor j in range(0, i): \\r\\n\\t\\t\\tif (arr[i].a > arr[j].b and\\r\\n\\t\\t\\t\\tmcl[i] < mcl[j] + 1): \\r\\n\\t\\t\\t\\tmcl[i] = mcl[j] + 1\\r\\n\\tfor i in range(n): \\r\\n\\t\\tif (max < mcl[i]): \\r\\n\\t\\t\\tmax = mcl[i] \\r\\n\\treturn max', 'solution_cot': \"Greedy\\n\\n1. Input/output: Given a list of n pairs of integers [(a1, b1), (a2, b2), ..., (an, bn)], return an integer representing the length of the longest chain.\\n2. Approach: Greedy\\n3. Key steps: \\n   - Sort pairs by their second element in ascending order\\n   - Initialize variable 'chains' to 0\\n   - Iterate through sorted pairs, increment 'chains' when current pair's first element is greater than previous pair's second element\\n4. Edge cases: \\n   - Empty list\\n   - All pairs have overlapping intervals\\n   - First pair's second element is less than second pair's first element To find the longest chain from a\", 'code': \"```python\\ndef findLongestChain(pairs):\\n    if not pairs:\\n        return 0\\n    \\n    # Sort pairs by their second element\\n    pairs.sort(key=lambda x: x[1])\\n    \\n    chains = 0\\n    end = float('-inf')\\n    \\n    for start, finish in pairs:\\n        if start > end:\\n            chains += 1\\n            end = finish\\n    \\n    return chains\\n```\\n\\nThis code implements the greedy algorithm strategy to find the longest chain from a given set of pairs. It sorts the pairs by their second element and iterates through them, counting the number of non-overlapping pairs that form the chain. The time complexity is O(n log n) due to sorting, and the space complexity is O(1).\", 'explanation': \"This Python function solves the problem of finding the longest chain that can be formed from a given set of pairs. It uses a greedy algorithm approach to sort the pairs by their ending value and then iteratively selects the next pair that starts after the previous one ended, effectively forming the longest possible non-overlapping sequence. By always choosing the earliest ending pair available at each step, the algorithm ensures it maximizes the number of chains that can be formed. A key insight is that sorting by the second element allows us to make locally optimal choices (selecting the next best available ending) that lead to a globally optimal solution (the longest chain). The clever trick here is realizing that this strategy guarantees we never miss out on a potential chain link, as we're always picking the earliest possible end point that fits. This results in an efficient O(n log n) time complexity with constant space usage. The code is concise yet powerful, demonstrating how simple algorithms can elegantly solve seemingly complex problems. ```python\\ndef findLongestChain(pairs):\\n    if not pairs:\\n        return 0\\n    \\n    # Sort pairs by their second element\\n    pairs.sort(key=lambda x: x[1])\\n    \\n    chains = 0\\n    end = float('-inf')\\n    \\n    for start, finish in pairs:\\n        if start > end:\\n            chains += 1\\n            end = finish\\n    \\n    return chains\\n```\\n\\nThis function takes a list of pairs and returns the length of the longest chain that can be formed where each subsequent pair's start value is greater than the end value of the previous pair. It does so by first sorting the pairs based on their end values. Then, it iterates through the sorted pairs, selecting pairs whose start value is greater than the current end value, thereby forming a chain. This greedy approach ensures that the selected pairs are as early as possible, allowing for the formation of the longest possible chain. The function efficiently finds the solution in O(n log n) time due to the sorting step, with O(1) space complexity since it only uses a few extra variables. The insight behind this approach is that by prioritizing pairs that end earlier, we maximize the number of pairs we can potentially add to our chain, ensuring we get the longest possible chain. ```\"}\n",
      "{'problem': 'Write a python function to remove first and last occurrence of a given character from the string.', 'test_case': ['assert remove_Occ(\"hello\",\"l\") == \"heo\"', 'assert remove_Occ(\"abcda\",\"a\") == \"bcd\"', 'assert remove_Occ(\"PHP\",\"P\") == \"H\"'], 'solution_code': 'def remove_Occ(s,ch): \\r\\n    for i in range(len(s)): \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    for i in range(len(s) - 1,-1,-1):  \\r\\n        if (s[i] == ch): \\r\\n            s = s[0 : i] + s[i + 1:] \\r\\n            break\\r\\n    return s ', 'solution_cot': 'Rabin-Karp String Manipulation\\n\\n1. Input/output: A string and a character; returns modified string without first and last occurrences\\n2. Approach: Modified Rabin-Karp String Search\\n3. Key steps: \\n   - Compute prefix sums array\\n   - Perform modified Rabin-Karp String Search on each prefix\\n   - Return modified string\\n4. Edge cases: Given string \"abc\", character \\'b\\' appears once before and once after, return \"ac\"\\n5. Complexity: O(n) time, O(1) space\\n6. Note: Modified Rabin-Karp String Search is more efficient than regular search\\n\\n1. Input/output: Given string \"abc\" and character \\'b\\', return modified string \"', 'code': '```python\\ndef remove_char(s, c):\\n    if not s or c not in s:\\n        return s\\n    \\n    n = len(s)\\n    prefix_sum = [0] * (n + 1)\\n    \\n    for i in range(1, n + 1):\\n        prefix_sum[i] = prefix_sum[i - 1] * 10 + ord(s[i - 1])\\n    \\n    result = []\\n    for char in s:\\n        if char == c:\\n            result.append(prefix_sum[char])\\n            prefix_sum[char] -= ord(char)\\n    \\n    return \\'\\'.join(result)\\n\\n# Test case\\nprint(remove_char(\"abc\", \"b\"))  # Output: \"ac\"\\n```\\n```'}\n"
     ]
    }
   ],
   "source": [
    "# # in case of disk/memory filling, this reloads the examples from json\n",
    "\n",
    "# clear_gpu_memory()\n",
    "\n",
    "# train_explain_examples_file = open(f\"dataset/{SOLUTION_FIELD}_teacher_{TEACHER_EXAMPLE_LEN}_dataset.json\")\n",
    "# train_explain_examples = json.load(train_explain_examples_file)\n",
    "\n",
    "# print(train_explain_examples[0])\n",
    "\n",
    "# trained_code_examples_file = open(f\"dataset/code_student_{STUDENT_EXAMPLE_LEN+1}_dataset.json\")\n",
    "# trained_code_examples = json.load(trained_code_examples_file)\n",
    "\n",
    "# print(trained_code_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "k5Wsi2sGqgqi",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 09:49:22,180 - INFO - Starting training the student model for 6 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning explanation on Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|| 38/38 [00:41<00:00,  1.10s/it, loss=1.04] \n",
      "2025-04-20 09:50:03,901 - INFO - Epoch 1/6 - Average loss: 1.1583\n",
      "Epoch 2/6: 100%|| 38/38 [00:41<00:00,  1.08s/it, loss=0.791]\n",
      "2025-04-20 09:50:44,973 - INFO - Epoch 2/6 - Average loss: 0.6859\n",
      "Epoch 3/6:  61%|    | 23/38 [00:26<00:16,  1.10s/it, loss=0.351]2025-04-20 09:51:11,280 - INFO - Step 100: loss = 0.3514\n",
      "Epoch 3/6: 100%|| 38/38 [00:41<00:00,  1.08s/it, loss=0.286]\n",
      "2025-04-20 09:51:26,045 - INFO - Epoch 3/6 - Average loss: 0.3454\n",
      "Epoch 4/6: 100%|| 38/38 [00:41<00:00,  1.08s/it, loss=0.0932]\n",
      "2025-04-20 09:52:07,062 - INFO - Epoch 4/6 - Average loss: 0.1563\n",
      "Epoch 5/6: 100%|| 38/38 [00:41<00:00,  1.08s/it, loss=0.0517]\n",
      "2025-04-20 09:52:48,090 - INFO - Epoch 5/6 - Average loss: 0.0658\n",
      "Epoch 6/6:  24%|       | 9/38 [00:10<00:31,  1.09s/it, loss=0.0397]2025-04-20 09:52:59,034 - INFO - Step 200: loss = 0.0397\n",
      "Epoch 6/6: 100%|| 38/38 [00:40<00:00,  1.08s/it, loss=0.0276]\n",
      "2025-04-20 09:53:29,034 - INFO - Epoch 6/6 - Average loss: 0.0345\n",
      "2025-04-20 09:53:29,035 - INFO - Training completed. Saving final model to results/student_model_explanation_final\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "# Fine-tune the student model\n",
    "student_model.load_state_dict(student_initial_state)\n",
    "trained_explain_student_model = start_fine_tuning(student_model, tokenizer, train_explain_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cgCdYmvJqgqi"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the trained model from files\n",
    "\n",
    "# trained_student_path = f\"results/student_model_{SOLUTION_FIELD}_final\"\n",
    "# trained_student_model = AutoModelForCausalLM.from_pretrained(trained_student_path).to(device)\n",
    "# trained_tokenizer = AutoTokenizer.from_pretrained(trained_student_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "j4E2fK3Vqgqi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 10:06:40,801 - INFO - Generating explanation with student for 51 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating explanation examples using Trained Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanation:   2%|         | 1/51 [00:09<07:37,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): This Python function `remove_char` removes all occurrences of a specified character from a given string. It uses a prefix sum array to efficiently cal...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanation:   4%|         | 2/51 [00:18<07:24,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): This Python function sorts a given matrix (list of lists) based on the sum of its rows. Instead of sorting directly, which would involve comparing eve...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanation:  18%|        | 9/51 [01:06<05:10,  7.39s/it]2025-04-20 10:07:55,052 - INFO - Generated 10/51 solutions\n",
      "Generating explanation:  37%|      | 19/51 [02:24<04:11,  7.87s/it]2025-04-20 10:09:09,804 - INFO - Generated 20/51 solutions\n",
      "Generating explanation:  57%|    | 29/51 [03:34<02:41,  7.34s/it]2025-04-20 10:10:18,739 - INFO - Generated 30/51 solutions\n",
      "Generating explanation:  76%|  | 39/51 [04:41<01:16,  6.38s/it]2025-04-20 10:11:27,776 - INFO - Generated 40/51 solutions\n",
      "Generating explanation:  96%|| 49/51 [05:48<00:14,  7.31s/it]2025-04-20 10:12:35,384 - INFO - Generated 50/51 solutions\n",
      "Generating explanation: 100%|| 51/51 [06:03<00:00,  7.13s/it]\n",
      "2025-04-20 10:12:44,412 - INFO - Successfully generated 51 explanation solutions\n",
      "2025-04-20 10:12:44,424 - INFO - Dataset saved to dataset/explanation_student_51_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Generate explanation examples using Trained Student model\n",
    "trained_explain_examples = generate_trained_model_examples(trained_code_examples, trained_explain_student_model, tokenizer)\n",
    "\n",
    "# print(\"Evaluating CoT student model...\")\n",
    "# evaluation_results = evaluate_student_model(\n",
    "#     student_model=student_model,\n",
    "#     student_tokenizer=student_tokenizer,\n",
    "#     test_problems=test_problems,\n",
    "#     teacher_model=teacher_model,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     max_length=GENERATED_TOKEN_LEN,\n",
    "#     output_dir=\"results/evaluations\"\n",
    "# )\n",
    "\n",
    "del teacher_model, tokenizer\n",
    "del student_model, student_tokenizer\n",
    "torch.cuda.empty_cache()  # Clear CUDA cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgScz68PAJVB",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yJx5XabAGP7"
   },
   "outputs": [],
   "source": [
    "def extract_problem_description(source_code):\n",
    "    \"\"\"\n",
    "    Extracts the problem description from the first docstring in the source code,\n",
    "    whether it's enclosed in triple double quotes or triple single quotes.\n",
    "    \"\"\"\n",
    "    docstring_pattern = re.compile(r'(\"\"\"|\\'\\'\\')(.*?)(\\1)', re.DOTALL)\n",
    "    match = docstring_pattern.search(source_code)\n",
    "\n",
    "    if match:\n",
    "        description = match.group(2)\n",
    "        # Clean up leading/trailing whitespace on each line\n",
    "        cleaned_lines = [line.strip() for line in description.strip().splitlines() if line.strip()]\n",
    "        return ' '.join(cleaned_lines)\n",
    "\n",
    "    raise Exception(f\"Error: Unable to extract problem description. Please check the format of the prompt:\\n{source_code}\")\n",
    "    return None\n",
    "\n",
    "def extract_code_header(source_code):\n",
    "    \"\"\"\n",
    "    Extracts everything from the beginning of the source code up to\n",
    "    the first occurrence of either triple single quotes or triple double quotes.\n",
    "    \"\"\"\n",
    "    # Match from start of string to the first triple quotes (single or double)\n",
    "    docstring_pattern = re.compile(r'^(.*?)(?=\"\"\"|\\'\\'\\')', re.DOTALL)\n",
    "    match = docstring_pattern.search(source_code)\n",
    "\n",
    "    if match:\n",
    "        header = match.group(1)\n",
    "        # Clean up leading/trailing whitespace on each line\n",
    "        cleaned_lines = [line.strip() for line in header.strip().splitlines() if line.strip()]\n",
    "        return ' '.join(cleaned_lines)\n",
    "    raise Exception(f\"Error: Unable to extract code header. Please check the format of the prompt:\\n{source_code}\")\n",
    "    return None\n",
    "\n",
    "def load_human_eval_dataset():\n",
    "    human_eval = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "    train_problems = []\n",
    "    # Extract problems from the MBPP dataset with correct field names\n",
    "    for item in human_eval[\"test\"]:\n",
    "        train_problems.append({\n",
    "            \"problem\": extract_problem_description(item[\"prompt\"]),\n",
    "            \"code_header\": extract_code_header(item[\"prompt\"]),\n",
    "            \"test_case\": item[\"prompt\"],\n",
    "            \"solution_code\": item[\"prompt\"] + item[\"canonical_solution\"]\n",
    "        })\n",
    "    return train_problems\n",
    "\n",
    "COT_PROMPT_TEMPLATE = \"\"\"Generate a detailed step-by-step solution for this coding problem.\n",
    "Break down your thought process clearly, explaining your reasoning while considering:\n",
    "- What are the inputs and outputs of the function?\n",
    "- What algorithm or data structure is most appropriate?\n",
    "- Are there any edge cases to handle?\n",
    "- What's the efficiency of your approach?\n",
    "\n",
    "Be concise in your explanation.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Step-by-step solution:\"\"\"\n",
    "\n",
    "# CODER_PROMPT_TEMPLATE = \"\"\"Generate only a markdown code block that contains clean, efficient\n",
    "# Python code for this coding problem based on the solution approach. The code block must start\n",
    "# with ```python on its own line, then the code, and end with ``` on its own line.\n",
    "# Focus on:\n",
    "# - Implementing the key algorithmic insights\n",
    "# - Handling edge cases identified in the solution\n",
    "# - Maintaining readability and efficiency\n",
    "# Do not include:\n",
    "# - test cases\n",
    "# - extra code explanation\n",
    "\n",
    "# Step-by-step solution:\n",
    "# {cot_solution}\n",
    "\n",
    "# Python code:\n",
    "# {code_header}\"\"\"\n",
    "\n",
    "CODER_PROMPT_TEMPLATE = \"\"\"Generate only a markdown code block that contains clean, efficient\n",
    "Python code for this coding problem based on the solution approach. The code block must start\n",
    "with ```python on its own line, then the code, and end with ``` on its own line. Do not include\n",
    "test cases or code explanations.\n",
    "Focus on:\n",
    "- Implementing the key algorithmic insights\n",
    "- Handling edge cases identified in the solution\n",
    "- Maintaining readability and efficiency\n",
    "\n",
    "Step-by-step solution:\n",
    "{cot_solution}\n",
    "\n",
    "Python code:\n",
    "{code_header}\"\"\"\n",
    "\n",
    "\n",
    "human_eval = load_human_eval_dataset()\n",
    "print(human_eval[0])\n",
    "\n",
    "print(\"loaded dataset\")\n",
    "\n",
    "trained_cot_student_path = f\"results/student_model_cot_solution_final\"\n",
    "trained_cot_student_model = AutoModelForCausalLM.from_pretrained(trained_cot_student_path).to(device)\n",
    "trained_cot_tokenizer = AutoTokenizer.from_pretrained(trained_cot_student_path)\n",
    "\n",
    "untrained_coder_model_name = \"Qwen/Qwen2.5-Coder-0.5B\"\n",
    "untrained_coder_tokenizer = AutoTokenizer.from_pretrained(untrained_coder_model_name)\n",
    "untrained_coder_model = AutoModelForCausalLM.from_pretrained(\n",
    "    untrained_coder_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "print(\"Loaded models\")\n",
    "\n",
    "trained_cot_examples = generate_dataset(\n",
    "    human_eval,\n",
    "    COT_PROMPT_TEMPLATE,\n",
    "    \"cot_solution\",\n",
    "    \"Step-by-step solution:\",\n",
    "    trained_cot_student_model,\n",
    "    trained_cot_tokenizer,\n",
    "    num_examples=100,\n",
    "    max_new_tokens=512,\n",
    "    teacher=False\n",
    ")\n",
    "\n",
    "print(\"cot examples generated\")\n",
    "\n",
    "code_examples = generate_dataset(\n",
    "    trained_cot_examples,\n",
    "    CODER_PROMPT_TEMPLATE,\n",
    "    \"gen_code\",\n",
    "    \"Python code:\",\n",
    "    untrained_coder_model,\n",
    "    untrained_coder_tokenizer,\n",
    "    num_examples=100,\n",
    "    max_new_tokens=512,\n",
    "    teacher=False\n",
    ")\n",
    "\n",
    "print(\"code generated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ko3EYETrAGP7"
   },
   "outputs": [],
   "source": [
    "i = 22\n",
    "\n",
    "#print('\\nproblem:')\n",
    "#print(new_code_examples[i]['problem'])\n",
    "#print('\\ncot')\n",
    "#print(new_code_examples[i]['cot_solution'])\n",
    "#print('\\ngenerated_code')\n",
    "print(new_code_examples[i]['gen_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoAB5YRwAGP8"
   },
   "outputs": [],
   "source": [
    "def extract_before_def(source_code):\n",
    "    \"\"\"\n",
    "    Extracts everything from the beginning of the source code up to\n",
    "    but not including the first occurrence of the 'def' keyword.\n",
    "    Preserves original formatting.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'^(.*?)(?=def)', re.DOTALL)\n",
    "    match = pattern.search(source_code)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    raise Exception(f\"Error: Unable to extract content before 'def'. No 'def' keyword found in:\\n{source_code}\")\n",
    "    return None\n",
    "\n",
    "def extract_until_code_block(source_code):\n",
    "    \"\"\"\n",
    "    Extracts everything from the beginning of the string up to\n",
    "    but not including the first occurrence of three backticks (```).\n",
    "    Preserves original formatting.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'^(.*?)(?=```)', re.DOTALL)\n",
    "    match = pattern.search(source_code)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return 'BAD'\n",
    "\n",
    "solutions = [item['solution_code'] for item in new_code_examples]\n",
    "generated_codes = [item['gen_code'] for item in new_code_examples]\n",
    "for i, generated_code in enumerate(generated_codes):\n",
    "    generated_codes[i] = extract_before_def(solutions[i]) + extract_until_code_block(generated_codes[i])\n",
    "print(generated_codes[0])\n",
    "\n",
    "def remove_bad_strings(string_array):\n",
    "    \"\"\"\n",
    "    Removes any strings containing 'BAD' from the given array.\n",
    "    Also prints the indices of removed strings.\n",
    "\n",
    "    Args:\n",
    "        string_array: A list of strings to filter\n",
    "\n",
    "    Returns:\n",
    "        A new list with all strings containing 'BAD' removed\n",
    "    \"\"\"\n",
    "    bad_indices = []\n",
    "    clean_strings = []\n",
    "\n",
    "    for i, s in enumerate(string_array):\n",
    "        if 'BAD' in s:\n",
    "            bad_indices.append(i)\n",
    "        else:\n",
    "            clean_strings.append(s)\n",
    "\n",
    "    # Print the indices of bad strings\n",
    "    if bad_indices:\n",
    "        print(f\"Found 'BAD' in strings at indices: {bad_indices}\")\n",
    "    else:\n",
    "        print(\"No strings containing 'BAD' found.\")\n",
    "\n",
    "    return clean_strings, bad_indices\n",
    "\n",
    "edited_codes, bad_indices = remove_bad_strings(generated_codes)\n",
    "print(len(edited_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37i6Qj5UAGP8"
   },
   "outputs": [],
   "source": [
    "human_eval['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQLpT9d1AGP8"
   },
   "outputs": [],
   "source": [
    "%pip install evaluate\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "# Load evaluation metric\n",
    "code_eval = load(\"code_eval\")\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "problems = human_eval\n",
    "test = []\n",
    "for i, item in enumerate(edited_codes):\n",
    "    edited_codes[i] = [item]\n",
    "pred = edited_codes\n",
    "c = 0\n",
    "\n",
    "for i, s in enumerate(human_eval[:100]):\n",
    "    if i not in bad_indices:\n",
    "        test.append(s)\n",
    "        c = c+1\n",
    "        print(c)\n",
    "\n",
    "pass_at_k = code_eval.compute(\n",
    "        predictions=pred,\n",
    "        references=test,\n",
    "        k=[1]\n",
    ")\n",
    "print(pass_at_k)\n",
    "print(pass_at_k[0]['pass@1']*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3eEhla-BwuO"
   },
   "outputs": [],
   "source": [
    "# Generate CoT (Chain of Thought) dataset\n",
    "cot_examples = generate_dataset(\n",
    "    problem_dataset=mbpp_problems,\n",
    "    task_prompt=COT_PROMPT_TEMPLATE,\n",
    "    solution_field=\"solution_cot\",\n",
    "    output_marker=\"Step-by-step solution:\",\n",
    "    teacher_model=teacher_model,\n",
    "    teacher_tokenizer=teacher_tokenizer,\n",
    "    num_examples=50,\n",
    "    output_file=\"datasets/cot_dataset.json\"\n",
    ")\n",
    "\n",
    "# Generate code dataset from CoT\n",
    "code_examples = generate_dataset(\n",
    "    problem_dataset=cot_examples,  # Use the output from CoT as input\n",
    "    task_prompt=DEVELOPER_PROMPT_TEMPLATE,\n",
    "    solution_field=\"code\",\n",
    "    output_marker=\"Python code:\",\n",
    "    teacher_model=teacher_model,\n",
    "    teacher_tokenizer=teacher_tokenizer,\n",
    "    num_examples=50,\n",
    "    output_file=\"datasets/code_dataset.json\"\n",
    ")\n",
    "\n",
    "# Generate debugged code dataset\n",
    "debugged_examples = generate_dataset(\n",
    "    problem_dataset=code_examples,  # Use the code examples as input\n",
    "    task_prompt=DEBUGGER_PROMPT_TEMPLATE,\n",
    "    solution_field=\"debugged_code\",\n",
    "    output_marker=\"Debugged Python code:\",\n",
    "    teacher_model=teacher_model,\n",
    "    teacher_tokenizer=teacher_tokenizer,\n",
    "    num_examples=50,\n",
    "    output_file=\"datasets/debugged_code_dataset.json\"\n",
    ")\n",
    "\n",
    "# Generate code explanations\n",
    "explanation_examples = generate_dataset(\n",
    "    problem_dataset=code_examples,  # Use code examples that also have CoT\n",
    "    task_prompt=EXPLAINER_PROMPT_TEMPLATE,\n",
    "    solution_field=\"explanation\",\n",
    "    output_marker=\"Explanation of the code:\",\n",
    "    teacher_model=teacher_model,\n",
    "    teacher_tokenizer=teacher_tokenizer,\n",
    "    num_examples=50,\n",
    "    output_file=\"datasets/explanation_dataset.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7Zgy_sdNKNV"
   },
   "outputs": [],
   "source": [
    "for i, example in enumerate(mbpp_problems):\n",
    "  print(f\"Problem number: {i}\")\n",
    "  print(f\"Problem: {example['problem']}\")\n",
    "  print(\"Test cases:\")\n",
    "  print(example['test_case'])\n",
    "  print(\"Code Solution:\")\n",
    "  print(example['solution'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
