{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4240cc14-50b6-4341-ba04-dcde88f52a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "teacher = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "\n",
    "student = \"Qwen/Qwen2.5-Coder-0.5B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    Load_in_8bit_fp32_cpu_offload=True)\n",
    "\n",
    "token_hf = \"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\"\n",
    "\n",
    "human_eval = load_dataset(\"openai_humaneval\")['test']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# device_map = {\n",
    "#     \"transformer.h\": \"cuda:0\",\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef6900ad-50e4-451b-8cbb-114cde1697aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eda.s/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e26c06dd614a59ae891d56849119e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      6\u001b[0m teacher_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      7\u001b[0m     teacher, \n\u001b[1;32m      8\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m      9\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     token \u001b[38;5;241m=\u001b[39m token_hf\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m student_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     22\u001b[0m     teacher,\n\u001b[1;32m     23\u001b[0m     token \u001b[38;5;241m=\u001b[39m token_hf)\n\u001b[1;32m     25\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:3032\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit \u001b[38;5;129;01mor\u001b[39;00m load_in_4bit:\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m-> 3032\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[1;32m   3034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   3035\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3036\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3037\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `pip install bitsandbytes`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3038\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    teacher, \n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    token = token_hf\n",
    ")\n",
    "\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\n",
    "    student,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token = token_hf,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    teacher,\n",
    "    token = token_hf)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfde5f20-a17b-438e-9121-9f170c2f14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code(generated_text):\n",
    "    start_marker = \"```python\"\n",
    "    end_marker = \"```\"\n",
    "\n",
    "    start_index = generated_text.find(start_marker)\n",
    "    if start_index == -1:\n",
    "         return \"\"\n",
    "    \n",
    "    start_index += len(start_marker)\n",
    "    \n",
    "    end_index = generated_text.find(end_marker, start_index)\n",
    "    if end_index == -1:\n",
    "        code = generated_text[start_index:].strip()\n",
    "    else:\n",
    "        code = generated_text[start_index:end_index].strip()\n",
    "    \n",
    "    return code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b969a9-66c1-4752-b70f-9e1dfc70f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_human_eval = human_eval.map(lambda ex: tokenizer(ex[\"prompt\"], truncation=True, max_length=512), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23983f42-291f-4494-9211-aa5617848491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(model, prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a coding assistant. When given a prompt, generate only a markdown code block that contains \"\n",
    "                \"valid Python code. The code block must start with ```python on its own line, then include the code, and \"\n",
    "                \"finally end with ``` on its own line. Do not include any extra text, comments, or explanations.\"\n",
    "            )\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt[0]}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(teacher_model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=512,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     print(full_response)\n",
    "    assistant_response = full_response.split(\"assistant\\n\")[-1].strip()\n",
    "    \n",
    "    return assistant_response\n",
    "\n",
    "# batch = tokenized_human_eval[0:1]\n",
    "# teacher_output = extract_code(get_response(batch[\"prompt\"]))\n",
    "\n",
    "# print(batch['prompt'][0])\n",
    "# teacher_response = tokenizer.batch_decode(teacher_output_ids, skip_special_tokens=True)\n",
    "# print(\"Teacher Response:\", teacher_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa820b5-a718-4c69-a9cc-ff24a8c7260b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "807425c5-7585-4de5-9007-47974345bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, temperature):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence loss between the teacher's softened predictions\n",
    "    and the student's predictions.\n",
    "    \"\"\"\n",
    "    # Scale logits by temperature; the temperature softens the probability distribution.\n",
    "    student_logits_T = student_logits / temperature\n",
    "    teacher_logits_T = teacher_logits / temperature\n",
    "\n",
    "    # Compute soft targets and student log probabilities\n",
    "    teacher_probs = F.softmax(teacher_logits_T, dim=-1)\n",
    "    student_log_probs = F.log_softmax(student_logits_T, dim=-1)\n",
    "    \n",
    "    # KL divergence; note the temperature factor scaling the gradients appropriately.\n",
    "    loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771dd7d6-59d5-4119-bf1e-713f3b2511e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eda.s/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss: 3042.7477097284227\n",
      "Epoch 2: Average Loss: 2651.3402157738096\n",
      "Epoch 3: Average Loss: 2404.1801874069943\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set training parameters\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 3\n",
    "batch_size = 8\n",
    "temperature = 2.0        \n",
    "\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "\n",
    "student_model.train()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(examples):\n",
    "    texts = [ex[\"prompt\"] for ex in examples]  \n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(tokenized_human_eval, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move inputs to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Teacher generates response (or logits) for distillation\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Use logits before softmax; may need to adjust if using generation outputs\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "        \n",
    "        # Student forward pass\n",
    "        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        common_vocab_size = min(student_logits.size(-1), teacher_logits.size(-1))\n",
    "        student_logits_aligned = student_logits[..., :common_vocab_size]\n",
    "        teacher_logits_aligned = teacher_logits[..., :common_vocab_size]\n",
    "        \n",
    "        # Compute distillation loss. For demonstration, using only soft target loss.\n",
    "        loss = distillation_loss(student_logits_aligned, teacher_logits_aligned, temperature)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Average Loss: {total_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e8e8e2-08f8-41c4-aa0b-b8a8a129f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss (per batch): 2222.2794\n",
      "Epoch 1: Average Loss (per token): 2.1221\n",
      "Epoch 2: Average Loss (per batch): 2092.5472\n",
      "Epoch 2: Average Loss (per token): 1.9982\n",
      "Epoch 3: Average Loss (per batch): 2000.7823\n",
      "Epoch 3: Average Loss (per token): 1.9106\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move inputs to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Teacher forward pass (no gradients)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "\n",
    "        # Student forward pass\n",
    "        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        # Align logits dimensions\n",
    "        common_vocab_size = min(student_logits.size(-1), teacher_logits.size(-1))\n",
    "        student_logits_aligned = student_logits[..., :common_vocab_size]\n",
    "        teacher_logits_aligned = teacher_logits[..., :common_vocab_size]\n",
    "\n",
    "        # Compute distillation loss\n",
    "        loss = distillation_loss(student_logits_aligned, teacher_logits_aligned, temperature)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # Count tokens processed in this batch; assume attention_mask sums to number of tokens\n",
    "        total_tokens += attention_mask.sum().item()\n",
    "\n",
    "    avg_loss_overall = total_loss / len(train_loader)        # Loss averaged per batch\n",
    "    avg_loss_per_token = total_loss / total_tokens             # Loss averaged per token\n",
    "    print(f\"Epoch {epoch+1}: Average Loss (per batch): {avg_loss_overall:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}: Average Loss (per token): {avg_loss_per_token:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb1c7a0-f369-41a8-8ff2-c2c8f81bb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.eval()\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "# Load evaluation metric\n",
    "code_eval = load(\"code_eval\")\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53855ad8-94c9-443b-bb3b-24cc7403011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(student_model.state_dict(),\"distilled_student_developer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac5bd09d-f16f-4cd2-8685-0f4e4cb08f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstudent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/accelerate/big_modeling.py:460\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:2575\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2571\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule\u001b[38;5;241m.\u001b[39mto)\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2573\u001b[0m     \u001b[38;5;66;03m# Checks if the model has been loaded in 8-bit\u001b[39;00m\n\u001b[1;32m   2574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES:\n\u001b[0;32m-> 2575\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2576\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2579\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mGPTQ:\n\u001b[1;32m   2580\u001b[0m         \u001b[38;5;66;03m# For GPTQ models, we prevent users from casting the model to another dytpe to restrict unwanted behaviours.\u001b[39;00m\n\u001b[1;32m   2581\u001b[0m         \u001b[38;5;66;03m# the correct API should be to load the model with the desired dtype directly through `from_pretrained`.\u001b[39;00m\n\u001b[1;32m   2582\u001b[0m         dtype_present_in_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`."
     ]
    }
   ],
   "source": [
    "student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "709115af-b9e7-458c-8c20-68be62261185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eda.s/.local/lib/python3.9/site-packages/transformers/generation/utils.py:2388: UserWarning: No PYTORCH_KERNEL_CACHE_PATH or HOME environment variable set! This disables kernel caching. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1426.)\n",
      "  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "({'pass@1': 0.9939024390243902}, defaultdict(<class 'list'>, {3: [(0, {'task_id': 3, 'passed': True, 'result': 'passed', 'completion_id': 0})], 1: [(0, {'task_id': 1, 'passed': True, 'result': 'passed', 'completion_id': 0})], 0: [(0, {'task_id': 0, 'passed': True, 'result': 'passed', 'completion_id': 0})], 4: [(0, {'task_id': 4, 'passed': True, 'result': 'passed', 'completion_id': 0})], 5: [(0, {'task_id': 5, 'passed': True, 'result': 'passed', 'completion_id': 0})], 6: [(0, {'task_id': 6, 'passed': True, 'result': 'passed', 'completion_id': 0})], 7: [(0, {'task_id': 7, 'passed': True, 'result': 'passed', 'completion_id': 0})], 9: [(0, {'task_id': 9, 'passed': True, 'result': 'passed', 'completion_id': 0})], 8: [(0, {'task_id': 8, 'passed': True, 'result': 'passed', 'completion_id': 0})], 12: [(0, {'task_id': 12, 'passed': True, 'result': 'passed', 'completion_id': 0})], 11: [(0, {'task_id': 11, 'passed': True, 'result': 'passed', 'completion_id': 0})], 14: [(0, {'task_id': 14, 'passed': True, 'result': 'passed', 'completion_id': 0})], 2: [(0, {'task_id': 2, 'passed': False, 'result': 'timed out', 'completion_id': 0})], 10: [(0, {'task_id': 10, 'passed': True, 'result': 'passed', 'completion_id': 0})], 15: [(0, {'task_id': 15, 'passed': True, 'result': 'passed', 'completion_id': 0})], 13: [(0, {'task_id': 13, 'passed': True, 'result': 'passed', 'completion_id': 0})], 17: [(0, {'task_id': 17, 'passed': True, 'result': 'passed', 'completion_id': 0})], 19: [(0, {'task_id': 19, 'passed': True, 'result': 'passed', 'completion_id': 0})], 20: [(0, {'task_id': 20, 'passed': True, 'result': 'passed', 'completion_id': 0})], 21: [(0, {'task_id': 21, 'passed': True, 'result': 'passed', 'completion_id': 0})], 22: [(0, {'task_id': 22, 'passed': True, 'result': 'passed', 'completion_id': 0})], 16: [(0, {'task_id': 16, 'passed': True, 'result': 'passed', 'completion_id': 0})], 25: [(0, {'task_id': 25, 'passed': True, 'result': 'passed', 'completion_id': 0})], 26: [(0, {'task_id': 26, 'passed': True, 'result': 'passed', 'completion_id': 0})], 18: [(0, {'task_id': 18, 'passed': True, 'result': 'passed', 'completion_id': 0})], 28: [(0, {'task_id': 28, 'passed': True, 'result': 'passed', 'completion_id': 0})], 29: [(0, {'task_id': 29, 'passed': True, 'result': 'passed', 'completion_id': 0})], 24: [(0, {'task_id': 24, 'passed': True, 'result': 'passed', 'completion_id': 0})], 23: [(0, {'task_id': 23, 'passed': True, 'result': 'passed', 'completion_id': 0})], 32: [(0, {'task_id': 32, 'passed': True, 'result': 'passed', 'completion_id': 0})], 27: [(0, {'task_id': 27, 'passed': True, 'result': 'passed', 'completion_id': 0})], 30: [(0, {'task_id': 30, 'passed': True, 'result': 'passed', 'completion_id': 0})], 31: [(0, {'task_id': 31, 'passed': True, 'result': 'passed', 'completion_id': 0})], 33: [(0, {'task_id': 33, 'passed': True, 'result': 'passed', 'completion_id': 0})], 34: [(0, {'task_id': 34, 'passed': True, 'result': 'passed', 'completion_id': 0})], 35: [(0, {'task_id': 35, 'passed': True, 'result': 'passed', 'completion_id': 0})], 36: [(0, {'task_id': 36, 'passed': True, 'result': 'passed', 'completion_id': 0})], 37: [(0, {'task_id': 37, 'passed': True, 'result': 'passed', 'completion_id': 0})], 38: [(0, {'task_id': 38, 'passed': True, 'result': 'passed', 'completion_id': 0})], 39: [(0, {'task_id': 39, 'passed': True, 'result': 'passed', 'completion_id': 0})], 40: [(0, {'task_id': 40, 'passed': True, 'result': 'passed', 'completion_id': 0})], 42: [(0, {'task_id': 42, 'passed': True, 'result': 'passed', 'completion_id': 0})], 41: [(0, {'task_id': 41, 'passed': True, 'result': 'passed', 'completion_id': 0})], 43: [(0, {'task_id': 43, 'passed': True, 'result': 'passed', 'completion_id': 0})], 44: [(0, {'task_id': 44, 'passed': True, 'result': 'passed', 'completion_id': 0})], 45: [(0, {'task_id': 45, 'passed': True, 'result': 'passed', 'completion_id': 0})], 46: [(0, {'task_id': 46, 'passed': True, 'result': 'passed', 'completion_id': 0})], 47: [(0, {'task_id': 47, 'passed': True, 'result': 'passed', 'completion_id': 0})], 48: [(0, {'task_id': 48, 'passed': True, 'result': 'passed', 'completion_id': 0})], 49: [(0, {'task_id': 49, 'passed': True, 'result': 'passed', 'completion_id': 0})], 50: [(0, {'task_id': 50, 'passed': True, 'result': 'passed', 'completion_id': 0})], 51: [(0, {'task_id': 51, 'passed': True, 'result': 'passed', 'completion_id': 0})], 52: [(0, {'task_id': 52, 'passed': True, 'result': 'passed', 'completion_id': 0})], 54: [(0, {'task_id': 54, 'passed': True, 'result': 'passed', 'completion_id': 0})], 53: [(0, {'task_id': 53, 'passed': True, 'result': 'passed', 'completion_id': 0})], 56: [(0, {'task_id': 56, 'passed': True, 'result': 'passed', 'completion_id': 0})], 55: [(0, {'task_id': 55, 'passed': True, 'result': 'passed', 'completion_id': 0})], 58: [(0, {'task_id': 58, 'passed': True, 'result': 'passed', 'completion_id': 0})], 57: [(0, {'task_id': 57, 'passed': True, 'result': 'passed', 'completion_id': 0})], 59: [(0, {'task_id': 59, 'passed': True, 'result': 'passed', 'completion_id': 0})], 60: [(0, {'task_id': 60, 'passed': True, 'result': 'passed', 'completion_id': 0})], 62: [(0, {'task_id': 62, 'passed': True, 'result': 'passed', 'completion_id': 0})], 61: [(0, {'task_id': 61, 'passed': True, 'result': 'passed', 'completion_id': 0})], 63: [(0, {'task_id': 63, 'passed': True, 'result': 'passed', 'completion_id': 0})], 64: [(0, {'task_id': 64, 'passed': True, 'result': 'passed', 'completion_id': 0})], 65: [(0, {'task_id': 65, 'passed': True, 'result': 'passed', 'completion_id': 0})], 66: [(0, {'task_id': 66, 'passed': True, 'result': 'passed', 'completion_id': 0})], 67: [(0, {'task_id': 67, 'passed': True, 'result': 'passed', 'completion_id': 0})], 68: [(0, {'task_id': 68, 'passed': True, 'result': 'passed', 'completion_id': 0})], 69: [(0, {'task_id': 69, 'passed': True, 'result': 'passed', 'completion_id': 0})], 70: [(0, {'task_id': 70, 'passed': True, 'result': 'passed', 'completion_id': 0})], 72: [(0, {'task_id': 72, 'passed': True, 'result': 'passed', 'completion_id': 0})], 71: [(0, {'task_id': 71, 'passed': True, 'result': 'passed', 'completion_id': 0})], 73: [(0, {'task_id': 73, 'passed': True, 'result': 'passed', 'completion_id': 0})], 74: [(0, {'task_id': 74, 'passed': True, 'result': 'passed', 'completion_id': 0})], 76: [(0, {'task_id': 76, 'passed': True, 'result': 'passed', 'completion_id': 0})], 75: [(0, {'task_id': 75, 'passed': True, 'result': 'passed', 'completion_id': 0})], 77: [(0, {'task_id': 77, 'passed': True, 'result': 'passed', 'completion_id': 0})], 78: [(0, {'task_id': 78, 'passed': True, 'result': 'passed', 'completion_id': 0})], 79: [(0, {'task_id': 79, 'passed': True, 'result': 'passed', 'completion_id': 0})], 80: [(0, {'task_id': 80, 'passed': True, 'result': 'passed', 'completion_id': 0})], 81: [(0, {'task_id': 81, 'passed': True, 'result': 'passed', 'completion_id': 0})], 82: [(0, {'task_id': 82, 'passed': True, 'result': 'passed', 'completion_id': 0})], 84: [(0, {'task_id': 84, 'passed': True, 'result': 'passed', 'completion_id': 0})], 83: [(0, {'task_id': 83, 'passed': True, 'result': 'passed', 'completion_id': 0})], 85: [(0, {'task_id': 85, 'passed': True, 'result': 'passed', 'completion_id': 0})], 86: [(0, {'task_id': 86, 'passed': True, 'result': 'passed', 'completion_id': 0})], 88: [(0, {'task_id': 88, 'passed': True, 'result': 'passed', 'completion_id': 0})], 87: [(0, {'task_id': 87, 'passed': True, 'result': 'passed', 'completion_id': 0})], 89: [(0, {'task_id': 89, 'passed': True, 'result': 'passed', 'completion_id': 0})], 90: [(0, {'task_id': 90, 'passed': True, 'result': 'passed', 'completion_id': 0})], 91: [(0, {'task_id': 91, 'passed': True, 'result': 'passed', 'completion_id': 0})], 92: [(0, {'task_id': 92, 'passed': True, 'result': 'passed', 'completion_id': 0})], 93: [(0, {'task_id': 93, 'passed': True, 'result': 'passed', 'completion_id': 0})], 94: [(0, {'task_id': 94, 'passed': True, 'result': 'passed', 'completion_id': 0})], 95: [(0, {'task_id': 95, 'passed': True, 'result': 'passed', 'completion_id': 0})], 96: [(0, {'task_id': 96, 'passed': True, 'result': 'passed', 'completion_id': 0})], 97: [(0, {'task_id': 97, 'passed': True, 'result': 'passed', 'completion_id': 0})], 98: [(0, {'task_id': 98, 'passed': True, 'result': 'passed', 'completion_id': 0})], 99: [(0, {'task_id': 99, 'passed': True, 'result': 'passed', 'completion_id': 0})], 100: [(0, {'task_id': 100, 'passed': True, 'result': 'passed', 'completion_id': 0})], 101: [(0, {'task_id': 101, 'passed': True, 'result': 'passed', 'completion_id': 0})], 102: [(0, {'task_id': 102, 'passed': True, 'result': 'passed', 'completion_id': 0})], 104: [(0, {'task_id': 104, 'passed': True, 'result': 'passed', 'completion_id': 0})], 103: [(0, {'task_id': 103, 'passed': True, 'result': 'passed', 'completion_id': 0})], 106: [(0, {'task_id': 106, 'passed': True, 'result': 'passed', 'completion_id': 0})], 105: [(0, {'task_id': 105, 'passed': True, 'result': 'passed', 'completion_id': 0})], 107: [(0, {'task_id': 107, 'passed': True, 'result': 'passed', 'completion_id': 0})], 108: [(0, {'task_id': 108, 'passed': True, 'result': 'passed', 'completion_id': 0})], 110: [(0, {'task_id': 110, 'passed': True, 'result': 'passed', 'completion_id': 0})], 109: [(0, {'task_id': 109, 'passed': True, 'result': 'passed', 'completion_id': 0})], 112: [(0, {'task_id': 112, 'passed': True, 'result': 'passed', 'completion_id': 0})], 111: [(0, {'task_id': 111, 'passed': True, 'result': 'passed', 'completion_id': 0})], 113: [(0, {'task_id': 113, 'passed': True, 'result': 'passed', 'completion_id': 0})], 114: [(0, {'task_id': 114, 'passed': True, 'result': 'passed', 'completion_id': 0})], 116: [(0, {'task_id': 116, 'passed': True, 'result': 'passed', 'completion_id': 0})], 115: [(0, {'task_id': 115, 'passed': True, 'result': 'passed', 'completion_id': 0})], 117: [(0, {'task_id': 117, 'passed': True, 'result': 'passed', 'completion_id': 0})], 118: [(0, {'task_id': 118, 'passed': True, 'result': 'passed', 'completion_id': 0})], 120: [(0, {'task_id': 120, 'passed': True, 'result': 'passed', 'completion_id': 0})], 119: [(0, {'task_id': 119, 'passed': True, 'result': 'passed', 'completion_id': 0})], 121: [(0, {'task_id': 121, 'passed': True, 'result': 'passed', 'completion_id': 0})], 122: [(0, {'task_id': 122, 'passed': True, 'result': 'passed', 'completion_id': 0})], 124: [(0, {'task_id': 124, 'passed': True, 'result': 'passed', 'completion_id': 0})], 123: [(0, {'task_id': 123, 'passed': True, 'result': 'passed', 'completion_id': 0})], 126: [(0, {'task_id': 126, 'passed': True, 'result': 'passed', 'completion_id': 0})], 125: [(0, {'task_id': 125, 'passed': True, 'result': 'passed', 'completion_id': 0})], 127: [(0, {'task_id': 127, 'passed': True, 'result': 'passed', 'completion_id': 0})], 128: [(0, {'task_id': 128, 'passed': True, 'result': 'passed', 'completion_id': 0})], 129: [(0, {'task_id': 129, 'passed': True, 'result': 'passed', 'completion_id': 0})], 130: [(0, {'task_id': 130, 'passed': True, 'result': 'passed', 'completion_id': 0})], 132: [(0, {'task_id': 132, 'passed': True, 'result': 'passed', 'completion_id': 0})], 131: [(0, {'task_id': 131, 'passed': True, 'result': 'passed', 'completion_id': 0})], 133: [(0, {'task_id': 133, 'passed': True, 'result': 'passed', 'completion_id': 0})], 134: [(0, {'task_id': 134, 'passed': True, 'result': 'passed', 'completion_id': 0})], 135: [(0, {'task_id': 135, 'passed': True, 'result': 'passed', 'completion_id': 0})], 136: [(0, {'task_id': 136, 'passed': True, 'result': 'passed', 'completion_id': 0})], 137: [(0, {'task_id': 137, 'passed': True, 'result': 'passed', 'completion_id': 0})], 138: [(0, {'task_id': 138, 'passed': True, 'result': 'passed', 'completion_id': 0})], 139: [(0, {'task_id': 139, 'passed': True, 'result': 'passed', 'completion_id': 0})], 140: [(0, {'task_id': 140, 'passed': True, 'result': 'passed', 'completion_id': 0})], 141: [(0, {'task_id': 141, 'passed': True, 'result': 'passed', 'completion_id': 0})], 142: [(0, {'task_id': 142, 'passed': True, 'result': 'passed', 'completion_id': 0})], 144: [(0, {'task_id': 144, 'passed': True, 'result': 'passed', 'completion_id': 0})], 143: [(0, {'task_id': 143, 'passed': True, 'result': 'passed', 'completion_id': 0})], 146: [(0, {'task_id': 146, 'passed': True, 'result': 'passed', 'completion_id': 0})], 145: [(0, {'task_id': 145, 'passed': True, 'result': 'passed', 'completion_id': 0})], 147: [(0, {'task_id': 147, 'passed': True, 'result': 'passed', 'completion_id': 0})], 148: [(0, {'task_id': 148, 'passed': True, 'result': 'passed', 'completion_id': 0})], 150: [(0, {'task_id': 150, 'passed': True, 'result': 'passed', 'completion_id': 0})], 149: [(0, {'task_id': 149, 'passed': True, 'result': 'passed', 'completion_id': 0})], 152: [(0, {'task_id': 152, 'passed': True, 'result': 'passed', 'completion_id': 0})], 151: [(0, {'task_id': 151, 'passed': True, 'result': 'passed', 'completion_id': 0})], 153: [(0, {'task_id': 153, 'passed': True, 'result': 'passed', 'completion_id': 0})], 154: [(0, {'task_id': 154, 'passed': True, 'result': 'passed', 'completion_id': 0})], 155: [(0, {'task_id': 155, 'passed': True, 'result': 'passed', 'completion_id': 0})], 156: [(0, {'task_id': 156, 'passed': True, 'result': 'passed', 'completion_id': 0})], 157: [(0, {'task_id': 157, 'passed': True, 'result': 'passed', 'completion_id': 0})], 158: [(0, {'task_id': 158, 'passed': True, 'result': 'passed', 'completion_id': 0})], 159: [(0, {'task_id': 159, 'passed': True, 'result': 'passed', 'completion_id': 0})], 160: [(0, {'task_id': 160, 'passed': True, 'result': 'passed', 'completion_id': 0})], 161: [(0, {'task_id': 161, 'passed': True, 'result': 'passed', 'completion_id': 0})], 162: [(0, {'task_id': 162, 'passed': True, 'result': 'passed', 'completion_id': 0})], 163: [(0, {'task_id': 163, 'passed': True, 'result': 'passed', 'completion_id': 0})]}))\n",
      "99.39024390243902\n"
     ]
    }
   ],
   "source": [
    "problems = human_eval\n",
    "test = []\n",
    "pred = [] \n",
    "c= 0\n",
    "for i in range(164):\n",
    "    prompt = problems['prompt'][i]\n",
    "    # print(prompt)\n",
    "    completion = extract_code(get_response(student_model,prompt))\n",
    "    final_code = prompt + \"\\n\" +  completion  \n",
    "    test.append(problems['test'][i])\n",
    "    pred.append([final_code])\n",
    "    c = c+1\n",
    "    print(c)\n",
    "\n",
    "pass_at_k = code_eval.compute(\n",
    "        predictions=pred,\n",
    "        references=test,\n",
    "        k=[1]\n",
    ")\n",
    "print(pass_at_k)\n",
    "print(pass_at_k[0]['pass@1']*100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec77da5e-080f-4279-9fed-09180094d983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n",
      "    # Your code here\n",
      "    return False\n"
     ]
    }
   ],
   "source": [
    "student_model.eval()\n",
    "\n",
    "# Example evaluation on a subset:\n",
    "eval_batch = human_eval.select(range(1))\n",
    "inputs_eval = tokenizer(eval_batch[\"prompt\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "inputs_eval = {k: v.to(device) for k, v in inputs_eval.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = student_model.generate(**inputs_eval, max_new_tokens=128)\n",
    "    \n",
    "generated_responses = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "for idx, response in enumerate(generated_responses):\n",
    "    print(f\"Example {idx+1}: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "283222f9-be4a-4476-b38a-8e7baf1d985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "\n",
      "Hello2\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "Hello3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "problems = human_eval\n",
    "test = []\n",
    "pred = [] \n",
    "c= 0\n",
    "for i in range(1):\n",
    "    prompt = problems['prompt'][i]\n",
    "    print(\"Hello\")\n",
    "    # print(prompt)\n",
    "    completion = extract_code(get_response(student_model,prompt))\n",
    "    print(completion)\n",
    "    print(\"Hello2\")\n",
    "    final_code = prompt + \"\\n\" +  completion  \n",
    "    print(final_code)\n",
    "    print(\"Hello3\")\n",
    "    test.append(problems['test'][i])\n",
    "    pred.append([final_code])\n",
    "    c = c+1\n",
    "    print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "341abd3a-9828-4eb3-8554-08ee7ce0805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n\\n']]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b95e803c-a91c-4f6d-8de7-9d0027bc68e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eda.s/.local/lib/python3.9/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 402656256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "student_model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    student,\n",
    "    device_map=\"auto\",\n",
    "    token = token_hf,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804ceb1-6b7a-40b7-b95a-58019b6894ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
