{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc0zIIPjmHLe",
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVlZuvOkmJu0",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ozqz7L-aByro"
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade pip\n",
    "#%pip install transformers==4.37.0\n",
    "#%pip uninstall torch torchvision torchaudio -y\n",
    "#%pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116 -y\n",
    "#%pip install torch torchvision torchaudio\n",
    "#%pip install tqdm\n",
    "#%pip install numpy==1.24 #probably not needed, leave this commented\n",
    "#%pip install urllib3==1.26.15\n",
    "#%pip install accelerate==0.25.0\n",
    "#%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ScYYGSMOpU_X",
    "outputId": "061854c3-0977-4445-f330-df7a079e5b76",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "We are using the device cuda.\n",
      "Device count: 1\n",
      "Device name: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import copy\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('We are using the device {}.'.format(device))\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41BNhCQ72BD3",
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C5c0hfrX2BD3"
   },
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KtAQQDYt9tpn",
    "outputId": "185d672c-0fe8-4d22-dc5b-7fd16b17e856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory: 79.15 GB\n",
      "Currently allocated: 0.00 GB\n",
      "Cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Display total GPU memory\n",
    "print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Display currently allocated memory\n",
    "print(f\"Currently allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "# Display cached memory (reserved by PyTorch but not used)\n",
    "print(f\"Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Gh1-kS8L9j_7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                  Size  Used Avail Use% Mounted on\n",
      "devtmpfs                                    252G     0  252G   0% /dev\n",
      "tmpfs                                       252G  228K  252G   1% /dev/shm\n",
      "tmpfs                                       252G   59M  252G   1% /run\n",
      "tmpfs                                       252G     0  252G   0% /sys/fs/cgroup\n",
      "/dev/sda3                                    20G  5.3G   15G  27% /\n",
      "/dev/sda2                                   994M  188M  806M  19% /boot\n",
      "/dev/sda11                                  359G  401M  358G   1% /tmp\n",
      "/dev/sda7                                   9.8G  479M  9.3G   5% /var\n",
      "/dev/sda8                                   9.8G  299M  9.5G   3% /var/log\n",
      "/dev/sda9                                   9.8G   69M  9.7G   1% /var/log/audit\n",
      "/dev/sda10                                  9.8G   33M  9.8G   1% /var/tmp\n",
      "vast1-mghpcc-ib.neu.edu:/discovery/home     155T  133T   23T  86% /home\n",
      "vast1-mghpcc-ib.neu.edu:/vast_shared         30T   18T   13T  60% /shared\n",
      "vast1-mghpcc-ib.neu.edu:/courses             36T   14T   23T  38% /courses\n",
      "vast1-mghpcc-ib.neu.edu:/work_project       3.7P  2.5P  1.2P  69% /work\n",
      "vast1-mghpcc-ib.neu.edu:/discovery/scratch  2.2P  1.4P  753T  65% /scratch\n",
      "192.168.4.70:/datasets                       21T   21T  256G  99% /datasets\n"
     ]
    }
   ],
   "source": [
    "# # Check disk space\n",
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"results/student_model_final\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iKaqNcom15F",
    "tags": []
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l14eRfYHW4xi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QFlkmSr7l_qS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "COT_PROMPT_TEMPLATE = \"\"\"Generate a detailed step-by-step solution for this coding problem.\n",
    "Break down your thought process into clear, concise steps, explaining your reasoning at each stage.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Step-by-step solution:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Y1Fjaq8F2BD4"
   },
   "outputs": [],
   "source": [
    "COT_PROMPT_TEMPLATE = \"\"\"Generate a detailed step-by-step solution for this coding problem.\n",
    "Break down your thought process clearly, explaining your reasoning while considering:\n",
    "- What are the inputs and outputs of the function?\n",
    "- What algorithm or data structure is most appropriate?\n",
    "- Are there any edge cases to handle?\n",
    "- What's the efficiency of your approach?\n",
    "\n",
    "Be concise in your explanation.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Step-by-step solution:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "akyq5K0OAbnA"
   },
   "outputs": [],
   "source": [
    "CODER_PROMPT_TEMPLATE = \"\"\"Generate only a markdown code block that contains clean, efficient\n",
    "Python code for this coding problem based on the solution approach. The code block must start\n",
    "with ```python on its own line, then the code, and end with ``` on its own line. Do not include\n",
    "test cases or code explanations.\n",
    "Focus on:\n",
    "- Implementing the key algorithmic insights\n",
    "- Handling edge cases identified in the solution\n",
    "- Maintaining readability and efficiency\n",
    "\n",
    "Step-by-step solution:\n",
    "{solution_cot}\n",
    "\n",
    "Python code:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GbeODeJS4U1G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "CODER_PROMPT_TEMPLATE = \"\"\"Generate the python code for this coding problem. Follow the\n",
    "step-by-step process as a guideline for how to solve the problem. Only return python code.\n",
    "\n",
    "Step-by-step solution:\n",
    "{solution_cot}\n",
    "\n",
    "Python code:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "778DzMEG7FSW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUGGER_PROMPT_TEMPLATE = \"\"\"Check the provided python code for any errors. Then regenerate\n",
    "the code so that any errors have been debugged.\n",
    "\n",
    "Python code:\n",
    "{code}\n",
    "\n",
    "Debugged Python code:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6c3mVY375SPC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPLAINER_PROMPT_TEMPLATE = \"\"\"Generate an explanation of this code using the step-by-step\n",
    "solution and the code itself. Keep the explanation concise.\n",
    "\n",
    "Step-by-step solution:\n",
    "{solution_cot}\n",
    "\n",
    "Python code:\n",
    "{code}\n",
    "\n",
    "Python code explanation:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvGPN7o2W8G5",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dataset Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FLXXoByzpNP2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CodeCraftDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A generalized dataset for Code Craft agents that works with various prompt templates.\n",
    "\n",
    "    Args:\n",
    "        examples: List of dictionaries that hold all agent prompt information.\n",
    "        tokenizer: Used to tokenize the inputs to the model.\n",
    "        prompt_template: The prompt template string with placeholders.\n",
    "        output_field: The name of the field in examples that contains the expected output.\n",
    "        max_length: The maximum token length of the inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, examples, tokenizer, prompt_template, output_field, max_length=512):\n",
    "        self.examples = examples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_template = prompt_template\n",
    "        self.output_field = output_field\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        output = example[self.output_field]\n",
    "\n",
    "        # Create prompt by formatting template with example data\n",
    "        # This will use all fields from the example that match placeholders in the template\n",
    "        try:\n",
    "            prompt = self.prompt_template.format(**example)\n",
    "        except KeyError as e:\n",
    "            missing_key = str(e).strip(\"'\")\n",
    "            raise KeyError(f\"Example at index {idx} is missing required field '{missing_key}' \"\n",
    "                          f\"for prompt template: {self.prompt_template}\")\n",
    "\n",
    "        # Combine prompt with expected output\n",
    "        full_text_with_output = prompt + output\n",
    "\n",
    "        # Tokenize the combined text\n",
    "        encoded = self.tokenizer(\n",
    "            full_text_with_output,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Create labels (same as input_ids but with -100 for prompt tokens)\n",
    "        prompt_tokens = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        prompt_length = len(prompt_tokens)\n",
    "\n",
    "        labels = encoded[\"input_ids\"].clone()\n",
    "        labels[0, :prompt_length] = -100  # Don't compute loss for prompt tokens\n",
    "\n",
    "        result = {\n",
    "            \"input_ids\": encoded[\"input_ids\"][0],\n",
    "            \"attention_mask\": encoded[\"attention_mask\"][0],\n",
    "            \"labels\": labels[0]\n",
    "        }\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3x0EQoEK8Oql",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dataset(problem_dataset, task_prompt, solution_field, output_marker,\n",
    "    model, tokenizer, num_examples=50, max_new_tokens=512, teacher=True, regen=False,\n",
    "    output_dir=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Generate a dataset by prompting a teacher model to solve problems for distillation.\n",
    "\n",
    "    Args:\n",
    "        problem_dataset: List of dictionaries containing problem data\n",
    "        task_prompt: Prompt template string with placeholders\n",
    "        solution_field: Field name for the generated solution in output examples\n",
    "        output_marker: String marker after which the solution starts in the model output\n",
    "                       (or None if the entire output is the solution)\n",
    "        model: The model used to generate solutions\n",
    "        tokenizer: Tokenizer for the model\n",
    "        num_examples: Number of examples to generate\n",
    "        max_new_tokens: Maximum token length for generation\n",
    "        teacher: a flag indicating if the model is teacher (true) or student (f)\n",
    "        regen: a flag indicating if the data should be regenerated if it already exists\n",
    "        output_dir: Directory to save the generated examples\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing the problems and their solutions\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get the model type from the teacher param\n",
    "    if teacher:\n",
    "        model_name = \"teacher\"\n",
    "    else:\n",
    "        model_name = \"student\"\n",
    "\n",
    "    # If indicated not to regenerate the examples and they exist then return them\n",
    "    file_name = os.path.join(output_dir, f\"{solution_field}_{model_name}_{num_examples}_dataset.json\")\n",
    "    if regen and os.path.exists(file_name):\n",
    "        with open(file_name, 'r') as examples_file:\n",
    "            examples = json.load(examples_file)\n",
    "        print(\"loaded examples from json\")\n",
    "        return examples\n",
    "\n",
    "    examples = []\n",
    "    logger.info(f\"Generating {solution_field} with {model_name} for {num_examples} problems...\")\n",
    "\n",
    "    # Take a subset of problems for efficiency\n",
    "    problems_subset = problem_dataset[:num_examples]\n",
    "\n",
    "    for i, problem in enumerate(tqdm(problems_subset, desc=f\"Generating {solution_field}\")):\n",
    "        try:\n",
    "            # Format the prompt with the problem data\n",
    "            prompt = task_prompt.format(**problem)\n",
    "\n",
    "            # Tokenize the prompt\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            # Generate the solution from the model\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    num_return_sequences=1\n",
    "                )\n",
    "\n",
    "            # Decode the model output\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "            # Extract the solution portion if an output marker is provided\n",
    "            if output_marker and output_marker in generated_text:\n",
    "                solution_start_idx = generated_text.find(output_marker) + len(output_marker)\n",
    "                solution = generated_text[solution_start_idx:].strip()\n",
    "            else:\n",
    "                # Use the entire output if no marker is provided or found\n",
    "                solution = generated_text.replace(prompt, \"\").strip()\n",
    "\n",
    "            # Create the example with all original problem fields plus the solution\n",
    "            example = problem.copy()  # Preserve all original fields\n",
    "            example[solution_field] = solution  # Add the generated solution\n",
    "            examples.append(example)\n",
    "\n",
    "            # Save a few examples for inspection\n",
    "            if i < 2:\n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                print(f\"Problem: {example['problem'][:150]}...\")\n",
    "                print(f\"Solution (first 150 chars): {example[solution_field][:150]}...\")\n",
    "\n",
    "            # Log progress details periodically\n",
    "            if (i + 1) % 10 == 0:\n",
    "                logger.info(f\"Generated {i + 1}/{len(problems_subset)} solutions\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating solution for problem {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"Successfully generated {len(examples)} {solution_field} solutions\")\n",
    "\n",
    "    # Save the dataset\n",
    "    with open(file_name, \"w\") as f:\n",
    "        json.dump(examples, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Dataset saved to {file_name}\")\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJYlB4F42BD5",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load Dataset Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vDP6VUFU2BD5"
   },
   "outputs": [],
   "source": [
    "# Load MBPP dataset\n",
    "def load_mbpp_dataset():\n",
    "    mbpp = load_dataset(\"mbpp\")\n",
    "\n",
    "    train_problems = []\n",
    "    # Extract problems from the MBPP dataset with correct field names\n",
    "    for item in mbpp[\"train\"]:\n",
    "        train_problems.append({\n",
    "            \"problem\": item[\"text\"],\n",
    "            \"test_case\": item[\"test_list\"],\n",
    "            \"solution_code\": item[\"code\"]\n",
    "        })\n",
    "\n",
    "    test_problems = []\n",
    "    for item in mbpp[\"test\"]:\n",
    "        test_problems.append({\n",
    "            \"problem\": item[\"text\"],\n",
    "            \"test_case\": item[\"test_list\"],\n",
    "            \"solution_code\": item[\"code\"]\n",
    "        })\n",
    "\n",
    "    print(f\"Loaded {len(train_problems)} train problems and {len(test_problems)} evaluation problems from MBPP dataset\")\n",
    "    return train_problems, test_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nh2SiyeD2BD6"
   },
   "outputs": [],
   "source": [
    "# Load BAAI/TACO dataset\n",
    "def load_taco_dataset():\n",
    "    taco = load_dataset(\"BAAI/TACO\")\n",
    "\n",
    "    train_problems = []\n",
    "    for item in taco[\"train\"]:\n",
    "        train_problems.append({\n",
    "            \"problem\": item[\"question\"],\n",
    "            \"test_case\": item[\"input_output\"],\n",
    "            \"solution_code\": item[\"solutions\"][0]\n",
    "        })\n",
    "\n",
    "    test_problems = []\n",
    "    for item in taco[\"test\"]:\n",
    "        train_problems.append({\n",
    "            \"problem\": item[\"question\"],\n",
    "            \"test_case\": item[\"test_cases\"],\n",
    "            \"solution_code\": item[\"solutions\"][0]\n",
    "        })\n",
    "\n",
    "    print(f\"Loaded {len(train_problems)} train problems and {len(test_problems)} test problems from TACO dataset\")\n",
    "    return train_problems, test_problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnoxXUO9FAOU",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Agent Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvOVJuMDFCLy",
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "h2l4VajfELPR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load models\n",
    "def load_models(teacher_model_name, student_model_name):\n",
    "    logger.info(f\"Loading teacher model: {teacher_model_name}\")\n",
    "    teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
    "    teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "        teacher_model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "    logger.info(f\"Teacher model loaded successfully\")\n",
    "\n",
    "    logger.info(f\"Loading student model: {student_model_name}\")\n",
    "    student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "    student_model = AutoModelForCausalLM.from_pretrained(\n",
    "        student_model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float32\n",
    "    )\n",
    "    logger.info(f\"Student model loaded successfully\")\n",
    "\n",
    "    return teacher_model, teacher_tokenizer, student_model, student_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0hx7iq02UDu",
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "AcHhd7BGRdQC"
   },
   "outputs": [],
   "source": [
    "def fine_tune_student_model(student_model, student_tokenizer, train_data, prompt,\n",
    "                        output_field, batch_size=8, num_epochs=3, learning_rate=5e-5,\n",
    "                        max_grad_norm=1.0, warmup_steps=0, max_length=512,\n",
    "                        output_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    Fine-tune the student model on examples generated by the teacher model.\n",
    "\n",
    "    Args:\n",
    "        student_model: The student model to train\n",
    "        student_tokenizer: Tokenizer for the student model\n",
    "        train_data: List of data dictionaries for training\n",
    "        prompt: The prompt containing fields for training\n",
    "        output: The output data field to train on\n",
    "        batch_size: Training batch size\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for the optimizer\n",
    "        max_grad_norm: Maximum gradient norm for gradient clipping\n",
    "        warmup_steps: Linear warmup steps for the learning rate scheduler\n",
    "        max_length: the maximum number of tokens in the dataset values\n",
    "        output_dir: Directory to save the trained model\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logger.info(f\"Starting training the student model for {num_epochs} epochs\")\n",
    "\n",
    "    # Create PyTorch dataset and dataloader\n",
    "    dataset = CodeCraftDataset(\n",
    "        examples=train_data,\n",
    "        tokenizer=student_tokenizer,\n",
    "        prompt_template=prompt,\n",
    "        output_field=output_field,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Set up optimizer and learning rate scheduler\n",
    "    optimizer = optim.AdamW(student_model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(dataloader) * num_epochs\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=learning_rate, total_steps=total_steps,\n",
    "        pct_start=warmup_steps/total_steps if warmup_steps > 0 else 0.1\n",
    "    )\n",
    "\n",
    "    # Set up training tracking\n",
    "    best_loss = float('inf')\n",
    "    global_step = 0\n",
    "    student_model.train()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            input_ids = batch[\"input_ids\"].to(student_model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(student_model.device)\n",
    "            labels = batch[\"labels\"].to(student_model.device)\n",
    "\n",
    "            # Forward pass - compute student model outputs\n",
    "            outputs = student_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_grad_norm)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Track loss\n",
    "            epoch_loss += loss.item()\n",
    "            global_step += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "            # Save checkpoint occasionally\n",
    "            if global_step % 100 == 0:\n",
    "                logger.info(f\"Step {global_step}: loss = {loss.item():.4f}\")\n",
    "\n",
    "        # Compute average epoch loss\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs} - Average loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint if it's the best model so far\n",
    "        # if avg_epoch_loss < best_loss:\n",
    "        #     best_loss = avg_epoch_loss\n",
    "        #     checkpoint_path = os.path.join(output_dir, f\"student_model_{output_field}_epoch_{epoch+1}\")\n",
    "        #     logger.info(f\"Saving best model so far (loss: {best_loss:.4f}) to {checkpoint_path}\")\n",
    "        #     student_model.save_pretrained(checkpoint_path)\n",
    "        #     student_tokenizer.save_pretrained(checkpoint_path)\n",
    "\n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(output_dir, f\"student_model_{output_field}_final\")\n",
    "    logger.info(f\"Training completed. Saving final model to {final_model_path}\")\n",
    "    student_model.save_pretrained(final_model_path)\n",
    "    student_tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7qenJ2u3lPzp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logit_distillation_loss(student_logits, teacher_logits, temperature=1.0, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the knowledge distillation loss between student and teacher logits.\n",
    "\n",
    "    Args:\n",
    "        student_logits: Logits from the student model [batch_size, seq_len, vocab_size]\n",
    "        teacher_logits: Logits from the teacher model [batch_size, seq_len, vocab_size]\n",
    "        temperature: Temperature parameter to soften the distributions\n",
    "        alpha: Weight for the distillation loss (1-alpha for the regular CE loss)\n",
    "\n",
    "    Returns:\n",
    "        The distillation loss\n",
    "    \"\"\"\n",
    "    # Apply temperature scaling\n",
    "    student_logits_scaled = student_logits / temperature\n",
    "    teacher_logits_scaled = teacher_logits / temperature\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    student_probs = F.softmax(student_logits_scaled, dim=-1)\n",
    "    teacher_probs = F.softmax(teacher_logits_scaled, dim=-1)\n",
    "\n",
    "    # Calculate KL divergence loss\n",
    "    kl_div = F.kl_div(\n",
    "        F.log_softmax(student_logits, dim=-1),\n",
    "        F.softmax(teacher_logits, dim=-1, dtype=torch.float32),  # Specify dtype\n",
    "        reduction='batchmean',\n",
    "        log_target=False  # Teacher probs are not in log space\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rX-BCQrvlUm4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_with_logit_distillation(\n",
    "    model, train_dataloader, optimizer, scheduler=None,\n",
    "    num_epochs=3, device=\"cuda\", alpha=0.5, temperature=2.0,\n",
    "    max_grad_norm=1.0):\n",
    "    \"\"\"\n",
    "    Train a model using logit distillation.\n",
    "\n",
    "    Args:\n",
    "        model: The student model to train\n",
    "        train_dataloader: DataLoader containing training examples with teacher logits\n",
    "        optimizer: Optimizer for training\n",
    "        scheduler: Learning rate scheduler (optional)\n",
    "        num_epochs: Number of training epochs\n",
    "        device: Device to use for training\n",
    "        alpha: Weight for distillation loss vs standard cross-entropy loss\n",
    "        temperature: Temperature for softening logit distributions\n",
    "        max_grad_norm: Maximum gradient norm for clipping\n",
    "\n",
    "    Returns:\n",
    "        Trained model and training losses\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    # Create cross entropy loss for regular training\n",
    "    ce_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            model_device = next(model.parameters()).device\n",
    "            input_ids = batch[\"input_ids\"].to(model_device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model_device)\n",
    "            labels = batch[\"labels\"].to(model_device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "\n",
    "            # Standard cross-entropy loss from labels\n",
    "            ce_loss = outputs.loss\n",
    "\n",
    "            # Get student logits\n",
    "            student_logits = outputs.logits\n",
    "\n",
    "            # Extract teacher logits if available and calculate distillation loss\n",
    "            total_loss = ce_loss\n",
    "            if \"teacher_logits\" in batch:\n",
    "                teacher_logits = batch[\"teacher_logits\"].to(device)\n",
    "\n",
    "                # Make sure teacher_logits has the same shape as student_logits\n",
    "                if teacher_logits.shape != student_logits.shape:\n",
    "                    # Handle different sequence lengths if needed\n",
    "                    min_len = min(teacher_logits.shape[1], student_logits.shape[1])\n",
    "                    teacher_logits = teacher_logits[:, :min_len, :]\n",
    "                    student_logits = student_logits[:, :min_len, :]\n",
    "\n",
    "                # Calculate distillation loss\n",
    "                kd_loss = logit_distillation_loss(\n",
    "                    student_logits,\n",
    "                    teacher_logits,\n",
    "                    temperature=temperature\n",
    "                )\n",
    "\n",
    "                # Combine losses\n",
    "                total_loss = (1 - alpha) * ce_loss + alpha * kd_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            # Track loss\n",
    "            epoch_losses.append(total_loss.item())\n",
    "            progress_bar.set_postfix({\"loss\": total_loss.item()})\n",
    "\n",
    "        # Calculate and report average loss for the epoch\n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JENI9d5P2UDu",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ePM5WNnP2UDu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_student_model(student_model, student_tokenizer, test_problems, teacher_model=None,\n",
    "                          batch_size=4, max_length=512, output_dir=\"results/evaluations\"):\n",
    "    \"\"\"\n",
    "    Evaluate the student model on a set of test problems.\n",
    "\n",
    "    Args:\n",
    "        student_model: Trained student model\n",
    "        student_tokenizer: Tokenizer for the student model\n",
    "        test_problems: List of test problems to evaluate on\n",
    "        teacher_model: Optional teacher model for comparison\n",
    "        batch_size: Batch size for evaluation\n",
    "        max_length: Maximum sequence length for generation\n",
    "        output_dir: Directory to save evaluation results\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    logger.info(f\"Evaluating student model on {len(test_problems)} test problems\")\n",
    "\n",
    "    # Set models to evaluation mode\n",
    "    student_model.eval()\n",
    "    if teacher_model is not None:\n",
    "        teacher_model.eval()\n",
    "\n",
    "    results = {\n",
    "        \"total_problems\": len(test_problems),\n",
    "        \"student_generations\": [],\n",
    "        \"teacher_generations\": [] if teacher_model else None,\n",
    "        \"prompts\": []\n",
    "    }\n",
    "\n",
    "    # Process test problems in batches\n",
    "    for i in range(0, len(test_problems), batch_size):\n",
    "        batch_problems = test_problems[i:i+batch_size]\n",
    "        batch_prompts = []\n",
    "\n",
    "        for problem in batch_problems:\n",
    "            prompt = PROMPT_TEMPLATE.format(problem=problem[\"problem\"])\n",
    "            batch_prompts.append(prompt)\n",
    "            results[\"prompts\"].append(prompt)\n",
    "\n",
    "        # Generate solutions with student model\n",
    "        student_outputs = []\n",
    "        for prompt in tqdm(batch_prompts, desc=\"Generating student solutions\"):\n",
    "            inputs = student_tokenizer(prompt, return_tensors=\"pt\").to(student_model.device)\n",
    "\n",
    "            student_model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = student_model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=max_length,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    num_return_sequences=1\n",
    "                )\n",
    "\n",
    "            decoded_output = student_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            student_outputs.append(decoded_output)\n",
    "\n",
    "        results[\"student_generations\"].extend(student_outputs)\n",
    "\n",
    "        # If teacher model is provided, generate solutions for comparison\n",
    "        if teacher_model:\n",
    "            teacher_outputs = []\n",
    "            for prompt in tqdm(batch_prompts, desc=\"Generating teacher solutions\"):\n",
    "                inputs = student_tokenizer(prompt, return_tensors=\"pt\").to(teacher_model.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = teacher_model.generate(\n",
    "                        **inputs,\n",
    "                        max_length=max_length,\n",
    "                        temperature=0.7,\n",
    "                        do_sample=True,\n",
    "                        top_p=0.9,\n",
    "                        num_return_sequences=1\n",
    "                    )\n",
    "\n",
    "                decoded_output = student_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "                teacher_outputs.append(decoded_output)\n",
    "\n",
    "            results[\"teacher_generations\"].extend(teacher_outputs)\n",
    "\n",
    "    # Process and extract solutions\n",
    "    logger.info(\"Processing generated solutions\")\n",
    "    student_solutions = []\n",
    "    teacher_solutions = [] if teacher_model else None\n",
    "\n",
    "    for output in results[\"student_generations\"]:\n",
    "        solution_start_marker = \"Step-by-step solution:\"\n",
    "        solution_start_idx = output.find(solution_start_marker) + len(solution_start_marker)\n",
    "        solution = output[solution_start_idx:].strip()\n",
    "        student_solutions.append(solution)\n",
    "\n",
    "    if teacher_model:\n",
    "        for output in results[\"teacher_generations\"]:\n",
    "            solution_start_marker = \"Step-by-step solution:\"\n",
    "            solution_start_idx = output.find(solution_start_marker) + len(solution_start_marker)\n",
    "            solution = output[solution_start_idx:].strip()\n",
    "            teacher_solutions.append(solution)\n",
    "\n",
    "    # Calculate some basic metrics\n",
    "    logger.info(\"Calculating evaluation metrics\")\n",
    "\n",
    "    # Calculate average solution length\n",
    "    student_avg_length = sum(len(solution.split()) for solution in student_solutions) / len(student_solutions)\n",
    "    results[\"student_avg_word_count\"] = student_avg_length\n",
    "\n",
    "    if teacher_model:\n",
    "        teacher_avg_length = sum(len(solution.split()) for solution in teacher_solutions) / len(teacher_solutions)\n",
    "        results[\"teacher_avg_word_count\"] = teacher_avg_length\n",
    "        results[\"length_ratio\"] = student_avg_length / teacher_avg_length if teacher_avg_length > 0 else 0\n",
    "\n",
    "    # Check for step-by-step reasoning keywords\n",
    "    reasoning_keywords = [\"first\", \"second\", \"third\", \"next\", \"then\", \"finally\", \"step\", \"let's\", \"because\", \"reason\"]\n",
    "    student_keyword_counts = []\n",
    "\n",
    "    for solution in student_solutions:\n",
    "        solution_lower = solution.lower()\n",
    "        count = sum(1 for keyword in reasoning_keywords if keyword in solution_lower)\n",
    "        student_keyword_counts.append(count)\n",
    "\n",
    "    results[\"student_avg_reasoning_markers\"] = sum(student_keyword_counts) / len(student_keyword_counts)\n",
    "\n",
    "    if teacher_model:\n",
    "        teacher_keyword_counts = []\n",
    "        for solution in teacher_solutions:\n",
    "            solution_lower = solution.lower()\n",
    "            count = sum(1 for keyword in reasoning_keywords if keyword in solution_lower)\n",
    "            teacher_keyword_counts.append(count)\n",
    "\n",
    "        results[\"teacher_avg_reasoning_markers\"] = sum(teacher_keyword_counts) / len(teacher_keyword_counts)\n",
    "        results[\"reasoning_marker_ratio\"] = (results[\"student_avg_reasoning_markers\"] /\n",
    "                                           results[\"teacher_avg_reasoning_markers\"]\n",
    "                                           if results[\"teacher_avg_reasoning_markers\"] > 0 else 0)\n",
    "\n",
    "    # Save a few example comparisons\n",
    "    with open(os.path.join(output_dir, \"solution_examples.txt\"), \"w\") as f:\n",
    "        for i in range(min(5, len(student_solutions))):\n",
    "            f.write(f\"Problem {i+1}:\\n\")\n",
    "            f.write(f\"{results['prompts'][i]}\\n\\n\")\n",
    "            f.write(f\"Student solution:\\n{student_solutions[i]}\\n\\n\")\n",
    "            if teacher_model:\n",
    "                f.write(f\"Teacher solution:\\n{teacher_solutions[i]}\\n\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "    # Save all evaluation results\n",
    "    with open(os.path.join(output_dir, \"evaluation_results.json\"), \"w\") as f:\n",
    "        # Create a summary version without the full generations for easier reading\n",
    "        summary_results = {k: v for k, v in results.items()\n",
    "                         if k not in [\"student_generations\", \"teacher_generations\", \"prompts\"]}\n",
    "        json.dump(summary_results, f, indent=2)\n",
    "\n",
    "    # Save the full results separately\n",
    "    with open(os.path.join(output_dir, \"full_results.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Evaluation complete. Results saved to {output_dir}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qS8JBDuQ2UDv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def track_best_model(evaluation_results, best_metrics, model_path, output_dir=\"results/best_model\"):\n",
    "    \"\"\"\n",
    "    Track and save the best student model based on evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        evaluation_results: Results dictionary from evaluate_student_model\n",
    "        best_metrics: Dictionary with current best metrics\n",
    "        model_path: Path to the current model\n",
    "        output_dir: Directory to save the best model\n",
    "\n",
    "    Returns:\n",
    "        Updated best_metrics dictionary\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define a scoring function to rank models (higher is better)\n",
    "    # Here we prioritize reasoning marker ratio and solution length ratio\n",
    "    current_score = (\n",
    "        evaluation_results.get(\"reasoning_marker_ratio\", 0) * 0.7 +\n",
    "        evaluation_results.get(\"length_ratio\", 0) * 0.3\n",
    "    )\n",
    "\n",
    "    best_score = (\n",
    "        best_metrics.get(\"reasoning_marker_ratio\", 0) * 0.7 +\n",
    "        best_metrics.get(\"length_ratio\", 0) * 0.3\n",
    "    )\n",
    "\n",
    "    # Check if current model is better than the best so far\n",
    "    if current_score > best_score:\n",
    "        logger.info(f\"New best model found! Score: {current_score:.4f} (previous: {best_score:.4f})\")\n",
    "\n",
    "        # Update best metrics\n",
    "        best_metrics = {\n",
    "            \"score\": current_score,\n",
    "            \"model_path\": model_path,\n",
    "            \"reasoning_marker_ratio\": evaluation_results.get(\"reasoning_marker_ratio\", 0),\n",
    "            \"length_ratio\": evaluation_results.get(\"length_ratio\", 0),\n",
    "            \"student_avg_reasoning_markers\": evaluation_results.get(\"student_avg_reasoning_markers\", 0),\n",
    "            \"student_avg_word_count\": evaluation_results.get(\"student_avg_word_count\", 0)\n",
    "        }\n",
    "\n",
    "        # Copy the model to the best model directory\n",
    "        if os.path.exists(model_path):\n",
    "            logger.info(f\"Copying best model from {model_path} to {output_dir}\")\n",
    "\n",
    "            # Clear previous best model\n",
    "            if os.path.exists(output_dir):\n",
    "                for file in os.listdir(output_dir):\n",
    "                    file_path = os.path.join(output_dir, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "\n",
    "            # Copy new best model\n",
    "            for file in os.listdir(model_path):\n",
    "                source_file = os.path.join(model_path, file)\n",
    "                if os.path.isfile(source_file):\n",
    "                    shutil.copy(source_file, os.path.join(output_dir, file))\n",
    "\n",
    "        # Save best metrics\n",
    "        with open(os.path.join(output_dir, \"best_metrics.json\"), \"w\") as f:\n",
    "            json.dump(best_metrics, f, indent=2)\n",
    "\n",
    "    return best_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgfrDq5qFP78",
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Params and Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xEqcyVZA5mtZ"
   },
   "outputs": [],
   "source": [
    "# Global Params\n",
    "TEACHER_EXAMPLE_LEN = 5 # number of train mbpp problems\n",
    "STUDENT_EXAMPLE_LEN = 5\n",
    "GENERATED_TOKEN_LEN = 100\n",
    "\n",
    "# Training Params\n",
    "NUM_EPOCHS = 6\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 10\n",
    "WARMUP_STEPS = TEACHER_EXAMPLE_LEN * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "llYcwAkM5FW7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MBPP dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 01:43:49,745 - INFO - Loading teacher model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 374 train problems and 500 evaluation problems from MBPP dataset\n",
      "Loading Instruct models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javvaji.m/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-04-18 01:43:51,477 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2a8de9260a4822ab6b6b596e226270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javvaji.m/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2025-04-18 01:44:00,373 - INFO - Teacher model loaded successfully\n",
      "2025-04-18 01:44:00,374 - INFO - Loading student model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-04-18 01:44:00,659 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-04-18 01:44:01,332 - INFO - Student model loaded successfully\n",
      "2025-04-18 01:44:01,333 - INFO - Loading teacher model: Qwen/Qwen2.5-Coder-7B-Instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Coder models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-04-18 01:44:03,088 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fb72d144d74d62b99b0bf39c7eb3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 01:44:11,427 - INFO - Teacher model loaded successfully\n",
      "2025-04-18 01:44:11,428 - INFO - Loading student model: Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2025-04-18 01:44:11,724 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-04-18 01:44:12,371 - INFO - Student model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "print(\"Loading MBPP dataset...\")\n",
    "mbpp_train_examples, mbpp_test_examples = load_mbpp_dataset()\n",
    "\n",
    "print(\"Loading Instruct models...\")\n",
    "teacher_model, tokenizer, student_model, student_tokenizer = load_models(\"Qwen/Qwen2.5-7B-Instruct\", \"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "\n",
    "print(\"Loading Coder models...\")\n",
    "code_teacher_model, code_tokenizer, code_student_model, code_student_tokenizer = load_models(\"Qwen/Qwen2.5-Coder-7B-Instruct\", \"Qwen/Qwen2.5-Coder-0.5B-Instruct\")\n",
    "\n",
    "# save student model initial states for efficient memory storage when fine tuning later\n",
    "student_initial_state = {k: v.detach().clone() for k, v in student_model.state_dict().items()}\n",
    "code_student_initial_state = {k: v.detach().clone() for k, v in code_student_model.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "DpMPflAGee4h"
   },
   "outputs": [],
   "source": [
    "def generate_base_model_examples(train_problems, test_problems, teacher_model, student_model, tokenizer):\n",
    "    print(f\"Generating {SOLUTION_FIELD} examples using Teacher model...\")\n",
    "    train_examples = generate_dataset(\n",
    "        train_problems,\n",
    "        PROMPT_TEMPLATE,\n",
    "        SOLUTION_FIELD,\n",
    "        OUTPUT_MARKER,\n",
    "        teacher_model,\n",
    "        tokenizer,\n",
    "        num_examples=TEACHER_EXAMPLE_LEN,\n",
    "        max_new_tokens=GENERATED_TOKEN_LEN\n",
    "    )\n",
    "\n",
    "    print(f\"Generating {SOLUTION_FIELD} examples using untrained Student model...\")\n",
    "    untrained_examples = generate_dataset(\n",
    "        test_problems,\n",
    "        PROMPT_TEMPLATE,\n",
    "        SOLUTION_FIELD,\n",
    "        OUTPUT_MARKER,\n",
    "        student_model,\n",
    "        tokenizer,\n",
    "        num_examples=STUDENT_EXAMPLE_LEN,\n",
    "        max_new_tokens=GENERATED_TOKEN_LEN,\n",
    "        teacher=False\n",
    "    )\n",
    "\n",
    "    return train_examples, untrained_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "WPMrOcQthBnH"
   },
   "outputs": [],
   "source": [
    "def start_fine_tuning(student_model, tokenizer, train_examples):\n",
    "  print(f\"Fine-Tuning {SOLUTION_FIELD} on Student model...\")\n",
    "  trained_student_model = fine_tune_student_model(\n",
    "      student_model=student_model,\n",
    "      student_tokenizer=tokenizer,\n",
    "      train_data=train_examples,\n",
    "      prompt=PROMPT_TEMPLATE,\n",
    "      output_field=SOLUTION_FIELD,\n",
    "      batch_size=BATCH_SIZE,\n",
    "      num_epochs=NUM_EPOCHS,\n",
    "      learning_rate=LEARNING_RATE,\n",
    "      warmup_steps=WARMUP_STEPS,\n",
    "      max_length=GENERATED_TOKEN_LEN\n",
    "  )\n",
    "\n",
    "  return trained_student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZhIOlsM0iZ3A"
   },
   "outputs": [],
   "source": [
    "def generate_trained_model_examples(test_problems, trained_student_model, tokenizer):\n",
    "    print(f\"Generating {SOLUTION_FIELD} examples using Trained Student model...\")\n",
    "    print(SOLUTION_FIELD)\n",
    "    print(OUTPUT_MARKER)\n",
    "    print(PROMPT_TEMPLATE)\n",
    "    trained_examples = generate_dataset(\n",
    "        test_problems,\n",
    "        PROMPT_TEMPLATE,\n",
    "        SOLUTION_FIELD,\n",
    "        OUTPUT_MARKER,\n",
    "        trained_student_model,\n",
    "        tokenizer,\n",
    "        num_examples=STUDENT_EXAMPLE_LEN+1, # add 1 to not overwrite the untrained student data file\n",
    "        max_new_tokens=GENERATED_TOKEN_LEN,\n",
    "        teacher=False\n",
    "    )\n",
    "\n",
    "    return trained_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Restart and Run all above\n",
    "In case of disk/memory filling, restart the kernel and run cells above here. Then load data generated so far from json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEujXBBEVQ9Z",
    "tags": []
   },
   "source": [
    "## CoT Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "EqccoRV4nKQG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clear_gpu_memory()\n",
    "teacher_model.to(device)\n",
    "code_teacher_model.cpu\n",
    "student_model.to(device)\n",
    "code_student_model.cpu\n",
    "\n",
    "# CoT Agent Params\n",
    "PROMPT_TEMPLATE = COT_PROMPT_TEMPLATE\n",
    "SOLUTION_FIELD = \"solution_cot\"\n",
    "OUTPUT_MARKER = \"Step-by-step solution:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "PEkb1GQgVUDu",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 01:44:12,692 - INFO - Generating solution_cot with teacher for 5 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating solution_cot examples using Teacher model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:  20%|        | 1/5 [00:02<00:11,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a function to find the longest chain which can be formed from the given set of pairs....\n",
      "Solution (first 150 chars): 1. Understand inputs and outputs\n",
      "   - Input: A list of n pairs (a, b) where 0  a < b  10^9\n",
      "   - Output: The length of the longest chain that can be ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:  40%|      | 2/5 [00:05<00:07,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a python function to find the first repeated character in a given string....\n",
      "Solution (first 150 chars): 1. Understand inputs and outputs\n",
      "   - Input: A string of characters (can contain upper and lower case letters, numbers, special characters)\n",
      "   - Outpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot: 100%|| 5/5 [00:12<00:00,  2.47s/it]\n",
      "2025-04-18 01:44:25,053 - INFO - Successfully generated 5 solution_cot solutions\n",
      "2025-04-18 01:44:25,060 - INFO - Dataset saved to dataset/solution_cot_teacher_5_dataset.json\n",
      "2025-04-18 01:44:25,060 - INFO - Generating solution_cot with student for 5 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating solution_cot examples using untrained Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:  20%|        | 1/5 [00:01<00:07,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): 1. Define the function `remove_first_last_occurrence` that takes two parameters: `s` (the input string) and `char` (a single character).\n",
      "2. Initialize...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:  40%|      | 2/5 [00:03<00:05,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): 1. Initialize an empty list called \"result\" to store the sorted matrix elements.\n",
      "\n",
      "2. Iterate through each row of the matrix using a nested loop:\n",
      "\n",
      "   -...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot: 100%|| 5/5 [00:09<00:00,  1.85s/it]\n",
      "2025-04-18 01:44:34,311 - INFO - Successfully generated 5 solution_cot solutions\n",
      "2025-04-18 01:44:34,320 - INFO - Dataset saved to dataset/solution_cot_student_5_dataset.json\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "student_model.load_state_dict(student_initial_state)\n",
    "train_cot_examples, untrained_cot_examples = generate_base_model_examples(\n",
    "    mbpp_train_examples,\n",
    "    mbpp_test_examples,\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "3spoWpDH2BD9"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the examples from json\n",
    "\n",
    "# clear_gpu_memory()\n",
    "\n",
    "# mdpp_examples_file = open(f\"dataset/{SOLUTION_FIELD}_teacher_{TEACHER_EXAMPLE_LEN}_dataset.json\")\n",
    "# train_cot_examples = json.load(mdpp_examples_file)\n",
    "\n",
    "# print(train_cot_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "jPV6i43RnKQH",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 01:44:34,450 - INFO - Starting training the student model for 6 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning solution_cot on Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|| 1/1 [00:00<00:00,  3.70it/s, loss=2.92]\n",
      "2025-04-18 01:44:35,718 - INFO - Epoch 1/6 - Average loss: 2.9186\n",
      "Epoch 2/6: 100%|| 1/1 [00:00<00:00,  6.66it/s, loss=0.616]\n",
      "2025-04-18 01:44:35,869 - INFO - Epoch 2/6 - Average loss: 0.6162\n",
      "Epoch 3/6: 100%|| 1/1 [00:00<00:00,  6.70it/s, loss=0.341]\n",
      "2025-04-18 01:44:36,020 - INFO - Epoch 3/6 - Average loss: 0.3409\n",
      "Epoch 4/6: 100%|| 1/1 [00:00<00:00,  6.69it/s, loss=0.341]\n",
      "2025-04-18 01:44:36,170 - INFO - Epoch 4/6 - Average loss: 0.3408\n",
      "Epoch 5/6: 100%|| 1/1 [00:00<00:00,  6.71it/s, loss=0.302]\n",
      "2025-04-18 01:44:36,320 - INFO - Epoch 5/6 - Average loss: 0.3017\n",
      "Epoch 6/6: 100%|| 1/1 [00:00<00:00,  6.70it/s, loss=0.274]\n",
      "2025-04-18 01:44:36,471 - INFO - Epoch 6/6 - Average loss: 0.2744\n",
      "2025-04-18 01:44:36,471 - INFO - Training completed. Saving final model to results/student_model_solution_cot_final\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "# Fine-tune the student model\n",
    "student_model.load_state_dict(student_initial_state)\n",
    "trained_cot_student_model = start_fine_tuning(student_model, tokenizer, train_cot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "W82IwgDA2BED"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the trained model from files\n",
    "\n",
    "# trained_student_path = f\"results/student_model_{SOLUTION_FIELD}_final\"\n",
    "# trained_student_model = AutoModelForCausalLM.from_pretrained(trained_student_path).to(device)\n",
    "# trained_tokenizer = AutoTokenizer.from_pretrained(trained_student_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "RSpu8jkY2BED",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 01:44:41,381 - INFO - Generating solution_cot with student for 6 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating solution_cot examples using Trained Student model...\n",
      "solution_cot\n",
      "Step-by-step solution:\n",
      "Generate a detailed step-by-step solution for this coding problem.\n",
      "Break down your thought process clearly, explaining your reasoning while considering:\n",
      "- What are the inputs and outputs of the function?\n",
      "- What algorithm or data structure is most appropriate?\n",
      "- Are there any edge cases to handle?\n",
      "- What's the efficiency of your approach?\n",
      "\n",
      "Be concise in your explanation.\n",
      "\n",
      "Problem:\n",
      "{problem}\n",
      "\n",
      "Step-by-step solution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:  17%|        | 1/6 [00:01<00:09,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): 1. Understand inputs and outputs\n",
      "   - Input: \"abcdefg\"\n",
      "     - Outputs: \"bdf\"\n",
      "   - Input: \"abcdef\"\n",
      "     - Outputs: \"\"\n",
      "   - Input: \"abcedfg\"\n",
      "     - Outp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot:  33%|      | 2/6 [00:03<00:07,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): 1. Understand inputs & outputs\n",
      "   - Input: A 2D array (matrix)\n",
      "   - Output: Sorted row sums\n",
      "\n",
      "   - Understand inputs & outputs\n",
      "   - Understand inputs &...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating solution_cot: 100%|| 6/6 [00:11<00:00,  1.86s/it]\n",
      "2025-04-18 01:44:52,540 - INFO - Successfully generated 6 solution_cot solutions\n",
      "2025-04-18 01:44:52,545 - INFO - Dataset saved to dataset/solution_cot_student_6_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Generate fine-tuned student model outputs\n",
    "trained_cot_examples = generate_trained_model_examples(mbpp_test_examples, trained_cot_student_model, tokenizer)\n",
    "\n",
    "# print(\"Evaluating CoT student model...\")\n",
    "# evaluation_results = evaluate_student_model(\n",
    "#     student_model=student_model,\n",
    "#     student_tokenizer=student_tokenizer,\n",
    "#     test_problems=test_problems,\n",
    "#     teacher_model=teacher_model,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     max_length=GENERATED_TOKEN_LEN,\n",
    "#     output_dir=\"results/evaluations\"\n",
    "# )\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj_ICDvhmYWF",
    "tags": []
   },
   "source": [
    "## Coder Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Kp9ioW4XmYWG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clear_gpu_memory()\n",
    "teacher_model.cpu()\n",
    "code_teacher_model.to(device)\n",
    "student_model.cpu()\n",
    "code_student_model.to(device)\n",
    "\n",
    "# CoT Agent Params\n",
    "PROMPT_TEMPLATE = CODER_PROMPT_TEMPLATE\n",
    "SOLUTION_FIELD = \"code\"\n",
    "OUTPUT_MARKER = \"Python code:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "feu-GFfqmYWG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 01:45:03,036 - INFO - Generating code with teacher for 5 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating code examples using Teacher model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:  20%|        | 1/5 [00:02<00:08,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a function to find the longest chain which can be formed from the given set of pairs....\n",
      "Solution (first 150 chars): ```python def findLongestChain(pairs): pairs.sort(key=lambda x: x[1]) dp = [1] * len(pairs) for i in range(1, len(pairs)): for j in range(i): if pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:  40%|      | 2/5 [00:02<00:04,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a python function to find the first repeated character in a given string....\n",
      "Solution (first 150 chars): ```python def find_first_duplicate_char(s): seen = set() for char in s: if char in seen: return char seen.add(char) return None ```\n",
      "```...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code: 100%|| 5/5 [00:09<00:00,  1.97s/it]\n",
      "2025-04-18 01:45:12,907 - INFO - Successfully generated 5 code solutions\n",
      "2025-04-18 01:45:12,911 - INFO - Dataset saved to dataset/code_teacher_5_dataset.json\n",
      "2025-04-18 01:45:12,912 - INFO - Generating code with student for 5 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating code examples using untrained Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:  20%|        | 1/5 [00:01<00:07,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): ```python\n",
      "def reverse_string(input_str):\n",
      "    # Reverse the string using slicing\n",
      "    return input_str[::-1]\n",
      "\n",
      "# Test cases\n",
      "print(reverse_string(\"abcdefg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:  40%|      | 2/5 [00:03<00:05,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): ```python\n",
      "def sorted_row_sums(matrix):\n",
      "    # Sort the rows based on their sum\n",
      "    sorted_rows = sorted(matrix, key=sum)\n",
      "    return sorted_rows\n",
      "\n",
      "# Exam...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code: 100%|| 5/5 [00:09<00:00,  1.86s/it]\n",
      "2025-04-18 01:45:22,234 - INFO - Successfully generated 5 code solutions\n",
      "2025-04-18 01:45:22,245 - INFO - Dataset saved to dataset/code_student_5_dataset.json\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "code_student_model.load_state_dict(code_student_initial_state)\n",
    "train_code_examples, untrained_code_examples = generate_base_model_examples(\n",
    "    train_cot_examples,\n",
    "    trained_cot_examples,\n",
    "    code_teacher_model,\n",
    "    code_student_model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "NuwbUK2lmYWG"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the examples from json\n",
    "\n",
    "# clear_gpu_memory()\n",
    "\n",
    "# train_code_examples_file = open(f\"dataset/{SOLUTION_FIELD}_teacher_{TEACHER_EXAMPLE_LEN}_dataset.json\")\n",
    "# train_code_examples = json.load(train_code_examples_file)\n",
    "\n",
    "# print(train_code_examples[0])\n",
    "\n",
    "# trained_cot_examples_file = open(f\"dataset/solution_cot_student_{STUDENT_EXAMPLE_LEN+1}_dataset.json\")\n",
    "# trained_cot_examples = json.load(trained_cot_examples_file)\n",
    "\n",
    "# print(trained_cot_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "WiF4P6bSmYWG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 01:45:22,400 - INFO - Starting training the student model for 6 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning code on Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|| 1/1 [00:00<00:00,  5.96it/s, loss=nan]\n",
      "2025-04-18 01:45:22,571 - INFO - Epoch 1/6 - Average loss: nan\n",
      "Epoch 2/6: 100%|| 1/1 [00:00<00:00,  6.64it/s, loss=nan]\n",
      "2025-04-18 01:45:22,723 - INFO - Epoch 2/6 - Average loss: nan\n",
      "Epoch 3/6: 100%|| 1/1 [00:00<00:00,  6.62it/s, loss=nan]\n",
      "2025-04-18 01:45:22,876 - INFO - Epoch 3/6 - Average loss: nan\n",
      "Epoch 4/6: 100%|| 1/1 [00:00<00:00,  6.63it/s, loss=nan]\n",
      "2025-04-18 01:45:23,028 - INFO - Epoch 4/6 - Average loss: nan\n",
      "Epoch 5/6: 100%|| 1/1 [00:00<00:00,  6.63it/s, loss=nan]\n",
      "2025-04-18 01:45:23,180 - INFO - Epoch 5/6 - Average loss: nan\n",
      "Epoch 6/6: 100%|| 1/1 [00:00<00:00,  6.66it/s, loss=nan]\n",
      "2025-04-18 01:45:23,331 - INFO - Epoch 6/6 - Average loss: nan\n",
      "2025-04-18 01:45:23,331 - INFO - Training completed. Saving final model to results/student_model_code_final\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "# Fine-tune the student model\n",
    "code_student_model.load_state_dict(code_student_initial_state)\n",
    "trained_code_student_model = start_fine_tuning(code_student_model, tokenizer, train_code_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mH0JEK1kmYWH"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the trained model from files\n",
    "\n",
    "# trained_student_path = f\"results/student_model_{SOLUTION_FIELD}_final\"\n",
    "# trained_student_model = AutoModelForCausalLM.from_pretrained(trained_student_path).to(device)\n",
    "# trained_tokenizer = AutoTokenizer.from_pretrained(trained_student_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "p63Lz1d6mYWH",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 01:45:28,397 - INFO - Generating code with student for 6 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating code examples using Trained Student model...\n",
      "code\n",
      "Python code:\n",
      "Generate the python code for this coding problem. Follow the\n",
      "step-by-step process as a guideline for how to solve the problem. Only return python code.\n",
      "\n",
      "Step-by-step solution:\n",
      "{solution_cot}\n",
      "\n",
      "Python code:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:  17%|        | 1/6 [00:01<00:09,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): ```python\n",
      "def remove_char(input_string):\n",
      "    # Initialize an empty string to store the result\n",
      "    result = \"\"\n",
      "    \n",
      "    # Iterate over each character i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code:  33%|      | 2/6 [00:03<00:07,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): To solve this problem, we can use the built-in `sorted()` function in Python. This function sorts the elements of an iterable (like a list or tuple) a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating code: 100%|| 6/6 [00:11<00:00,  1.87s/it]\n",
      "2025-04-18 01:45:39,593 - INFO - Successfully generated 6 code solutions\n",
      "2025-04-18 01:45:39,609 - INFO - Dataset saved to dataset/code_student_6_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# Generate Code examples using Trained Student model\n",
    "trained_code_examples = generate_trained_model_examples(trained_cot_examples, trained_code_student_model, tokenizer)\n",
    "\n",
    "# print(\"Evaluating CoT student model...\")\n",
    "# evaluation_results = evaluate_student_model(\n",
    "#     student_model=student_model,\n",
    "#     student_tokenizer=student_tokenizer,\n",
    "#     test_problems=test_problems,\n",
    "#     teacher_model=teacher_model,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     max_length=GENERATED_TOKEN_LEN,\n",
    "#     output_dir=\"results/evaluations\"\n",
    "# )\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear CUDA cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHeEWY2In-LU",
    "tags": []
   },
   "source": [
    "## Debugger Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "9SLWObWSn-LU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clear_gpu_memory()\n",
    "teacher_model.cpu()\n",
    "code_teacher_model.to(device)\n",
    "student_model.cpu()\n",
    "code_student_model.to(device)\n",
    "\n",
    "# CoT Agent Params\n",
    "PROMPT_TEMPLATE = DEBUGGER_PROMPT_TEMPLATE\n",
    "SOLUTION_FIELD = \"debugged\"\n",
    "OUTPUT_MARKER = \"Debugged Python code:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "EtoWGgLUn-LV",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 01:45:39,871 - INFO - Generating debugged with teacher for 5 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating debugged examples using Teacher model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged:  40%|      | 2/5 [00:02<00:02,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a function to find the longest chain which can be formed from the given set of pairs....\n",
      "Solution (first 150 chars): ```python def findLongestChain(pairs): pairs.sort(key=lambda x: x[1]) dp = [1] * len(pairs) for i in range(1, len(pairs)): for j in range(i): if pairs...\n",
      "\n",
      "Example 2:\n",
      "Problem: Write a python function to find the first repeated character in a given string....\n",
      "Solution (first 150 chars): ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged: 100%|| 5/5 [00:07<00:00,  1.46s/it]\n",
      "2025-04-18 01:45:47,157 - INFO - Successfully generated 5 debugged solutions\n",
      "2025-04-18 01:45:47,161 - INFO - Dataset saved to dataset/debugged_teacher_5_dataset.json\n",
      "2025-04-18 01:45:47,162 - INFO - Generating debugged with student for 5 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating debugged examples using untrained Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged:  20%|        | 1/5 [00:01<00:07,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): ```python\n",
      "def remove_char(input_string):\n",
      "    # Initialize an empty string to store the result\n",
      "    result = \"\"\n",
      "    \n",
      "    # Iterate over each character i...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged:  40%|      | 2/5 [00:03<00:05,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): ```\n",
      "To solve this problem, we can use the built-in `sorted()` function in Python. This function sorts the elements of an iterable (like a list or tupl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged: 100%|| 5/5 [00:09<00:00,  1.84s/it]\n",
      "2025-04-18 01:45:56,382 - INFO - Successfully generated 5 debugged solutions\n",
      "2025-04-18 01:45:56,388 - INFO - Dataset saved to dataset/debugged_student_5_dataset.json\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "code_student_model.load_state_dict(code_student_initial_state)\n",
    "train_debug_examples, untrained_debug_examples = generate_base_model_examples(\n",
    "    train_code_examples,\n",
    "    trained_code_examples,\n",
    "    code_teacher_model,\n",
    "    code_student_model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "-UAHuDYfn-LV"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the examples from json\n",
    "\n",
    "# clear_gpu_memory()\n",
    "\n",
    "# train_debug_examples_file = open(f\"dataset/{SOLUTION_FIELD}_teacher_{TEACHER_EXAMPLE_LEN}_dataset.json\")\n",
    "# train_debug_examples = json.load(train_debug_examples_file)\n",
    "\n",
    "# print(train_debug_examples[0])\n",
    "\n",
    "# trained_code_examples_file = open(f\"dataset/code_student_{STUDENT_EXAMPLE_LEN+1}_dataset.json\")\n",
    "# trained_code_examples = json.load(trained_code_examples_file)\n",
    "\n",
    "# print(trained_code_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "3jb3TN5In-LV",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 01:45:56,532 - INFO - Starting training the student model for 6 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning debugged on Student model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|| 1/1 [00:00<00:00,  5.96it/s, loss=12.8]\n",
      "2025-04-18 01:45:56,703 - INFO - Epoch 1/6 - Average loss: 12.7680\n",
      "Epoch 2/6: 100%|| 1/1 [00:00<00:00,  6.65it/s, loss=2.85]\n",
      "2025-04-18 01:45:56,854 - INFO - Epoch 2/6 - Average loss: 2.8532\n",
      "Epoch 3/6: 100%|| 1/1 [00:00<00:00,  6.68it/s, loss=0.0325]\n",
      "2025-04-18 01:45:57,005 - INFO - Epoch 3/6 - Average loss: 0.0325\n",
      "Epoch 4/6: 100%|| 1/1 [00:00<00:00,  6.66it/s, loss=0.000152]\n",
      "2025-04-18 01:45:57,156 - INFO - Epoch 4/6 - Average loss: 0.0002\n",
      "Epoch 5/6: 100%|| 1/1 [00:00<00:00,  6.66it/s, loss=7.74e-6]\n",
      "2025-04-18 01:45:57,307 - INFO - Epoch 5/6 - Average loss: 0.0000\n",
      "Epoch 6/6: 100%|| 1/1 [00:00<00:00,  6.68it/s, loss=3.45e-6]\n",
      "2025-04-18 01:45:57,458 - INFO - Epoch 6/6 - Average loss: 0.0000\n",
      "2025-04-18 01:45:57,458 - INFO - Training completed. Saving final model to results/student_model_debugged_final\n"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "# Fine-tune the student model\n",
    "code_student_model.load_state_dict(code_student_initial_state)\n",
    "trained_debug_student_model = start_fine_tuning(code_student_model, tokenizer, train_debug_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "56nI0JqUn-LW"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the trained model from files\n",
    "\n",
    "# trained_student_path = f\"results/student_model_{SOLUTION_FIELD}_final\"\n",
    "# trained_student_model = AutoModelForCausalLM.from_pretrained(trained_student_path).to(device)\n",
    "# trained_tokenizer = AutoTokenizer.from_pretrained(trained_student_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Wyl_u_DHn-LW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 01:46:38,970 - INFO - Generating debugged with student for 6 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating debugged examples using Trained Student model...\n",
      "debugged\n",
      "Debugged Python code:\n",
      "Check the provided python code for any errors. Then regenerate\n",
      "the code so that any errors have been debugged.\n",
      "\n",
      "Python code:\n",
      "{code}\n",
      "\n",
      "Debugged Python code:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating debugged: 100%|| 6/6 [00:00<00:00, 42.32it/s]\n",
      "2025-04-18 01:46:39,114 - INFO - Successfully generated 6 debugged solutions\n",
      "2025-04-18 01:46:39,117 - INFO - Dataset saved to dataset/debugged_student_6_dataset.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a python function to remove first and last occurrence of a given character from the string....\n",
      "Solution (first 150 chars): ...\n",
      "\n",
      "Example 2:\n",
      "Problem: Write a function to sort a given matrix in ascending order according to the sum of its rows....\n",
      "Solution (first 150 chars): ...\n"
     ]
    }
   ],
   "source": [
    "# Generate debug examples using Trained Student model...\n",
    "trained_debug_examples = generate_trained_model_examples(trained_code_examples, trained_debug_student_model, tokenizer)\n",
    "\n",
    "# print(\"Evaluating CoT student model...\")\n",
    "# evaluation_results = evaluate_student_model(\n",
    "#     student_model=student_model,\n",
    "#     student_tokenizer=student_tokenizer,\n",
    "#     test_problems=test_problems,\n",
    "#     teacher_model=teacher_model,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     max_length=GENERATED_TOKEN_LEN,\n",
    "#     output_dir=\"results/evaluations\"\n",
    "# )\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear CUDA cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5rBXu9_qgqh",
    "tags": []
   },
   "source": [
    "## Explainer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "TuMJYGSaqgqh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clear_gpu_memory()\n",
    "teacher_model.to(device)\n",
    "code_teacher_model.cpu\n",
    "student_model.to(device)\n",
    "code_student_model.cpu\n",
    "\n",
    "# CoT Agent Params\n",
    "SOLUTION_FIELD = \"explanation\"\n",
    "OUTPUT_MARKER = \"Python code explanation:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aeGsGNoUqgqi",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 01:47:11,406 - INFO - Generating explanation with teacher for 5 problems...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating explanation examples using Teacher model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanation:  40%|      | 2/5 [00:02<00:03,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Problem: Write a function to find the longest chain which can be formed from the given set of pairs....\n",
      "Solution (first 150 chars): ```python \n",
      "def findLongestChain(pairs):\n",
      "    pairs.sort(key=lambda x: x[1])\n",
      "    dp = [1] * len(pairs)\n",
      "    for i in range(1, len(pairs)):\n",
      "        for j ...\n",
      "\n",
      "Example 2:\n",
      "Problem: Write a python function to find the first repeated character in a given string....\n",
      "Solution (first 150 chars): ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanation:  80%|  | 4/5 [00:05<00:01,  1.44s/it]"
     ]
    }
   ],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "#TODO: concat debug code examples to the train code examples(?)\n",
    "student_model.load_state_dict(student_initial_state)\n",
    "train_explain_examples, untrained_explain_examples = generate_base_model_examples(\n",
    "    train_code_examples,\n",
    "    trained_code_examples,\n",
    "    code_teacher_model,\n",
    "    code_student_model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGqxHzXAqgqi"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the examples from json\n",
    "\n",
    "# clear_gpu_memory()\n",
    "\n",
    "# mdpp_examples_file = open(f\"{SOLUTION_FIELD}_teacher_{TEACHER_EXAMPLE_LEN}_dataset.json\")\n",
    "# train_cot_examples = json.load(mdpp_examples_file)\n",
    "\n",
    "# print(train_cot_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5Wsi2sGqgqi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clear_gpu_memory()\n",
    "\n",
    "# Fine-tune the student model\n",
    "student_model.load_state_dict(student_initial_state)\n",
    "trained_explain_student_model = start_fine_tuning(student_model, tokenizer, train_explain_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgCdYmvJqgqi"
   },
   "outputs": [],
   "source": [
    "# # in case of disk/memory filling, this reloads the trained model from files\n",
    "\n",
    "# trained_student_path = f\"results/student_model_{SOLUTION_FIELD}_final\"\n",
    "# trained_student_model = AutoModelForCausalLM.from_pretrained(trained_student_path).to(device)\n",
    "# trained_tokenizer = AutoTokenizer.from_pretrained(trained_student_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4E2fK3Vqgqi"
   },
   "outputs": [],
   "source": [
    "# Generate explanation examples using Trained Student model\n",
    "trained_explain_examples = generate_trained_model_examples(trained_code_examples, trained_explain_student_model, tokenizer)\n",
    "\n",
    "# print(\"Evaluating CoT student model...\")\n",
    "# evaluation_results = evaluate_student_model(\n",
    "#     student_model=student_model,\n",
    "#     student_tokenizer=student_tokenizer,\n",
    "#     test_problems=test_problems,\n",
    "#     teacher_model=teacher_model,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     max_length=GENERATED_TOKEN_LEN,\n",
    "#     output_dir=\"results/evaluations\"\n",
    "# )\n",
    "\n",
    "torch.cuda.empty_cache()  # Clear CUDA cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgScz68PAJVB",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "8d90920837664839a349149dd2863e07",
      "98d07d8cd4d14ac9bcbc89891b27f60e",
      "4e2d211c0d02491488291ddf9ba0d4f4",
      "753528e3444e434582aeae980634a170",
      "44d63282b1e14665a18cf30dc2c0f6b6",
      "e61f3aadd36343e0954555f7ef39e9c9",
      "c92df514aa624d78b2678595793e41a9"
     ]
    },
    "id": "9yJx5XabAGP7",
    "outputId": "86f4d157-3d16-4fa7-d345-d58ea229ebe4"
   },
   "outputs": [],
   "source": [
    "def extract_problem_description(source_code):\n",
    "    \"\"\"\n",
    "    Extracts the problem description from the first docstring in the source code,\n",
    "    whether it's enclosed in triple double quotes or triple single quotes.\n",
    "    \"\"\"\n",
    "    docstring_pattern = re.compile(r'(\"\"\"|\\'\\'\\')(.*?)(\\1)', re.DOTALL)\n",
    "    match = docstring_pattern.search(source_code)\n",
    "\n",
    "    if match:\n",
    "        description = match.group(2)\n",
    "        # Clean up leading/trailing whitespace on each line\n",
    "        cleaned_lines = [line.strip() for line in description.strip().splitlines() if line.strip()]\n",
    "        return ' '.join(cleaned_lines)\n",
    "\n",
    "    raise Exception(f\"Error: Unable to extract problem description. Please check the format of the prompt:\\n{source_code}\")\n",
    "    return None\n",
    "\n",
    "def extract_code_header(source_code):\n",
    "    \"\"\"\n",
    "    Extracts everything from the beginning of the source code up to\n",
    "    the first occurrence of either triple single quotes or triple double quotes.\n",
    "    \"\"\"\n",
    "    # Match from start of string to the first triple quotes (single or double)\n",
    "    docstring_pattern = re.compile(r'^(.*?)(?=\"\"\"|\\'\\'\\')', re.DOTALL)\n",
    "    match = docstring_pattern.search(source_code)\n",
    "\n",
    "    if match:\n",
    "        header = match.group(1)\n",
    "        # Clean up leading/trailing whitespace on each line\n",
    "        cleaned_lines = [line.strip() for line in header.strip().splitlines() if line.strip()]\n",
    "        return ' '.join(cleaned_lines)\n",
    "    raise Exception(f\"Error: Unable to extract code header. Please check the format of the prompt:\\n{source_code}\")\n",
    "    return None\n",
    "\n",
    "def load_human_eval_dataset():\n",
    "    human_eval = load_dataset(\"openai_humaneval\")\n",
    "\n",
    "    train_problems = []\n",
    "    # Extract problems from the MBPP dataset with correct field names\n",
    "    for item in human_eval[\"test\"]:\n",
    "        train_problems.append({\n",
    "            \"problem\": extract_problem_description(item[\"prompt\"]),\n",
    "            \"code_header\": extract_code_header(item[\"prompt\"]),\n",
    "            \"test_case\": item[\"prompt\"],\n",
    "            \"solution_code\": item[\"prompt\"] + item[\"canonical_solution\"]\n",
    "        })\n",
    "    return train_problems\n",
    "\n",
    "COT_PROMPT_TEMPLATE = \"\"\"Generate a detailed step-by-step solution for this coding problem.\n",
    "Break down your thought process clearly, explaining your reasoning while considering:\n",
    "- What are the inputs and outputs of the function?\n",
    "- What algorithm or data structure is most appropriate?\n",
    "- Are there any edge cases to handle?\n",
    "- What's the efficiency of your approach?\n",
    "\n",
    "Be concise in your explanation.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Step-by-step solution:\"\"\"\n",
    "\n",
    "# CODER_PROMPT_TEMPLATE = \"\"\"Generate only a markdown code block that contains clean, efficient\n",
    "# Python code for this coding problem based on the solution approach. The code block must start\n",
    "# with ```python on its own line, then the code, and end with ``` on its own line.\n",
    "# Focus on:\n",
    "# - Implementing the key algorithmic insights\n",
    "# - Handling edge cases identified in the solution\n",
    "# - Maintaining readability and efficiency\n",
    "# Do not include:\n",
    "# - test cases\n",
    "# - extra code explanation\n",
    "\n",
    "# Step-by-step solution:\n",
    "# {cot_solution}\n",
    "\n",
    "# Python code:\n",
    "# {code_header}\"\"\"\n",
    "\n",
    "CODER_PROMPT_TEMPLATE = \"\"\"Generate only a markdown code block that contains clean, efficient\n",
    "Python code for this coding problem based on the solution approach. The code block must start\n",
    "with ```python on its own line, then the code, and end with ``` on its own line. Do not include\n",
    "test cases or code explanations.\n",
    "Focus on:\n",
    "- Implementing the key algorithmic insights\n",
    "- Handling edge cases identified in the solution\n",
    "- Maintaining readability and efficiency\n",
    "\n",
    "Step-by-step solution:\n",
    "{cot_solution}\n",
    "\n",
    "Python code:\n",
    "{code_header}\"\"\"\n",
    "\n",
    "\n",
    "human_eval = load_human_eval_dataset()\n",
    "print(human_eval[0])\n",
    "\n",
    "print(\"loaded dataset\")\n",
    "\n",
    "trained_cot_student_path = f\"results/student_model_cot_solution_final\"\n",
    "trained_cot_student_model = AutoModelForCausalLM.from_pretrained(trained_cot_student_path).to(device)\n",
    "trained_cot_tokenizer = AutoTokenizer.from_pretrained(trained_cot_student_path)\n",
    "\n",
    "untrained_coder_model_name = \"Qwen/Qwen2.5-Coder-0.5B\"\n",
    "untrained_coder_tokenizer = AutoTokenizer.from_pretrained(untrained_coder_model_name)\n",
    "untrained_coder_model = AutoModelForCausalLM.from_pretrained(\n",
    "    untrained_coder_model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "print(\"Loaded models\")\n",
    "\n",
    "trained_cot_examples = generate_dataset(\n",
    "    human_eval,\n",
    "    COT_PROMPT_TEMPLATE,\n",
    "    \"cot_solution\",\n",
    "    \"Step-by-step solution:\",\n",
    "    trained_cot_student_model,\n",
    "    trained_cot_tokenizer,\n",
    "    num_examples=100,\n",
    "    max_new_tokens=512,\n",
    "    teacher=False\n",
    ")\n",
    "\n",
    "print(\"cot examples generated\")\n",
    "\n",
    "code_examples = generate_dataset(\n",
    "    trained_cot_examples,\n",
    "    CODER_PROMPT_TEMPLATE,\n",
    "    \"gen_code\",\n",
    "    \"Python code:\",\n",
    "    untrained_coder_model,\n",
    "    untrained_coder_tokenizer,\n",
    "    num_examples=100,\n",
    "    max_new_tokens=512,\n",
    "    teacher=False\n",
    ")\n",
    "\n",
    "print(\"code generated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ko3EYETrAGP7",
    "outputId": "b8b4452d-e701-406a-ef21-8f5aeec6d686"
   },
   "outputs": [],
   "source": [
    "i = 22\n",
    "\n",
    "#print('\\nproblem:')\n",
    "#print(new_code_examples[i]['problem'])\n",
    "#print('\\ncot')\n",
    "#print(new_code_examples[i]['cot_solution'])\n",
    "#print('\\ngenerated_code')\n",
    "print(new_code_examples[i]['gen_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoAB5YRwAGP8",
    "outputId": "1ebcf404-76fa-440c-a49a-4d56cdca4b6e"
   },
   "outputs": [],
   "source": [
    "def extract_before_def(source_code):\n",
    "    \"\"\"\n",
    "    Extracts everything from the beginning of the source code up to\n",
    "    but not including the first occurrence of the 'def' keyword.\n",
    "    Preserves original formatting.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'^(.*?)(?=def)', re.DOTALL)\n",
    "    match = pattern.search(source_code)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    raise Exception(f\"Error: Unable to extract content before 'def'. No 'def' keyword found in:\\n{source_code}\")\n",
    "    return None\n",
    "\n",
    "def extract_until_code_block(source_code):\n",
    "    \"\"\"\n",
    "    Extracts everything from the beginning of the string up to\n",
    "    but not including the first occurrence of three backticks (```).\n",
    "    Preserves original formatting.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'^(.*?)(?=```)', re.DOTALL)\n",
    "    match = pattern.search(source_code)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return 'BAD'\n",
    "\n",
    "solutions = [item['solution_code'] for item in new_code_examples]\n",
    "generated_codes = [item['gen_code'] for item in new_code_examples]\n",
    "for i, generated_code in enumerate(generated_codes):\n",
    "    generated_codes[i] = extract_before_def(solutions[i]) + extract_until_code_block(generated_codes[i])\n",
    "print(generated_codes[0])\n",
    "\n",
    "def remove_bad_strings(string_array):\n",
    "    \"\"\"\n",
    "    Removes any strings containing 'BAD' from the given array.\n",
    "    Also prints the indices of removed strings.\n",
    "\n",
    "    Args:\n",
    "        string_array: A list of strings to filter\n",
    "\n",
    "    Returns:\n",
    "        A new list with all strings containing 'BAD' removed\n",
    "    \"\"\"\n",
    "    bad_indices = []\n",
    "    clean_strings = []\n",
    "\n",
    "    for i, s in enumerate(string_array):\n",
    "        if 'BAD' in s:\n",
    "            bad_indices.append(i)\n",
    "        else:\n",
    "            clean_strings.append(s)\n",
    "\n",
    "    # Print the indices of bad strings\n",
    "    if bad_indices:\n",
    "        print(f\"Found 'BAD' in strings at indices: {bad_indices}\")\n",
    "    else:\n",
    "        print(\"No strings containing 'BAD' found.\")\n",
    "\n",
    "    return clean_strings, bad_indices\n",
    "\n",
    "edited_codes, bad_indices = remove_bad_strings(generated_codes)\n",
    "print(len(edited_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37i6Qj5UAGP8",
    "outputId": "c386c51f-39be-43db-d478-1c727392a46e"
   },
   "outputs": [],
   "source": [
    "human_eval['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQLpT9d1AGP8",
    "outputId": "d9981871-7050-4031-c0d6-08806681fba0"
   },
   "outputs": [],
   "source": [
    "%pip install evaluate\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "# Load evaluation metric\n",
    "code_eval = load(\"code_eval\")\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "problems = human_eval\n",
    "test = []\n",
    "for i, item in enumerate(edited_codes):\n",
    "    edited_codes[i] = [item]\n",
    "pred = edited_codes\n",
    "c = 0\n",
    "\n",
    "for i, s in enumerate(human_eval[:100]):\n",
    "    if i not in bad_indices:\n",
    "        test.append(s)\n",
    "        c = c+1\n",
    "        print(c)\n",
    "\n",
    "pass_at_k = code_eval.compute(\n",
    "        predictions=pred,\n",
    "        references=test,\n",
    "        k=[1]\n",
    ")\n",
    "print(pass_at_k)\n",
    "print(pass_at_k[0]['pass@1']*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3eEhla-BwuO"
   },
   "outputs": [],
   "source": [
    "# Generate CoT (Chain of Thought) dataset\n",
    "cot_examples = generate_dataset(\n",
    "    problem_dataset=mbpp_problems,\n",
    "    task_prompt=COT_PROMPT_TEMPLATE,\n",
    "    solution_field=\"solution_cot\",\n",
    "    output_marker=\"Step-by-step solution:\",\n",
    "    teacher_model=teacher_model,\n",
    "    teacher_tokenizer=teacher_tokenizer,\n",
    "    num_examples=50,\n",
    "    output_file=\"datasets/cot_dataset.json\"\n",
    ")\n",
    "\n",
    "# Generate code dataset from CoT\n",
    "code_examples = generate_dataset(\n",
    "    problem_dataset=cot_examples,  # Use the output from CoT as input\n",
    "    task_prompt=DEVELOPER_PROMPT_TEMPLATE,\n",
    "    solution_field=\"code\",\n",
    "    output_marker=\"Python code:\",\n",
    "    teacher_model=teacher_model,\n",
    "    teacher_tokenizer=teacher_tokenizer,\n",
    "    num_examples=50,\n",
    "    output_file=\"datasets/code_dataset.json\"\n",
    ")\n",
    "\n",
    "# Generate debugged code dataset\n",
    "debugged_examples = generate_dataset(\n",
    "    problem_dataset=code_examples,  # Use the code examples as input\n",
    "    task_prompt=DEBUGGER_PROMPT_TEMPLATE,\n",
    "    solution_field=\"debugged_code\",\n",
    "    output_marker=\"Debugged Python code:\",\n",
    "    teacher_model=teacher_model,\n",
    "    teacher_tokenizer=teacher_tokenizer,\n",
    "    num_examples=50,\n",
    "    output_file=\"datasets/debugged_code_dataset.json\"\n",
    ")\n",
    "\n",
    "# Generate code explanations\n",
    "explanation_examples = generate_dataset(\n",
    "    problem_dataset=code_examples,  # Use code examples that also have CoT\n",
    "    task_prompt=EXPLAINER_PROMPT_TEMPLATE,\n",
    "    solution_field=\"explanation\",\n",
    "    output_marker=\"Explanation of the code:\",\n",
    "    teacher_model=teacher_model,\n",
    "    teacher_tokenizer=teacher_tokenizer,\n",
    "    num_examples=50,\n",
    "    output_file=\"datasets/explanation_dataset.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7Zgy_sdNKNV"
   },
   "outputs": [],
   "source": [
    "for i, example in enumerate(mbpp_problems):\n",
    "  print(f\"Problem number: {i}\")\n",
    "  print(f\"Problem: {example['problem']}\")\n",
    "  print(\"Test cases:\")\n",
    "  print(example['test_case'])\n",
    "  print(\"Code Solution:\")\n",
    "  print(example['solution'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
