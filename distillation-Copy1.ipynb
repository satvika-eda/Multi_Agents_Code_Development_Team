{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4240cc14-50b6-4341-ba04-dcde88f52a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "teacher = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "\n",
    "student = \"Qwen/Qwen2.5-Coder-0.5B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    Load_in_8bit_fp32_cpu_offload=True)\n",
    "\n",
    "token_hf = \"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\"\n",
    "\n",
    "human_eval = load_dataset(\"openai_humaneval\")['test']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# device_map = {\n",
    "#     \"transformer.h\": \"cuda:0\",\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef6900ad-50e4-451b-8cbb-114cde1697aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ed54886ed6425fbd041ec69853150e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a75636063a9457daa31f7691cec189f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b6d81bf6204986a233b871ccba21e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f5f187441d4ceaacabcf1b48ed4409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39203ee176ef406695d4520dfff13e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    teacher, \n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    token = token_hf\n",
    ")\n",
    "\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\n",
    "    student,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token = token_hf,\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    teacher,\n",
    "    token = token_hf)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfde5f20-a17b-438e-9121-9f170c2f14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code(generated_text):\n",
    "    start_marker = \"```python\"\n",
    "    end_marker = \"```\"\n",
    "\n",
    "    start_index = generated_text.find(start_marker)\n",
    "    if start_index == -1:\n",
    "         return \"\"\n",
    "    \n",
    "    start_index += len(start_marker)\n",
    "    \n",
    "    end_index = generated_text.find(end_marker, start_index)\n",
    "    if end_index == -1:\n",
    "        code = generated_text[start_index:].strip()\n",
    "    else:\n",
    "        code = generated_text[start_index:end_index].strip()\n",
    "    \n",
    "    return code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17b969a9-66c1-4752-b70f-9e1dfc70f659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f817d5d056c646e4b14e78aea158fdd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_human_eval = human_eval.map(lambda ex: tokenizer(ex[\"prompt\"], truncation=True, max_length=512), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23983f42-291f-4494-9211-aa5617848491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eda.s/.local/lib/python3.9/site-packages/transformers/generation/utils.py:2672: UserWarning: No PYTORCH_KERNEL_CACHE_PATH or HOME environment variable set! This disables kernel caching. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1426.)\n",
      "  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Response: from typing import *\n",
      "from collections import *\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    for i in range(len(numbers)):\n",
      "        for j in range(i + 1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False\n"
     ]
    }
   ],
   "source": [
    "def get_teacher_response(prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a coding assistant. When given a prompt, generate only a markdown code block that contains \"\n",
    "                \"valid Python code. The code block must start with ```python on its own line, then include the code, and \"\n",
    "                \"finally end with ``` on its own line. Do not include any extra text, comments, or explanations.\"\n",
    "            )\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt[0]}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(teacher_model.device)\n",
    "    \n",
    "    outputs = teacher_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=512,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     print(full_response)\n",
    "    assistant_response = full_response.split(\"assistant\\n\")[-1].strip()\n",
    "    \n",
    "    return assistant_response\n",
    "\n",
    "batch = tokenized_human_eval[0:1]\n",
    "teacher_output = extract_code(get_teacher_response(batch[\"prompt\"]))\n",
    "\n",
    "# print(batch['prompt'][0])\n",
    "# teacher_response = tokenizer.batch_decode(teacher_output_ids, skip_special_tokens=True)\n",
    "print(\"Teacher Response:\", teacher_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa820b5-a718-4c69-a9cc-ff24a8c7260b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "807425c5-7585-4de5-9007-47974345bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, temperature):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence loss between the teacher's softened predictions\n",
    "    and the student's predictions.\n",
    "    \"\"\"\n",
    "    # Scale logits by temperature; the temperature softens the probability distribution.\n",
    "    student_logits_T = student_logits / temperature\n",
    "    teacher_logits_T = teacher_logits / temperature\n",
    "\n",
    "    # Compute soft targets and student log probabilities\n",
    "    teacher_probs = F.softmax(teacher_logits_T, dim=-1)\n",
    "    student_log_probs = F.log_softmax(student_logits_T, dim=-1)\n",
    "    \n",
    "    # KL divergence; note the temperature factor scaling the gradients appropriately.\n",
    "    loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "771dd7d6-59d5-4119-bf1e-713f3b2511e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eda.s/.local/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss: 3032.4798177083335\n",
      "Epoch 2: Average Loss: 2659.1771298363096\n",
      "Epoch 3: Average Loss: 2395.5379813058034\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set training parameters\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 3\n",
    "batch_size = 8\n",
    "temperature = 2.0        \n",
    "\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "\n",
    "student_model.train()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(examples):\n",
    "    texts = [ex[\"prompt\"] for ex in examples]  \n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(tokenized_human_eval, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move inputs to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Teacher generates response (or logits) for distillation\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Use logits before softmax; may need to adjust if using generation outputs\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "        \n",
    "        # Student forward pass\n",
    "        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        common_vocab_size = min(student_logits.size(-1), teacher_logits.size(-1))\n",
    "        student_logits_aligned = student_logits[..., :common_vocab_size]\n",
    "        teacher_logits_aligned = teacher_logits[..., :common_vocab_size]\n",
    "        \n",
    "        # Compute distillation loss. For demonstration, using only soft target loss.\n",
    "        loss = distillation_loss(student_logits_aligned, teacher_logits_aligned, temperature)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Average Loss: {total_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0e8e8e2-08f8-41c4-aa0b-b8a8a129f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Average Loss (per batch): 2218.1168\n",
      "Epoch 3: Average Loss (per token): 2.1182\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move inputs to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Teacher forward pass (no gradients)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "\n",
    "        # Student forward pass\n",
    "        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        # Align logits dimensions\n",
    "        common_vocab_size = min(student_logits.size(-1), teacher_logits.size(-1))\n",
    "        student_logits_aligned = student_logits[..., :common_vocab_size]\n",
    "        teacher_logits_aligned = teacher_logits[..., :common_vocab_size]\n",
    "\n",
    "        # Compute distillation loss\n",
    "        loss = distillation_loss(student_logits_aligned, teacher_logits_aligned, temperature)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # Count tokens processed in this batch; assume attention_mask sums to number of tokens\n",
    "        total_tokens += attention_mask.sum().item()\n",
    "\n",
    "    avg_loss_overall = total_loss / len(train_loader)        # Loss averaged per batch\n",
    "    avg_loss_per_token = total_loss / total_tokens             # Loss averaged per token\n",
    "    print(f\"Epoch {epoch+1}: Average Loss (per batch): {avg_loss_overall:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}: Average Loss (per token): {avg_loss_per_token:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec77da5e-080f-4279-9fed-09180094d983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: from typing import List\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n",
      "    given threshold.\n",
      "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
      "    False\n",
      "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "    True\n",
      "    \"\"\"\n",
      "\n",
      "    for i in range(len(numbers) - 1):\n",
      "        if abs(numbers[i] - numbers[i + 1]) < threshold:\n",
      "            return True\n",
      "    return False\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    import doctest\n",
      "    doctest.testmod()\n",
      "Example 2: from typing import List\n",
      "\n",
      "\n",
      "def separate_paren_groups(paren_string: str) -> List[str]:\n",
      "    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\n",
      "    separate those group into separate strings and return the list of those.\n",
      "    Separate groups are balanced (each open brace is properly closed) and not nested within each other\n",
      "    Ignore any spaces in the input string.\n",
      "    >>> separate_paren_groups('( ) (( )) (( )( ))')\n",
      "    ['()', '(())', '(()())']\n",
      "    \"\"\"\n",
      "\n",
      "    # Your code here\n",
      "    pass\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    import doctest\n",
      "    doctest.testmod()\n",
      "Example 3: \n",
      "\n",
      "def truncate_number(number: float) -> float:\n",
      "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
      "    and integer part (largest integer smaller than given number) and decimals\n",
      "    (leftover part always smaller than 1).\n",
      "\n",
      "    Return the decimal part of the number.\n",
      "    >>> truncate_number(3.5)\n",
      "    0.5\n",
      "    \"\"\"\n",
      ":\n",
      "    return number - int(number)\n",
      "\n",
      "def is_prime(number: int) -> bool:\n",
      "    \"\"\" Return True if number is prime, False otherwise.\n",
      "    >>> is_prime(1)\n",
      "    False\n",
      "    >>> is_prime(2)\n",
      "    True\n",
      "    >>> is_prime(3)\n",
      "    True\n",
      "    >>> is_prime(4)\n",
      "    False\n",
      "    >>> is_prime(5)\n",
      "    True\n",
      "    >>> is_prime(11)\n",
      "    True\n",
      "    >>> is_prime(12)\n",
      "    False\n",
      "    >>> is_prime(13)\n",
      "    True\n",
      "    >>> is_prime(14)\n",
      "    False\n",
      "    >>> is_prime\n",
      "Example 4: from typing import List\n",
      "\n",
      "\n",
      "def below_zero(operations: List[int]) -> bool:\n",
      "    \"\"\" You're given a list of deposit and withdrawal operations on a bank account that starts with\n",
      "    zero balance. Your task is to detect if at any point the balance of account fallls below zero, and\n",
      "    at that point function should return True. Otherwise it should return False.\n",
      "    >>> below_zero([1, 2, 3])\n",
      "    False\n",
      "    >>> below_zero([1, 2, -4, 5])\n",
      "    True\n",
      "    \"\"\"\n",
      "\n",
      "    balance = 0\n",
      "    for operation in operations:\n",
      "        balance += operation\n",
      "        if balance < 0:\n",
      "            return True\n",
      "    return False\n",
      "Example 5: from typing import List\n",
      "\n",
      "\n",
      "def mean_absolute_deviation(numbers: List[float]) -> float:\n",
      "    \"\"\" For a given list of input numbers, calculate Mean Absolute Deviation\n",
      "    around the mean of this dataset.\n",
      "    Mean Absolute Deviation is the average absolute difference between each\n",
      "    element and a centerpoint (mean in this case):\n",
      "    MAD = average | x - x_mean |\n",
      "    >>> mean_absolute_deviation([1.0, 2.0, 3.0, 4.0])\n",
      "    1.0\n",
      "    \"\"\"\n",
      "\n",
      "    # Your code here\n",
      "    return 0.0\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    import doctest\n",
      "\n",
      "    doctest.testmod()\n"
     ]
    }
   ],
   "source": [
    "student_model.eval()\n",
    "\n",
    "# Example evaluation on a subset:\n",
    "eval_batch = human_eval.select(range(5))\n",
    "inputs_eval = tokenizer(eval_batch[\"prompt\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "inputs_eval = {k: v.to(device) for k, v in inputs_eval.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = student_model.generate(**inputs_eval, max_new_tokens=128)\n",
    "    \n",
    "generated_responses = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "for idx, response in enumerate(generated_responses):\n",
    "    print(f\"Example {idx+1}: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5df2b0-d8e3-4e33-8411-e5ddcb833f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(student_model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33feb0b3-3d63-429c-b9f0-d3576ca983a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Prepare your models, optimizer, and dataloaders using the accelerator.\n",
    "# Note: Do not call `.to(device)` explicitly on quantized models.\n",
    "teacher, student, optimizer, dataloader = accelerator.prepare(\n",
    "    teacher, student, optimizer, dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4d106-3a10-417f-9b6e-5f93f1fe20a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3  # Adjust as needed.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Teacher forward pass (no gradients needed)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "\n",
    "        # Student forward pass\n",
    "        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits\n",
    "\n",
    "        # Compute the distillation loss\n",
    "        loss = distillation_loss(student_logits, teacher_logits, T=3.0, alpha=0.7)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86019600-1f2a-4011-8818-5df2189336af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8513990-59f4-4eb4-a541-01d84db4d5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher vocab size: 151643\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'student_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeacher vocab size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39mvocab_size)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudent vocab size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mstudent_tokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mvocab_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'student_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Teacher vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Student vocab size:\", student_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283222f9-be4a-4476-b38a-8e7baf1d985a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
